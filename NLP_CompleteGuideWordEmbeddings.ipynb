{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Word2Vec Algorithm\n",
    "\n",
    "This is the most popular algorithm for computing embeddings. It basically consists of a mini neural network that tries to learn a language model. Remember how we tried to generate text by picking probabilistically the next word? In its simplest form, the neural network can learn what is the next word after a given input node. Obviously, the results will be rather simplistic. We need more information about the context of a word in order to learn good embeddings.\n",
    "## CBOW vs Skip-Gram\n",
    "\n",
    "**CBOW (Continuous Bag-Of-Words**) is about creating a network that tries to predict the word in the middle given some surrounding words: [W[-3], W[-2], W[-1], W[1], W[2], W[3]] => W[0]\n",
    "\n",
    "**Skip-Gram** is the opposite of CBOW, try to predict the surrounding words given the word in the middle: W[0] => [W[-3], W[-2], W[-1], W[1], W[2], W[3]]\n",
    "\n",
    "The computed network weights are actually the word embeddings we were looking for. If you don’t have any neural network experience, don’t worry, it’s not needed for doing the practical exercises in this tutorial.\n",
    "Word2Vec with Gensim\n",
    "\n",
    "Gensim provides a quality implementation of the Word2Vec model. Let’s see it in action on the Brown Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\rzouga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "print(brown.sents())\n",
    "w2v_model = Word2Vec(brown.sents(), size=128, window=5, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now take the model for a spin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.72964776e-01 -4.32864279e-02 -8.88026059e-02  1.18742704e-01\n",
      "  1.23760151e-02  2.07855478e-01 -7.17213675e-02 -1.23750448e-01\n",
      "  4.34991382e-02  3.04020971e-01  2.05373392e-01  2.18561757e-02\n",
      "  1.36980131e-01 -7.64713716e-03 -1.53590009e-01 -1.13585025e-01\n",
      "  2.60129631e-01 -2.14413181e-01  5.43992519e-01 -4.22305129e-02\n",
      " -5.32292202e-02  2.98417270e-01  1.18801601e-01 -6.26715198e-02\n",
      "  1.38511822e-01  1.14757903e-02  2.63060689e-01  3.70042413e-01\n",
      " -1.95784763e-01  3.46381664e-01  1.17206506e-01  2.66734362e-01\n",
      "  3.11203927e-01  4.96482626e-02  1.34677754e-03 -2.59095728e-01\n",
      " -6.33635046e-03 -6.47251680e-02 -5.11556491e-02 -7.06180409e-02\n",
      "  4.45125774e-02  9.72890295e-03 -1.68661684e-01  4.98738401e-02\n",
      "  1.75717562e-01  3.97206992e-02  1.61752731e-01 -1.80850789e-01\n",
      " -2.39074558e-01  6.04641512e-02  1.51380375e-02 -4.40965556e-02\n",
      "  1.18253222e-02 -4.47812304e-02 -4.36309929e-04 -1.12051353e-01\n",
      " -4.52137589e-02  2.38479957e-01 -7.89065734e-02 -6.74944073e-02\n",
      " -2.59511888e-01 -1.35544315e-01  1.79441601e-01  3.00230160e-02\n",
      "  1.96906552e-02 -2.54492275e-02  1.37253746e-01  1.02121825e-03\n",
      " -8.82040858e-02  5.35202138e-02 -3.60872000e-02  1.54600844e-01\n",
      "  1.33303687e-01 -2.37190165e-02 -1.24244355e-01  1.92045718e-01\n",
      "  1.75053254e-02  5.50934449e-02  1.52713647e-02  1.34943426e-01\n",
      "  4.20891158e-02  2.31216103e-01  2.30101258e-01 -2.56702989e-01\n",
      " -7.59943724e-02 -1.28003478e-01 -2.30178490e-01  1.25442147e-01\n",
      " -5.25349341e-02 -2.57595867e-01 -5.38096763e-02 -4.50958312e-02\n",
      " -5.71431257e-02 -1.74608618e-01 -1.45224318e-01  1.86557323e-01\n",
      "  1.40318513e-01 -2.60820915e-03  6.19608862e-03  1.45233661e-01\n",
      "  2.34693475e-02  1.57602385e-01 -5.70507236e-02  3.20908353e-02\n",
      " -1.31717650e-02  2.23549902e-01 -1.14249088e-01  2.04805300e-01\n",
      "  7.00357109e-02  2.36805782e-01 -4.13965493e-01  6.11708686e-02\n",
      "  1.89511664e-02 -2.18395099e-01 -2.71892138e-02 -1.22805182e-02\n",
      "  3.62619944e-02  1.14051320e-01 -2.12996408e-01  4.07713354e-02\n",
      "  2.58247945e-02  1.17065221e-01 -1.72379270e-01  5.46043627e-02\n",
      " -8.70698839e-02  1.48948982e-01 -1.56118229e-01  5.87284341e-02] [ 3.18008065e-01 -1.31325245e-01 -1.52168632e-01  1.57519013e-01\n",
      "  5.85347153e-02  2.90737152e-01 -7.45136365e-02 -9.82052758e-02\n",
      "  6.79430440e-02  4.48128581e-01  2.94839740e-01  8.19119532e-03\n",
      "  1.51949435e-01  3.68278883e-02 -2.00453639e-01 -9.09985974e-02\n",
      "  3.18735331e-01 -2.51763672e-01  8.26719344e-01 -6.37132153e-02\n",
      " -6.99271038e-02  3.96733284e-01 -4.43071388e-02 -1.98881269e-01\n",
      "  3.09800893e-01  9.47352573e-02  3.78921002e-01  5.51414490e-01\n",
      " -2.45711073e-01  5.04568994e-01  1.47491351e-01  3.43584180e-01\n",
      "  4.70712453e-01  9.57819223e-02 -2.53876448e-02 -4.79622900e-01\n",
      " -4.36551943e-02 -1.30454510e-01 -1.64343625e-01 -1.02184013e-01\n",
      "  9.59494561e-02  6.20505251e-02 -2.49633282e-01  1.16722494e-01\n",
      "  3.33236963e-01  1.73063576e-01  1.61822513e-01 -3.12123686e-01\n",
      " -2.58294165e-01  1.04571953e-01 -1.25591923e-02 -1.31723518e-02\n",
      "  1.25341401e-01 -1.24248862e-01 -1.28622845e-01 -2.77866721e-01\n",
      " -1.31223015e-02  2.06069037e-01 -6.26522824e-02 -5.34922034e-02\n",
      " -3.13122034e-01 -1.53673112e-01  2.38541901e-01  1.67386886e-02\n",
      "  5.06166890e-02 -4.95366417e-02  1.77859187e-01  8.91197026e-02\n",
      " -1.27336457e-01  1.17008843e-01 -5.79689804e-04  1.49654612e-01\n",
      "  8.28856304e-02  3.77672329e-03 -1.99919134e-01  2.58352041e-01\n",
      "  5.04083261e-02  1.09418154e-01  1.05319738e-01  1.15120649e-01\n",
      "  1.26279108e-02  2.98392415e-01  2.47228786e-01 -3.36272985e-01\n",
      " -2.17832088e-01 -2.98665106e-01 -3.29023123e-01  6.06540516e-02\n",
      " -7.12993443e-02 -3.31567615e-01 -5.68724610e-02 -6.34386912e-02\n",
      " -4.90704216e-02 -2.21029460e-01 -2.63662636e-01  2.19036222e-01\n",
      "  1.09143637e-01 -3.11721638e-02  5.62135987e-02  1.92657873e-01\n",
      " -9.93370861e-02  2.40813032e-01 -1.77511144e-02  5.06321490e-02\n",
      " -2.20691576e-03  1.33085668e-01 -2.50778824e-01  2.52641320e-01\n",
      "  2.00819418e-01  2.74070680e-01 -6.20771348e-01 -7.84753263e-02\n",
      "  5.61347678e-02 -2.43497163e-01 -1.00442857e-01 -6.11536428e-02\n",
      " -5.61625101e-02  2.35096648e-01 -2.17410058e-01  8.52556676e-02\n",
      "  4.70514297e-02  3.90892506e-01 -3.82881850e-01  7.81366378e-02\n",
      " -2.54178286e-01  2.21720859e-01 -1.24955200e-01  9.92308259e-02]\n"
     ]
    }
   ],
   "source": [
    "# Getting the vector for a word\n",
    "print(w2v_model.wv['Italy'], w2v_model.wv['France'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Italy', 0.975581169128418), ('France', 0.9638313055038452), ('Rome', 0.9636051058769226), ('Eugene', 0.9622185230255127), ('headquarters', 0.9616745114326477), ('London', 0.9605855345726013), ('dancing', 0.9596837759017944), ('breakfast', 0.9589269757270813), ('Harvard', 0.957659125328064), ('Vienna', 0.9567881226539612)]\n"
     ]
    }
   ],
   "source": [
    "# Getting most similar vectors\n",
    "print(w2v_model.wv.most_similar('Paris'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('extracting', 0.96043461561203), ('united', 0.9594190120697021), ('duties', 0.9566020965576172), ('savage', 0.9526326060295105), ('stem', 0.9486423134803772), ('measuring', 0.944673478603363), ('patronage', 0.9441121816635132), ('Colonial', 0.9437551498413086), ('trigger', 0.9437280297279358), ('original', 0.9435445666313171)]\n",
      "[('reputed', 0.9475245475769043), ('earth', 0.9457082152366638), ('beach', 0.9398365020751953), ('sink', 0.9354729652404785), ('grounds', 0.9338986873626709), ('planks', 0.9321511387825012), ('Church', 0.9311625361442566), ('transformed', 0.9308785200119019), ('airfield', 0.9303730130195618), ('plantation', 0.9300895929336548)]\n"
     ]
    }
   ],
   "source": [
    "# \"King\" - \"Man\" + \"Woman\" == \"Queen\"\n",
    "print(w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "print(w2v_model.wv.most_similar(positive=[\"Rome\", \"France\"], negative=[\"Italy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’ve been coding along, you probably are pretty disappointed of the results. Let’s try a bigger corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "# Go here and download + unzip the Text8 Corpus: http://mattmahoney.net/dc/text8.zip\n",
    "# We take only words that appear more than 150 times for doing a visualization later\n",
    "w2v_model2 = Word2Vec(Text8Corpus('C:/Users/rzouga/Downloads/text8/text8'), size=100, window=5, min_count=150, workers=4)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opted to only use the most popular words so that it’s easier to make a visualization later. Let’s see how does the new model perform (words and values will differ a bit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting most similar vectors\n",
    "print(w2v_model2.wv.most_similar('paris'))\n",
    "# [('louvre', 0.7243613004684448), \n",
    "#  ('venice', 0.7047281265258789), \n",
    "#  ('vienna', 0.7043783068656921),\n",
    "#  ('montparnasse', 0.7016372680664062), \n",
    "#  ('le', 0.6870340704917908), \n",
    "#  ('sur', 0.6818796396255493), \n",
    "#  ('chapelle', 0.6787714958190918), \n",
    "#  ('rodin', 0.6766049265861511), \n",
    "#  ('bologna', 0.6761612892150879), \n",
    "#  ('munich', 0.6749240159988403)]\n",
    " \n",
    "# \"King\" - \"Man\" + \"Woman\" == \"Queen\"\n",
    "print(w2v_model2.most_similar(['woman', 'king'], ['man'], topn=3))\n",
    "# [('queen', 0.6777610778808594), ('throne', 0.6143913269042969), ('elizabeth', 0.593910813331604)]\n",
    " \n",
    "# \"Father\" - \"Boy\" + \"Girl\" == \"Mother\"\n",
    "print(w2v_model2.most_similar(['girl', 'father'], ['boy'], topn=3))\n",
    "# [('mother', 0.7972878813743591), ('wife', 0.7469687461853027), ('grandmother', 0.7419005632400513)]\n",
    " \n",
    "# \"Paris\" - \"France\" + \"Italy\" = \"Rome\"\n",
    "print(w2v_model2.most_similar(['paris', 'italy'], ['france'], topn=3))\n",
    "# [('venice', 0.7461134195327759), ('vienna', 0.7134193778038025), ('florence', 0.7019181251525879)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
