{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embeddings** provide a dense representation of words and their relative meanings.\n",
    "\n",
    "They are an improvement over sparse representations used in simpler bag of word model representations.\n",
    "\n",
    "Word embeddings can be learned from text data and reused among projects. They can also be learned as part of fitting a neural network on text data.\n",
    "\n",
    "In this tutorial, you will discover how to use word embeddings for deep learning in Python with Keras.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "About word embeddings and that Keras supports word embeddings via the Embedding layer.\n",
    "How to learn a word embedding while fitting a neural network.\n",
    "How to use a pre-trained word embedding in a neural network.\n",
    "\n",
    "https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "# Tutorial Overview\n",
    "This tutorial is divided into 3 parts; they are:\n",
    "\n",
    "#### Word Embedding\n",
    "#### Keras Embedding Layer\n",
    "#### Example of Learning an Embedding\n",
    "#### Example of Using Pre-Trained GloVe Embedding\n",
    "# 1. Word Embedding\n",
    "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
    "\n",
    "It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
    "\n",
    "Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
    "\n",
    "The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "\n",
    "The position of a word in the learned vector space is referred to as its embedding.\n",
    "\n",
    "Two popular examples of methods of learning word embeddings from text include:\n",
    "\n",
    "Word2Vec.\n",
    "GloVe.\n",
    "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.\n",
    "\n",
    "# 2. Keras Embedding Layer\n",
    "Keras offers an Embedding layer that can be used for neural networks on text data.\n",
    "\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "**input_dim** This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "**output_dim** This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "**input_length:** This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "For example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n",
    "\n",
    "\n",
    "**e = Embedding(200, 32, input_length=50)**\n",
    "The Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer.\n",
    "\n",
    "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
    "\n",
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
    "\n",
    "Now, let’s see how we can use an Embedding layer in practice.\n",
    "\n",
    "#### 3. Example of Learning an Embedding\n",
    "In this section, we will look at how we can learn a word embedding while fitting a neural network on a text classification problem.\n",
    "\n",
    "We will define a small problem where we have 10 text documents, each with a comment about a piece of work a student submitted. Each text document is classified as positive “1” or negative “0”. This is a simple sentiment analysis problem.\n",
    "# The complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 4], [3, 43], [25, 9], [15, 43], [4], [6], [43, 9], [32, 3], [43, 43], [2, 30, 4, 49]]\n",
      "[[ 4  4  0  0]\n",
      " [ 3 43  0  0]\n",
      " [25  9  0  0]\n",
      " [15 43  0  0]\n",
      " [ 4  0  0  0]\n",
      " [ 6  0  0  0]\n",
      " [43  9  0  0]\n",
      " [32  3  0  0]\n",
      " [43 43  0  0]\n",
      " [ 2 30  4 49]]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\rzouga\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000000010B5B840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000000010B5B840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from numpy import array\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "layers = tf.keras.layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
