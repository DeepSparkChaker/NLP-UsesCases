{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common regex patterns\n",
    "\n",
    "There are hundreds of characters and patterns you can learn and memorize with regular expressions, but to get started, I want to share a few common patterns.\n",
    "\n",
    "The first pattern **\\w** we already saw, it is used to match words.\n",
    "\n",
    "The **\\d** pattern allows us to match digits, which can be useful when you need to find them and separate them in a string. \n",
    "The **\\s** pattern matches spaces, the period is a wildcard character.\n",
    "\n",
    "The wildcard will match ANY letter or symbol.\n",
    "The **+ and * .** characters allow things to become greedy, grabbing repeats of single letters or whole patterns.\n",
    "\n",
    "For example to match a full word rather than one character, we need to add the + symbol after the \\w.\n",
    "\n",
    "Using these character classes as capital letters negates them so the **\\S** matches anything that is not a space. You can also create a group of characters you want by putting them inside \n",
    "\n",
    "https://www.computerhope.com/unix/regex-quickref.htm\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split lines or split the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string at line boundaries\n",
    "file_split = file.split(\"\\n\")\n",
    "\n",
    "# Print file_split\n",
    "print(file_split)\n",
    "\n",
    "# Complete for-loop to split by commas\n",
    "for substring in file_split:\n",
    "    substring_split = substring.split(\",\")\n",
    "    print(substring_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding and replacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movies:\n",
    "  \t# Find if actor occurrs between 37 and 41 inclusive\n",
    "    if movie.find(\"actor\", 37, 42) == -1:\n",
    "        print(\"Word not found\")\n",
    "    # Count occurrences and replace two by one\n",
    "    elif movie.count(\"actor\") == 2:  \n",
    "        print(movie.replace(\"actor actor\", \"actor\"))\n",
    "    else:\n",
    "        # Replace three occurrences by one\n",
    "        print(movie.replace(\"actor actor actor\", \"actor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where's the word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movies:\n",
    "  try:\n",
    "    # Find the first occurrence of word\n",
    "  \tprint(movie.index(\"movie\", 12, 50))\n",
    "  except ValueError:\n",
    "    print(\"substring not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negations \n",
    "movies_no_negation = movies.replace(\"isn't\", \"is\")\n",
    "\n",
    "# Replace important\n",
    "movies_antonym = movies_no_negation.replace(\"important\", \"insignificant\")\n",
    "\n",
    "# Print out\n",
    "print(movies_antonym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the substrings to the variables\n",
    "first_pos = wikipedia_article[3:19].lower()\n",
    "second_pos = wikipedia_article[21:44].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the substrings to the variables\n",
    "first_pos = wikipedia_article[3:19].lower()\n",
    "second_pos = wikipedia_article[21:44].lower()\n",
    "\n",
    "# Define string with placeholders \n",
    "my_list.append(\"The tool {} is used in {}\")\n",
    "\n",
    "# Define string with rearranged placeholders\n",
    "my_list.append(\"The tool {1} is used in {0}\")\n",
    "\n",
    "# Use format to print strings\n",
    "for my_string in my_list:\n",
    "  \tprint(my_string.format(first_pos, second_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses=['artificial intelligence', 'neural networks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are interested in artificial intelligence, you can take the course related to neural networks\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary\n",
    "plan = {\n",
    "  \t\t\"field\": courses[0],\n",
    "        \"tool\": courses[1]\n",
    "        }\n",
    "\n",
    "# Complete the placeholders accessing elements of field and tool keys in the data dictionary\n",
    "my_message = \"If you are interested in {data[field]}, you can take the course related to {data[tool]}\"\n",
    "\n",
    "# Use the plan dictionary to replace placeholders\n",
    "print(my_message.format(data=plan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning. Today is January 31, 2021. It's 20:30 ... time to work!\n"
     ]
    }
   ],
   "source": [
    "# Import datetime \n",
    "from datetime import datetime\n",
    "\n",
    "# Assign date to get_date\n",
    "get_date = datetime.now()\n",
    "\n",
    "# Add named placeholders with format specifiers\n",
    "message = \"Good morning. Today is {today:%B %d, %Y}. It's {today:%H:%M} ... time to work!\"\n",
    "\n",
    "# Format date\n",
    "print(message.format(today=get_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 1, 31, 20, 30, 49, 993956)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatted string literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the f-string : ''\n",
    "print(f\"Data science is considered {field1!r} in the {fact1:d}st century\")\n",
    "# Complete the f-string: exponentiel\n",
    "print(f\"About {fact2:e} of {field2} in the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the f-string\n",
    "print(f\"{field3} create around {fact3:.2f}% of the data but only {fact4:.1f}% is analyzed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include both variables and the result of dividing them \n",
    "print(f\"{number1} tweets were downloaded in {number2} minutes indicating a speed of {number1/number2:.1f} tweets per min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the substring http by an empty string\n",
    "print(f\"{string1.replace('https', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the length of list by 120 rounded to two decimals\n",
    "print(f\"Only {(len(list_links)*100/120):.2f}% of the posts contain links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access values of date and price in east dictionary\n",
    "print(f\"The price for a house in the east neighborhood was ${east['price']} in {east['date']:%m-%d-%Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access values of date and price in west dictionary\n",
    "print(f\"The price for a house in the west neighborhood was ${west['price']} in {west['date']:%m-%d-%Y}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Template\n",
    "from string import Template\n",
    "\n",
    "# Create a template\n",
    "wikipedia = Template(\"$tool is a $description\")\n",
    "\n",
    "# Substitute variables in template\n",
    "print(wikipedia.substitute(tool=tool1, description=description1))\n",
    "print(wikipedia.substitute(tool=tool2, description=description2))\n",
    "print(wikipedia.substitute(tool=tool3, description=description3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import template\n",
    "from string import Template\n",
    "\n",
    "# Select variables\n",
    "our_tool = tools[0]\n",
    "our_fee = tools[1]\n",
    "our_pay = tools[2]\n",
    "\n",
    "# Create template\n",
    "course = Template(\"We are offering a 3-month beginner course on $tool just for $$ $fee ${pay}ly\")\n",
    "\n",
    "# Substitute identifiers with three variables\n",
    "print(course.substitute(tool=our_tool, fee=our_fee, pay=our_pay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import template\n",
    "from string import Template\n",
    "\n",
    "# Complete template string using identifiers\n",
    "the_answers = Template(\"Check your answer 1: $answer1, and your answer 2: $answer2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import template\n",
    "from string import Template\n",
    "\n",
    "# Complete template string using identifiers\n",
    "the_answers = Template(\"Check your answer 1: $answer1, and your answer 2: $answer2\")\n",
    "\n",
    "# Use substitute to replace identifiers\n",
    "try:\n",
    "    print(the_answers.substitute(answers))\n",
    "except KeyError:\n",
    "    print(\"Missing information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the re module\n",
    "import re\n",
    "\n",
    "# Write the regex\n",
    "regex = r\"@robot\\d\\W\"\n",
    "\n",
    "# Find all matches of regex\n",
    "print(re.findall(regex, sentiment_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex to obtain user mentions\n",
    "print(re.findall(r\"User_mentions:\\d\", sentiment_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex to obtain number of retweets\n",
    "print(re.findall(r\"number\\sof\\sretweets:\\s\\d\", sentiment_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex to match pattern separating sentences\n",
    "regex_sentence = r\"\\W\\dbreak\\W\"\n",
    "\n",
    "# Replace the regex_sentence with a space\n",
    "sentiment_sub = re.sub(regex_sentence, \" \", sentiment_analysis)\n",
    "\n",
    "# Write a regex to match pattern separating words\n",
    "regex_words = r\"\\Wnew\\w\"\n",
    "\n",
    "# Replace the regex_words and print the result\n",
    "sentiment_final = re.sub(regex_words, \" \", sentiment_sub)\n",
    "print(sentiment_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import re module\n",
    "import re\n",
    "for tweet in sentiment_analysis:\n",
    "  \t# Write regex to match http links and print out result\n",
    "\tprint(re.findall(r\"http\\S+\", tweet))\n",
    "\n",
    "\t# Write regex to match user mentions and print out result\n",
    "\tprint(re.findall(r\"@\\w+\", tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the for loop with a regex to find dates;27 minutes ago\n",
    "for date in sentiment_analysis:\n",
    "\tprint(re.findall(r\"\\d{1,2}\\s\\w+\\sago\", date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the for loop with a regex to find dates;23rd june 2018\n",
    "for date in sentiment_analysis:\n",
    "\tprint(re.findall(r\"\\d{1,2}\\w+\\s\\w+\\s\\d{4}\", date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the for loop with a regex to find dates,1st september 2019 17:25\n",
    "for date in sentiment_analysis:\n",
    "\tprint(re.findall(r\"\\d{1,2}\\w+\\s\\w+\\s\\d{4}\\s\\d{1,2}:\\d{2}\", date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex matching the hashtag pattern\n",
    "regex = r\"#\\w+\"\n",
    "# Replace the regex by an empty string\n",
    "no_hashtag = re.sub(regex, \"\", sentiment_analysis)\n",
    "# Get tokens by splitting text\n",
    "print(re.split(r\"\\s+\",no_hashtag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex metacharacters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex to match text file name\n",
    "regex = r\"^[aeiouAEIOU]{2,3}.+txt\"\n",
    "for text in sentiment_analysis:\n",
    "\t# Find all matches of the regex\n",
    "\tprint(re.findall(regex, text))\n",
    "    \n",
    "\t# Replace all matches with empty string\n",
    "\tprint(re.sub(regex, \"\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex to match a valid email address\n",
    "regex = r\"[A-Za-z0-9!#%&*\\$\\.]+@\\w+\\.com\"\n",
    "\n",
    "for example in emails:\n",
    "  \t# Match the regex to the string\n",
    "    if re.match(regex, example):\n",
    "        # Complete the format method to print out the result\n",
    "      \tprint(\"The email {email_example} is a valid email\".format(email_example=example))\n",
    "    else:\n",
    "      \tprint(\"The email {email_example} is invalid\".format(email_example=example))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex to match a valid password\n",
    "regex = r\"[A-Za-z0-9!#%&*\\$\\.]{8,20}\" \n",
    "\n",
    "for example in passwords:\n",
    "  \t# Scan the strings to find a match\n",
    "    if re.search(regex, example):\n",
    "        # Complete the format method to print out the result\n",
    "      \tprint(\"The password {pass_example} is a valid password\".format(pass_example=example))\n",
    "    else:\n",
    "      \tprint(\"The password {pass_example} is invalid\".format(pass_example=example))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy vs. non-greedy matching\n",
    "**Well done! Remember that a greedy quantifier will try to match as much as possible while a non-greedy quantifier will do it as few times as needed, expanding one character at a time and giving us the match we are looking for. Good!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to see that amazing show again!\n"
     ]
    }
   ],
   "source": [
    "tences with the optional words\n",
    "regex_negative = r\"(hate|dislike|disapprove).+?(?:movie|concert)\\s(.+?)\\.\"# Import re\n",
    "import re \n",
    "string = \"I want to see that <strong>amazing show</strong> again!\"\n",
    "# Write a regex to eliminate tags\n",
    "string_notags = re.sub(r\"<.*?>\", \"\", string)\n",
    "# Print out the result\n",
    "print(string_notags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to see that amazing show again!\n"
     ]
    }
   ],
   "source": [
    "# Import re\n",
    "import re \n",
    "# Write a regex to eliminate tags\n",
    "string_notags = re.sub(r\"<.+?>\", \"\", string)\n",
    "# Print out the result\n",
    "print(string_notags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(20, 22), match='24'>\n"
     ]
    }
   ],
   "source": [
    "# Write a lazy regex expression \n",
    "sentiment_analysis=\"I was born on April 24t\"\n",
    "numbers_found_lazy = re.search(r\"\\d+\", sentiment_analysis)\n",
    "# Print out the result\n",
    "print(numbers_found_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '4']\n"
     ]
    }
   ],
   "source": [
    "# Write a lazy regex expression \n",
    "numbers_found_lazy = re.findall(r\"\\d+?\", sentiment_analysis)\n",
    "# Print out the result\n",
    "print(numbers_found_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a lazy regex expression \n",
    "numbers_found_lazy = re.finditer(r\"\\d+\", sentiment_analysis)\n",
    "matchings = [match.group() for match in numbers_found_lazy]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24']\n"
     ]
    }
   ],
   "source": [
    "# Write a greedy regex expression \n",
    "numbers_found_greedy = re.findall(r\"\\d+\", sentiment_analysis)\n",
    "# Print out the result\n",
    "print(numbers_found_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy approach\n",
    "\n",
    "\n",
    "You have done some cleaning in your dataset but you are worried that there are sentences encased in parentheses that may cloud your analysis.\n",
    "\n",
    "Again, a greedy or a lazy quantifier may lead to different results.\n",
    "\n",
    "For example, if you want to extract a word starting with a and ending with e in the string I like apple pie, you may think that applying the greedy regex a.+e will return apple. However, your match will be apple pie. A way to overcome this is to make it lazy by using ? which will return apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"(They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying)\"]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis=\"Put vacation photos online (They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site (I'm crying). \"\n",
    "# Write a greedy regex expression to match \n",
    "sentences_found_greedy = re.findall(r\"\\(.*\\)\", sentiment_analysis)\n",
    "\n",
    "# Print out the result\n",
    "print(sentences_found_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(They were so cute)', \"(I'm crying)\"]\n"
     ]
    }
   ],
   "source": [
    "# Write a lazy regex expression\n",
    "sentences_found_lazy = re.findall(r\"\\(.*?\\)\", sentiment_analysis)\n",
    "\n",
    "# Print out the results\n",
    "print(sentences_found_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They were so cute'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2='(They were so cute)'\n",
    "s3=re.sub('\\(|\\)', '', s2)\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Put vacation photos online They were so cute a few yrs ago. PC crashed, and now I forget the name of the site I'm crying. \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2=sentiment_analysis\n",
    "s3=re.sub('\\(|\\)', '', s2)\n",
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex that matches email\n",
    "regex_email = r\"([a-zA-Z0-9]+)@\\S+\"\n",
    "for tweet in sentiment_analysis:\n",
    "    # Find all matches of regex in each tweet\n",
    "    email_matched = re.findall(regex_email,tweet)\n",
    "    # Complete the format method to print the results\n",
    "    print(\"Lists of users found in this tweet: {}\".format(email_matched))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have your boarding pass LA4214 AER-CDB 06NOV.\n",
    "\n",
    "You need to extract the information about the flight:\n",
    "\n",
    "    The two letters indicate the airline (e.g LA),\n",
    "    The 4 numbers are the flight number (e.g. 4214).\n",
    "    The three letters correspond to the departure (e.g AER),\n",
    "    The destination (CDB),\n",
    "    The date (06NOV) of the flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airline: LA Flight number: 4214\n",
      "Departure: AER Destination: CDB\n",
      "Date: 06NOV\n"
     ]
    }
   ],
   "source": [
    "# Import re\n",
    "import re\n",
    "flight=\"Here you have your boarding pass LA4214 AER-CDB 06NOV.\"\n",
    "# Write regex to capture information of the flight\n",
    "regex = r\"([A-Z]{2})(\\d{4})\\s([A-Z]{3})-([A-Z]{3})\\s(\\d{2}[A-Z]{3})\"\n",
    "\n",
    "# Find all matches of the flight information\n",
    "flight_matches = re.findall(regex, flight)\n",
    "    \n",
    "#Print the matches\n",
    "print(\"Airline: {} Flight number: {}\".format(flight_matches[0][0], flight_matches[0][1]))\n",
    "print(\"Departure: {} Destination: {}\".format(flight_matches[0][2], flight_matches[0][3]))\n",
    "print(\"Date: {}\".format(flight_matches[0][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LA', '4214', 'AER', 'CDB', '06NOV')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternation and non-capturing groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex that matches sentences with the optional words\n",
    "regex_positive = r\"(love|like|enjoy).+?(movie|concert)\\s(.+?)\\.\"\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "\t# Find all matches of regex in tweet\n",
    "    positive_matches = re.findall(regex_positive, tweet)\n",
    "    \n",
    "    # Complete format to print out the results\n",
    "    print(\"Positive comments found {}\".format(positive_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the regular expression to capture the words hate or dislike or disapprove. Match but don't capture the words movie or concert. Match and capture anything appearing until the ..\n",
    "\n",
    "Find all matches of the regex in each element of sentiment_analysis.\n",
    "\n",
    "Assign them to negative_matches.\n",
    "\n",
    "Complete the .format() method to print out the results contained in negative_matches for each element in sentiment_analysi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regex that matches sentences with the optional words\n",
    "regex_negative = r\"(hate|dislike|disapprove).+?(?:movie|concert)\\s(.+?)\\.\"\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "\t# Find all matches of regex in tweet\n",
    "    negative_matches = re.findall(regex_negative, tweet)\n",
    "    # Complete format to print out the results\n",
    "    print(\"Negative comments found {}\".format(negative_matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backreferences\n",
    "## Parsing PDF files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our first contract is dated back to 2016. Particularly, the day 24 of the month 05.\n"
     ]
    }
   ],
   "source": [
    "# Write regex and scan contract to capture the dates described\n",
    "contract =\"Signed on 05/24/2016\"\n",
    "# Write regex and scan contract to capture the dates described\n",
    "# Write regex and scan contract to capture the dates described\n",
    "regex_dates = r\"Signed\\son\\s(\\d{2})/(\\d{2})/(\\d{4})\"\n",
    "dates = re.search(regex_dates, contract)\n",
    "\n",
    "# Assign to each key the corresponding match\n",
    "signature = {\n",
    "\t\"day\": dates.group(2),\n",
    "\t\"month\": dates.group(1),\n",
    "\t\"year\": dates.group(3)\n",
    "}\n",
    "# Complete the format method to print-out\n",
    "print(\"Our first contract is dated back to {data[year]}. Particularly, the day {data[day]} of the month {data[month]}.\".format(data=signature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that each capturing group is assigned a number according to its position in the regex. Only if you use .search() and .match(), you can use .group() to retrieve the groups.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in html_tags:\n",
    "    # Complete the regex and find if it matches a closed HTML tags\n",
    "    match_tag =  re.match(r\"<(\\w+)>.*?</\\1>\", string)\n",
    " \n",
    "    if match_tag:\n",
    "        # If it matches print the first group capture\n",
    "        print(\"Your tag {} is closed\".format(match_tag.group(1))) \n",
    "    else:\n",
    "        # If it doesn't match capture only the tag \n",
    "        notmatch_tag = re.match(r\"<(\\w+)>\", string)\n",
    "        # Print the first group capture\n",
    "        print(\"Close your {} tag!\".format(notmatch_tag.group(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backreferences are very helpful when you need to reuse part of the regex match inside the regex. Knowing when and how to use them will simplify many of your tasks!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the regex to match an elongated word\n",
    "regex_elongated = r\"\\w*(\\w)\\1\\w*\"\n",
    "\n",
    "for tweet in sentiment_analysis:\n",
    "\t# Find if there is a match in each tweet \n",
    "\tmatch_elongated = re.search(regex_elongated, tweet)\n",
    "    \n",
    "\tif match_elongated:\n",
    "\t\t# Assign the captured group zero \n",
    "\t\telongated_word = match_elongated.group(0)\n",
    "        \n",
    "\t\t# Complete the format method to print the word\n",
    "\t\tprint(\"Elongated word found: {word}\".format(word=elongated_word))\n",
    "\telse:\n",
    "\t\tprint(\"No elongated word found\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should remember that the group zero stands for the entire expression matched. It's always helpful to keep that in mind.**\n",
    "# Lookaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive lookahead\n",
    "look_ahead = re.findall(r\"\\w+(?=\\spython)\", sentiment_analysis)\n",
    "\n",
    "# Print out\n",
    "print(look_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive lookbehind\n",
    "look_behind = re.findall(r\"(?<=[Pp]ython\\s)\\w+\", sentiment_analysis)\n",
    "\n",
    "# Print out\n",
    "print(look_behind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering phone numbers\n",
    "\n",
    "Now, you need to write a script for a cell-phone searcher. It should scan a list of phone numbers and return those that meet certain characteristics.\n",
    "\n",
    "The phone numbers in the list have the structure:\n",
    "\n",
    "    Optional area code: 3 numbers\n",
    "    Prefix: 4 numbers\n",
    "    Line number: 6 numbers\n",
    "    Optional extension: 2 numbers\n",
    "\n",
    "E.g. 654-8764-439434-01.\n",
    "\n",
    "You decide to use .findall() and the non-capturing group's negative lookahead (?!) and negative lookbehind (?<!).\n",
    "\n",
    "The list cellphones, containing three phone numbers, and the re module are loaded in your session. You can use print() to view the data in the IPython Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phone in cellphones:\n",
    "\t# Get all phone numbers not preceded by area code\n",
    "\tnumber = re.findall(r\"(?<!\\d{3}-)\\d{4}-\\d{6}-\\d{2}\", phone)\n",
    "\tprint(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phone in cellphones:\n",
    "\t# Get all phone numbers not followed by optional extension\n",
    "\tnumber = re.findall(r\"\\d{3}-\\d{4}-\\d{6}(?!-\\d{2})\", phone)\n",
    "\tprint(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    ">>> my_string = \"Let's write RegEx!\"\n",
    "PATTERN = r\"\\w+\"\n",
    ">>> re.findall(PATTERN, my_string)\n",
    "# ['Let', 's', 'write', 'RegEx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be using more regex in this section as well, not only when you are tokenizing, but also figuring out how to parse tokens and text. Using the regex module's re.match and re.search are pretty essential tools for Python string processing. Learning when to use search versus match can be challenging, so let's take a look at how they are different. When we use search and match with the same pattern and string with the pattern is at the beginning of the string, we see we find identical matches. That is the case with matching and searching abcde with the pattern abc. When we use search for a pattern that appears later in the string we get a result, but we don't get the same result using match. This is because match will try and match a string from the beginning until it cannot match any longer. Search will go through the ENTIRE string to look for match options. If you need to find a pattern that might not be at the beginning of the string, you should use search. If you want to be specific about the composition of the entire string, or at least the initial pattern, then you should use match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1,scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w]+:\"\n",
    "print(re.match(pattern2,sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a tokenizer\n",
    "\n",
    "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"(\\w+|#\\d|\\?|!)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support@datacamp.com\n",
      "xyz@datacamp.com\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "statement = \"Please contact us at: support@datacamp.com, xyz@datacamp.com\"\n",
    "\n",
    "#'addresses' is a list that stores all the possible match\n",
    "addresses = re.finditer(r'[\\w\\.-]+@[\\w\\.-]+', statement)\n",
    "for address in addresses:\n",
    "    print(address.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'dogs']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall( r'all (.*?) are', 'all cats are smarter than dogs, all dogs are dumber than cats')\n",
    "# Output: ['cats', 'dogs']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all cats are', 'all dogs are']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.group() for x in re.finditer( r'all (.*?) are', 'all cats are smarter than dogs, all dogs are dumber than cats')]\n",
    "# Output: ['all cats are', 'all dogs are']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract List of Keywords :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Matcher \n",
    "### re.findall(pattern.string)\n",
    "\n",
    "findall() returns all non-overlapping matches of pattern in string as a list of strings.\n",
    "\n",
    "### re.finditer()\n",
    "\n",
    "finditer() returns callable object.\n",
    "\n",
    "In both functions, the string is scanned from left to right and matches are returned in order found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Total']\n",
      "[['Total'], ['Christophe de Margerie', 'Total']]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "class RawMatcher:\n",
    "\n",
    "    def __init__(self, keywords_list, ignore_case=True):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        keyword_list : list\n",
    "            list of keywords to be used for matching\n",
    "        ignore_case : bool, optional\n",
    "            Whether of not case should be ignored, by default True\n",
    "        \"\"\"\n",
    "\n",
    "        self.keywords_list = list(set(keywords_list))\n",
    "        self.keywords_patterns = self.create_patterns(ignore_case)\n",
    "\n",
    "    def create_patterns(self, ignore_case):\n",
    "        \"\"\"Create a regex pattern from list of keywords\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ignore_case : boleean\n",
    "            Boolean operator to specify case ignoring\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        re object\n",
    "            Compiled regex to be used for matching\n",
    "        \"\"\"\n",
    "        try:\n",
    "            keywords_patterns = re.compile(r'\\b(?:%s)\\b' % '|'.join(\n",
    "                self.keywords_list), re.IGNORECASE if ignore_case else 0)\n",
    "            return keywords_patterns\n",
    "        except re.error as e:\n",
    "            logging.error(f\"{e}\")\n",
    "\n",
    "    def batch_match(self, text_list):\n",
    "        \"\"\"Take a list of sentences or string and iterate over each of them\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_list : list\n",
    "            list of string\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[list]\n",
    "            2d list of matches\n",
    "\n",
    "        Example usage\n",
    "        -------------\n",
    "        >>> keywords = [\"Total\",\"Total S.A.\", \"Christophe de Margerie\", \"Ernest Mercier\", \"Elf Aquitaine\", \"Total SA\"]\n",
    "        >>> sentences = [\"This total entity sells gasoil\",\"Christophe de margerie was Total's CEO\"]\n",
    "        [['Christophe de margerie', 'Total'], ['total']]\n",
    "        \"\"\"\n",
    "        return [self.match(i) for i in text_list]\n",
    "\n",
    "    def match(self, text):\n",
    "        \"\"\"Get matches from string or sentence\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : string\n",
    "            sentence or string to be matched\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of matches\n",
    "        \"\"\"\n",
    "        matchings = [match.group() for match in self.keywords_patterns.finditer(text)]\n",
    "        return list(matchings)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  # pragma: no cover\n",
    "\n",
    "    keywords = [\"Total\", \"Total S.A.\", \"Christophe de Margerie\",\n",
    "                \"Ernest Mercier\", \"Elf Aquitaine\", \"Total SA\"]\n",
    "    sentences = [\"Total is a french oil company\", \"Christophe de Margerie was Total's CEO\"]\n",
    "    sentence = sentences[0]\n",
    "\n",
    "    c = RawMatcher(keywords)\n",
    "    batch_res = c.batch_match(sentences)\n",
    "    res = c.match(sentence)\n",
    "\n",
    "    print(res)\n",
    "    print(batch_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "## Text preprocessing steps:\n",
    "Lemmatization, lowercasing, removing unwanted tokens.\n",
    "###  Tokenization and Lemmatization\n",
    "####  Tokenizing Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text,capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Split the script holy_grail into lines using the newline ('\\n') character.\n",
    "    Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1. The pattern has been written for you.\n",
    "    Use a list comprehension to tokenize lines with regexp_tokenize(), keeping only words. Recall that the pattern for words is \"\\w+\".\n",
    "    Use a list comprehension to create a list of line lengths called line_num_words.\n",
    "        Use t_line as your iterator variable to iterate over tokenized_lines, and then len() function to compute line lengths.\n",
    "    Plot a histogram of line_num_words using plt.hist(). Don't forgot to use plt.show() as well to display the plot.\n",
    "\"\"\"\n",
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s,\"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops ]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Splitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "class SentenceSplitter:\n",
    "\n",
    "    def __init__(self, keys_map={\"text\": 0, \"title\": 1, \"thread.title\": 2}, keep_raw=False):\n",
    "        self.keys_map = keys_map\n",
    "        self.keep_text = keep_raw\n",
    "\n",
    "    def deep_get(self, dictionary, keys, default=None):\n",
    "        return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split(\".\"), dictionary)\n",
    "\n",
    "    def split_document(self, doc, id_key=\"_id\"):\n",
    "        sentences = []\n",
    "        doc_id = doc.get(id_key)\n",
    "        for key in self.keys_map.keys():\n",
    "            sentence = self.text_splitter(self.deep_get(\n",
    "                doc, key, ''), self.keys_map[key], doc_id, id_key)\n",
    "            sentences.extend(sentence)\n",
    "\n",
    "        return {\"sentences\": sentences, \"nb_sentences\": len(sentences)}\n",
    "\n",
    "    def text_splitter(self, text, type_key, doc_id, id_key):\n",
    "        index_splitted_text = [x + (type_key, doc_id)for x in list(\n",
    "            enumerate(nltk.sent_tokenize(text), 0))]\n",
    "        keys_list = [\"sentence_id\", \"text\", \"type\",  id_key]\n",
    "        splitted_text = [dict(zip(keys_list, x)) for x in index_splitted_text]\n",
    "        return splitted_text\n",
    "\n",
    "    def batch_spliting(self, document_list, id_key=\"_id\"):\n",
    "        return list(map(lambda d: self.split_document(d, id_key), document_list))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import jsonlines\n",
    "    import json\n",
    "    datapath = \"sentence_splitter/example.json\"\n",
    "    with jsonlines.open(datapath) as reader:\n",
    "        lines = [*reader]\n",
    "    test_data = lines[28:30]\n",
    "    s = SentenceSplitter()\n",
    "    res = s.batch_spliting(test_data,'id')\n",
    "    print(json.dumps(res, indent=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class Aggregator():\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        Parent Constructor\n",
    "\n",
    "        :param text: text to evaluate\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.sentences = nltk.sent_tokenize(self.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Agregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "from aggregator.baseaggregator import Aggregator\n",
    "\n",
    "import operator\n",
    "\n",
    "class SentimentAggregator(Aggregator):\n",
    "    _persist_methods = ['mean', 'max', 'sum']\n",
    "\n",
    "    def __init__(self, sentences, language, learners):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        :param sentences: sentence-tokenized text\n",
    "        :param language: language of the corpus\n",
    "        :param learners: model for sentiment analysis\n",
    "        \"\"\"\n",
    "        super().__init__(sentences)\n",
    "        self.language = language\n",
    "        self.learners = learners\n",
    "        self.summation_score = 0\n",
    "        self.polarity_score = 0\n",
    "        self.subjectivity_score = 0\n",
    "        self.bermingham_score = 0\n",
    "\n",
    "    def compute_polarity_score(self, method, value=None):\n",
    "        \"\"\"\n",
    "        Compute the sentiment score of a text based on the polarity of each sentence\n",
    "\n",
    "        :param method: method of aggregation ie mean, sum , max, quantile\n",
    "        :param value: if quantile choosen, value of the quantile between 0 and 1\n",
    "\n",
    "        :returns: The Sentiment score of the text\n",
    "        \"\"\"\n",
    "        final_res = []\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            d_result = self.get_sentence_predictions(sentence)\n",
    "            score = self.get_sentence_polarity_score(d_result['webhose_valence3_single_positive'], d_result['webhose_valence3_single_negative'])\n",
    "            final_res.append(score)\n",
    "        \n",
    "        if method == \"quantile\":\n",
    "            self.polarity_score = np.quantile(np.array(final_res), value)\n",
    "            return self.polarity_score\n",
    "        elif method in self._persist_methods:\n",
    "            self.polarity_score = getattr(np.array(final_res),method)()\n",
    "            return self.polarity_score\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sentence_polarity_score(positive_probability, negative_probability):\n",
    "        \"\"\"\n",
    "        Get the sentence score based on the compute of the polarity ie (positive_probability - negative_probability)\n",
    "\n",
    "        :param positive_probability: Positive probability returned by the sentiment classifier\n",
    "        :param negative_probability: Negative probability returned by the sentiment classifier\n",
    "\n",
    "        :returns: The sentence sentiment score \n",
    "        \"\"\"\n",
    "        polarity = positive_probability - negative_probability\n",
    "        sentence_score = 100 / (1+ np.exp(-polarity))\n",
    "        return sentence_score\n",
    "    \n",
    "    def compute_subjectivity_score(self, method, value=None):\n",
    "        \"\"\"\n",
    "        Compute the sentiment score of a text based on the subjectivity of each sentence\n",
    "\n",
    "        :param method: method of aggregation ie mean, sum , max, quantile\n",
    "        :param value: if quantile choosen, value of the quantile between 0 and 1\n",
    "\n",
    "        :returns: Sentiment score of the text\n",
    "        \"\"\"\n",
    "        final_res = []\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            d_result = self.get_sentence_predictions(sentence)\n",
    "            score = self.get_sentence_subjectivity_score(d_result['webhose_valence3_single_positive'], d_result['webhose_valence3_single_negative'])\n",
    "            final_res.append(score)\n",
    "        \n",
    "        if method == \"quantile\":\n",
    "            self.subjectivity_score = np.quantile(np.array(final_res), value)\n",
    "            return self.subjectivity_score\n",
    "        elif method in self._persist_methods:\n",
    "            self.subjectivity_score = getattr(np.array(final_res),method)()\n",
    "            return self.subjectivity_score\n",
    " \n",
    "    @staticmethod\n",
    "    def get_sentence_subjectivity_score(positive_probability, negative_probability):\n",
    "        \"\"\"\n",
    "        Get the sentence score based on the compute of the subjectivity ie (1 - neutral)\n",
    "\n",
    "        :param positive_probability: Positive probability returned by the sentiment classifier\n",
    "        :param negative_probability: Negative probability returned by the sentiment classifier\n",
    "\n",
    "        :returns : The sentence sentiment score\n",
    "        \"\"\"\n",
    "        subjectivity = positive_probability + negative_probability\n",
    "        sentence_score = 100 / (1+ np.exp(-subjectivity))\n",
    "        return sentence_score\n",
    "\n",
    "    def compute_bermingham_score(self, method, value=None):\n",
    "        \"\"\"\n",
    "        Compute the sentiment score of a text based on sentiment ratio of each sentence\n",
    "\n",
    "        :param method: method of aggregation ie mean, sum , max, quantile\n",
    "        :param value: if quantile choosen, value of the quantile between 0 and 1\n",
    "\n",
    "        :returns: Sentiment score of the text\n",
    "        \"\"\"\n",
    "        final_res = []\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            d_result = self.get_sentence_predictions(sentence)\n",
    "            score = self.get_sentence_ratio_score(d_result['webhose_valence3_single_positive'], d_result['webhose_valence3_single_negative'])\n",
    "            final_res.append(score)\n",
    "\n",
    "        if method == \"quantile\":\n",
    "            self.bermingham_score = np.quantile(np.array(final_res), value)\n",
    "            return self.bermingham_score\n",
    "        elif method in self._persist_methods:\n",
    "            self.bermingham_score = getattr(np.array(final_res),method)()\n",
    "            return self.bermingham_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_sentence_ratio_score(positive_probability, negative_probability):\n",
    "        \"\"\"\n",
    "        Get the sentence score based on the ratio ie (positive_probability/negative_probability)\n",
    "\n",
    "        :param positive_probability: Positive probability returned by the sentiment classifier\n",
    "        :param negative_probability: Negative probability returned by the sentiment classifier\n",
    "\n",
    "        :returns : The sentence sentiment score  \n",
    "        \"\"\"\n",
    "        sentiment_ratio = (positive_probability + 1) / (negative_probability + 1)\n",
    "        sentence_score = 100 * np.log(sentiment_ratio)\n",
    "        return sentence_score\n",
    "\n",
    "    def compute_summation_score(self):\n",
    "        \"\"\"\n",
    "        Compute the sentiment score of a text based on the difference of total positives and total negatives sentences\n",
    "\n",
    "        :returns: Sentiment score of the text\n",
    "        \"\"\"\n",
    "        for sentence in self.sentences:\n",
    "            d_result = self.get_sentence_predictions(sentence)\n",
    "            try:\n",
    "                d_result.pop('webhose_valence3_single_neutral')\n",
    "            except KeyError:\n",
    "                print(\"Key not found\") \n",
    "            res = list(d_result.keys())[0] \n",
    "            if res == 'webhose_valence3_single_positive':\n",
    "                self.summation_score += 1\n",
    "            elif res == 'webhose_valence3_single_negative':\n",
    "                self.summation_score -= 1\n",
    "        self.summation_score = (100 * self.summation_score/len(self.sentences))\n",
    "        return self.summation_score\n",
    "    \n",
    "    def get_sentence_predictions(self, sentence):\n",
    "        \"\"\"\n",
    "        Get the predictions performed by the sentiment classifier model on one sentence\n",
    "\n",
    "        :param sentence: The sentence to perform the sentiment analysis on\n",
    "\n",
    "        :returns: A dictionary of three probabilities: positive (webhose_valence3_single_positive), neutral (webhose_valence3_single_neutral) \n",
    "                  and negative (webhose_valence3_single_negative)\n",
    "        \"\"\"\n",
    "        result = self.learners.predict_sentence_affect(sentence, \n",
    "                                                       models=['news-sentiment'], \n",
    "                                                       language=self.language, \n",
    "                                                       models_preloaded=True)\n",
    "        d_result = defaultdict(int, result)\n",
    "        d_result = {k: v for k, v in sorted(d_result.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return d_result\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enity Agregator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "from aggregator.baseaggregator import Aggregator\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "class EntityAggregator(Aggregator):\n",
    "    def __init__(self, sentences, entity_of_interest, ranker):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        :param sentences: sentence-tokenized text\n",
    "        :param entity_of_interest: the entity that we are interested in ie. Q312\n",
    "        :param ranker: Entity Ranker object\n",
    "        \"\"\"\n",
    "        super().__init__(sentences)\n",
    "        self.entity_of_interest = entity_of_interest\n",
    "        self.ranker = ranker\n",
    "        self.results = np.array(self.ranker.bulk_filter_queries(self.sentences))\n",
    "        self.half_elements = len(self.results)/2\n",
    "\n",
    "    def compute_mean_score(self, noise_reducer=.5):\n",
    "        \"\"\"\n",
    "        Compute the sentence entity ranker based on mean of sentences\n",
    "\n",
    "        :param noise_reducer: thd / Slider to evaluate the aggregation method\n",
    "\n",
    "        :returns: Bool(mean_sentences_entity_of_interest > noise_reducer) \n",
    "        \"\"\"\n",
    "        mean_score = self.get_entity_of_interest_mean(self.entity_of_interest, self.results)\n",
    "        return (mean_score > noise_reducer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_entity_of_interest_mean(entity_of_interest, list_of_entities):\n",
    "        \"\"\"\n",
    "        Compute the mean of all sentences linked to our entity of interest \n",
    "\n",
    "        :param list_of_entities: List of entities \n",
    "\n",
    "        :returns: The mean of all sentences that talks about the entity of interest\n",
    "        \"\"\"\n",
    "        return (np.array(list_of_entities)==entity_of_interest).mean()\n",
    "\n",
    "    def compute_majority_voting_score(self):\n",
    "        \"\"\"\n",
    "        Compute the sentence entity ranker based on Majority Voting of sentences\n",
    "\n",
    "        :returns: Bool(entity_of_interest is majority) \n",
    "        \"\"\"\n",
    "        my_value = self.get_num_sentences(self.entity_of_interest, self.results)\n",
    "        return (my_value > self.half_elements)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_num_sentences(entity_of_interest, list_of_entities):\n",
    "        \"\"\"\n",
    "        Count the number of sentences that are linked with the entity of interest\n",
    "\n",
    "        :param list_of_entities: List of entities\n",
    "\n",
    "        :returns: The number of sentences that are linked with the entity of interest\n",
    "        \"\"\"\n",
    "        counter = collections.Counter(np.array(list_of_entities))\n",
    "        sorted_counter = {k: v for k, v in sorted(counter.items(), key=lambda item: item[1], reverse=True)}\n",
    "        my_value = sorted_counter.get(entity_of_interest)\n",
    "        if isinstance(my_value, type(None)):\n",
    "            my_value = 0\n",
    "        return my_value\n",
    "    \n",
    "    def compute_top_n_score(self, n=1):\n",
    "        \"\"\"\n",
    "        Compute the sentence entity ranker based on top n entities by score\n",
    "\n",
    "        :param n: Top n entities to retrieve\n",
    "\n",
    "        :returns: Bool(entity_of_interest is in top n)\n",
    "        \"\"\"\n",
    "        top_entities = self.get_top_entities(self.results, n)\n",
    "        return (self.entity_of_interest in top_entities)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_top_entities(list_of_entities, n=1):\n",
    "        \"\"\"\n",
    "        Get top n entities by score. \n",
    "\n",
    "        :param list_of_entities: List of entities\n",
    "        :param n: Top n entities to retrieve\n",
    "\n",
    "        :returns: A list containing ordered top n entities\n",
    "        \"\"\" \n",
    "        counter = collections.Counter(list_of_entities)\n",
    "        top = sorted(counter, key=counter.get, reverse=True)[:n]\n",
    "        return top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove html tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic feature extraction\n",
    "## Character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Your text here')\n",
    "# all tokens that arent stop words or punctuations\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "nouns = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"NOUN\"]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "\n",
    "# five most common noun tokens\n",
    "noun_freq = Counter(nouns)\n",
    "common_nouns = noun_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words =string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "print(article)\n",
    "sentences =sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy NER Categories vs NLTK \n",
    "\n",
    "Which are the extra categories that spacy uses compared to nltk in its named-entity recognition?\n",
    "\n",
    "**NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_persons(text):\n",
    "    # Create Doc object\n",
    "    doc = nlp(text)\n",
    "  \n",
    "    # Identify the persons\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "    # Return persons\n",
    "    return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual NER with polyglot\n",
    "You have access to the full article string in article. Additionally, the Text class of polyglot has been imported from polyglot.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You have access to the full article string in article. \n",
    "Additionally, the Text class of polyglot has been imported from polyglot.text.\"\"\"\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt =  Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities :\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if 'Márquez'in ent or  'Gabo' in ent:\n",
    "        # Increment count\n",
    "        count+=1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Counting nouns in a piece of text\n",
    "### proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "import numpy as np\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proper_nouns/Noun usage in fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractions Url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "class TextCleaner:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url_pattern = re.compile(r'(https?://[^\\s]+)|^.*(\\\\\\\\?.*)$')\n",
    "\n",
    "    def count_urls(self, text):\n",
    "        \"\"\" count urls tags containing hypertext links\n",
    "        \"\"\"\n",
    "        urls = self.url_pattern.findall(text)\n",
    "        return len(urls)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove html tags containing hypertext links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Filter_html:\n",
    "    \"\"\" remove html tags containing hypertext links\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.re_http = re.Regex('<[^>]*http[^>]*>')\n",
    "        self.re_end_tag = re.Regex('</[^>]*>')\n",
    "\n",
    "    def filter(self, text):\n",
    "        t = self.re_http.sub('',text)\n",
    "        return self.re_end_tag.sub('', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a bag of words model\n",
    "## BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer= CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping feature indices with feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer ()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow,y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow,y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an instance of MultinomialNB \n",
    "clf_ng =MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng ,y_train )\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng,y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing performance of n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "\n",
    "vectorizer= TfidfVectorizer()\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity matrix of a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim =cosine_similarity (tfidf_matrix,tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing linear_kernel and cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The recommender function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond n-grams: word embeddings\n",
    "## Generating word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying fake news using supervised learning with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df['label']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"],y,test_size=0.33,random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer =CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\" , max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test =tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = nb_classifier =MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train ,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test,pred, labels=['FAKE','REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier =MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train ,y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiemnt Analysis 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many positive and negative reviews are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of positive and negative reviews\n",
    "print('Number of positive and negative reviews: ', movies.label.value_counts())\n",
    "\n",
    "# Find the proportion of positive and negative reviews\n",
    "print('Proportion of positive and negative reviews: ', movies.label.value_counts()/ len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The .value_counts() method is an easy way to gain a first impression about the contents of the label column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longest and shortest reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_reviews = movies.review.str.len()\n",
    "# How long is the longest review\n",
    "print(max(length_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_reviews = movies.review.str.len()\n",
    "# How long is the shortest review\n",
    "print(min(length_reviews))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting the sentiment of Tale of Two Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object  \n",
    "blob_two_cities = TextBlob(two_cities)\n",
    "\n",
    "# Print out the sentiment \n",
    "print(blob_two_cities.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import  TextBlob\n",
    "\n",
    "# Create a textblob object \n",
    "blob_annak =  TextBlob(annak)\n",
    "blob_catcher = TextBlob(catcher)\n",
    "\n",
    "# Print out the sentiment   \n",
    "print('Sentiment of annak: ',blob_annak.sentiment)\n",
    "print('Sentiment of catcher: ', blob_catcher.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the sentiment of a movie review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import  TextBlob\n",
    "# Create a textblob object  \n",
    "blob_titanic = TextBlob(titanic)\n",
    "\n",
    "# Print out its sentiment  \n",
    "print(blob_titanic.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your first word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the word cloud from the east_of_eden string\n",
    "cloud_east_of_eden = WordCloud(background_color=\"white\").generate(east_of_eden)\n",
    "\n",
    "# Create a figure of the generated cloud\n",
    "plt.imshow(cloud_east_of_eden, interpolation='bilinear')  \n",
    "plt.axis('off')\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which words are in the word cloud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function  \n",
    "from wordcloud import WordCloud\n",
    "# Create and generate a word cloud image \n",
    "my_cloud =  WordCloud(background_color='white', stopwords=my_stopwords).generate(descriptions)\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting granular with n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(min_df=50)\n",
    "vect.fit(movies.review)\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "#Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] =len_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the language detection function and package\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# Detect the language of the foreign string\n",
    "print(detect_langs(foreign))\n",
    "--------------------------------------------------------------------------------------\n",
    "from langdetect import detect_langs\n",
    "\n",
    "languages = []\n",
    "\n",
    "# Loop over the sentences in the list and detect their language\n",
    "for sentence in range(len(sentences)):\n",
    "    languages.append(detect_langs(sentences[sentence]))\n",
    "    \n",
    "print('The detected languages are: ', languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(non_english_reviews)):\n",
    "    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] =languages\n",
    "\n",
    "print(non_english_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function and stop words list\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# Define and update the list of stopwords\n",
    "my_stop_words = STOPWORDS.update(['airline', 'airplane'])\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(stopwords=my_stop_words).generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer,ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(tweets.text)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(tweets.text)\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
    "vect.transform(tweets.text)\n",
    "print('Length of vectorizer: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize the text column\n",
    "word_tokens = [word_tokenize(review) for review in tweets.text]\n",
    "print('Original tokens: ', word_tokens[0])\n",
    "\n",
    "# Filter out non-letter characters\n",
    "cleaned_tokens = [[word for word in item if word.isalpha()] for item in word_tokens]\n",
    "print('Cleaned tokens: ', cleaned_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String operators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists, containing the tokens from list_tweets\n",
    "tokens = [word_tokenize(item) for item in tweets_list]\n",
    "# Remove characters and digits , i.e. retain only letters\n",
    "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
    "# Remove characters, i.e. retain only letters and digits\n",
    "let_digits = [[word for word in item if word.isalnum()] for item in tokens]\n",
    "# Remove letters and characters, retain only digits\n",
    "digits = [[word for word in item if word.isdigit()] for item in tokens]\n",
    "# Print the last item in each list\n",
    "print('Last item in alphabetic list: ', letters[2])\n",
    "print('Last item in list of alphanumerics: ', let_digits[2])\n",
    "print('Last item in the list of digits: ', digits[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stems and lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages from nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "porter = PorterStemmer()\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "# Tokenize the GoT string\n",
    "tokens = word_tokenize(GoT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a stemmed list\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens] \n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for stemming in seconds: ', end_time - start_time)\n",
    "print('Stemmed tokens: ', stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a lemmatized list\n",
    "lem_tokens = [WNlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for lemmatizing in seconds: ', end_time - start_time)\n",
    "print('Lemmatized tokens: ', lem_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import the Spanish SnowballStemmer\n",
    "SpanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Create a list of tokens\n",
    "tokens = [word_tokenize(review) for review in non_english_reviews.review] \n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[SpanishStemmer.stem(word) for word in token] for token in tokens]\n",
    "\n",
    "# Print the first item of the stemmed tokenss\n",
    "stemmed_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required vectorizer package and stop words list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "# Define the vectorizer and specify the arguments\n",
    "my_pattern = r'\\b[^\\d\\W][^\\d\\W]+\\b'\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), max_features=100, token_pattern=my_pattern, stop_words=ENGLISH_STOP_WORDS).fit(tweets.text)\n",
    "# Transform the vectorizer\n",
    "X_txt = vect.transform(tweets.text)\n",
    "# Transform to a data frame and specify the column names\n",
    "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: ', X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "# Build a BOW and tfidf vectorizers from the review column and with max of 100 features\n",
    "vect1 = CountVectorizer(max_features=100).fit(reviews.review)\n",
    "vect2 = TfidfVectorizer(max_features=100).fit(reviews.review) \n",
    "# Transform the vectorizers\n",
    "X1 = vect1.transform(reviews.review)\n",
    "X2 = vect2.transform(reviews.review)\n",
    "# Create DataFrames from the vectorizers \n",
    "X_df1 = pd.DataFrame(X1.toarray(), columns=vect1.get_feature_names())\n",
    "X_df2 = pd.DataFrame(X2.toarray(), columns=vect2.get_feature_names())\n",
    "print('Top 5 rows using BOW: \\n', X_df1.head())\n",
    "print('Top 5 rows using tfidf: \\n', X_df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Define the vector of targets and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did we really predict the sentiment well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define the vector of labels and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Build a logistic regression model and print out the accuracy\n",
    "log_reg = LogisticRegression().fit(X_train,y_train)\n",
    "print('Accuracy on train set: ', log_reg.score(X_train, y_train))\n",
    "print('Accuracy on test set: ', log_reg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123,stratify=y)\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train,y_train )\n",
    "# Make predictions on the test set\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "# Print the performance metrics\n",
    "print('Accuracy score test set: ', accuracy_score(y_test, y_predicted))\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_predicted)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build and assess a model: product reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the probability of the 0 class\n",
    "prob_0 = log_reg.predict_proba(X_test)[:, 0]\n",
    "# Predict the probability of the 1 class\n",
    "prob_1 = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"First 10 predicted probabilities of class 0: \", prob_0[:10])\n",
    "print(\"First 10 predicted probabilities of class 1: \", prob_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product reviews with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Train a logistic regression with regularization of 1000\n",
    "log_reg1 = LogisticRegression(C=1000).fit(X_train, y_train)\n",
    "# Train a logistic regression with regularization of 0.001\n",
    "log_reg2 = LogisticRegression(C=0.001).fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracies\n",
    "print('Accuracy of model 1: ', log_reg1.score(X_test,y_test))\n",
    "print('Accuracy of model 2: ', log_reg2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for Language Modeling in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its my job\\tÃ© o meu trabalho</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wholl cook\\tquem cozinharÃ¡</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>help me\\tajudemme</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i sat down\\teu me sentei</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im worn out\\testou exausto</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>call me later\\tme liga depois</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>look alive\\tse apresse</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>id buy that\\teu compraria aquele</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>i have won\\teu venci</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>youre old\\tvocÃª estÃ¡ velho</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0    1\n",
       "0        its my job\\tÃ© o meu trabalho  NaN\n",
       "1          wholl cook\\tquem cozinharÃ¡  NaN\n",
       "2                    help me\\tajudemme  NaN\n",
       "3             i sat down\\teu me sentei  NaN\n",
       "4           im worn out\\testou exausto  NaN\n",
       "...                                ...  ...\n",
       "4995     call me later\\tme liga depois  NaN\n",
       "4996            look alive\\tse apresse  NaN\n",
       "4997  id buy that\\teu compraria aquele  NaN\n",
       "4998              i have won\\teu venci  NaN\n",
       "4999      youre old\\tvocÃª estÃ¡ velho  NaN\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Data :\n",
    "import pandas as pd\n",
    "\n",
    "sheldon_quotes=pd.read_fwf('/home/abderrazak/ALLINHERE/NLP/Datacamp/sampleDlNlp.txt', header=None)\n",
    "sheldon_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '', 1: 'a', 2: 'abaixado', 3: 'abaixados', 4: 'abandon', 5: 'abenÃ§oe', 6: 'aberto', 7: 'aboard\\ttodos', 8: 'about', 9: 'above\\tvejam', 10: 'abrace', 11: 'abraÃ§ar', 12: 'abraÃ§aram', 13: 'abraÃ§o', 14: 'abraÃ§ou', 15: 'abraÃ§Ã¡lo', 16: 'abrir', 17: 'absurd\\tque', 18: 'absurdo', 19: 'acabou', 20: 'acalma', 21: 'acalme', 22: 'acalmese', 23: 'acenei', 24: 'acenou', 25: 'achamos', 26: 'achar', 27: 'achei', 28: 'acho', 29: 'acidente', 30: 'acima', 31: 'acontece', 32: 'acontecer', 33: 'acorda', 34: 'acordada', 35: 'acordado', 36: 'acordados', 37: 'acorde', 38: 'acordei', 39: 'acordo', 40: 'acordou', 41: 'acredito', 42: 'act', 43: 'act\\tnÃ³s', 44: 'action\\ttome', 45: 'action\\ttomem', 46: 'actor\\tsou', 47: 'addicted\\testou', 48: 'adentro', 49: 'adeus', 50: 'adiantada', 51: 'adiantado', 52: 'adiantados', 53: 'adiante', 54: 'admire', 55: 'admiro', 56: 'admit', 57: 'admito', 58: 'adoeceu', 59: 'adopted\\teu', 60: 'adora', 61: 'adorable\\tele', 62: 'adormeceu', 63: 'adormecido', 64: 'adoro', 65: 'adorÃ¡vel', 66: 'adotado', 67: 'adult\\teu', 68: 'adulta', 69: 'adulto', 70: 'adultos', 71: 'adults\\tnÃ³s', 72: 'afastado', 73: 'afaste', 74: 'afastou', 75: 'afastouse', 76: 'afogando', 77: 'afogou', 78: 'afraid\\testamos', 79: 'afraid\\testou', 80: 'afraid\\teu', 81: 'afraid\\ttom', 82: 'after', 83: 'again\\tcomece', 84: 'again\\tcomecem', 85: 'again\\tcomeÃ§a', 86: 'again\\tconfira', 87: 'again\\teu', 88: 'again\\tfaz', 89: 'again\\tfaÃ§a', 90: 'again\\tmais', 91: 'again\\tobrigado', 92: 'again\\tolhe', 93: 'again\\toutra', 94: 'again\\tpensa', 95: 'again\\tpense', 96: 'again\\tpergunte', 97: 'again\\trepita', 98: 'again\\ttenta', 99: 'again\\ttente', 100: 'again\\ttentem', 101: 'again\\tverifique', 102: 'age\\taja', 103: 'age\\tele', 104: 'age\\ttenho', 105: 'agir', 106: 'agora', 107: 'agradÃ¡vel', 108: 'agree\\tconcordam', 109: 'agree\\tconcordamos', 110: 'agree\\telas', 111: 'agree\\teles', 112: 'agree\\testou', 113: 'agree\\testÃ¡', 114: 'agree\\teu', 115: 'agree\\tnÃ£o', 116: 'agree\\tnÃ³s', 117: 'agree\\tvocÃª', 118: 'agree\\tvocÃªs', 119: 'agreed\\tconcordamos', 120: 'agreed\\teu', 121: 'agreed\\tnÃ³s', 122: 'agreed\\ttom', 123: 'agrees\\ttom', 124: 'agressivo', 125: 'agricultor', 126: 'ahead', 127: 'ahead\\tcontinua', 128: 'ahead\\tcontinue', 129: 'ahead\\tcontinuem', 130: 'ahead\\tvÃ¡', 131: 'aim', 132: 'ainda', 133: 'aint', 134: 'air\\teu', 135: 'ajoelhou', 136: 'ajuda', 137: 'ajudar', 138: 'ajudaremos', 139: 'ajudarÃ¡', 140: 'ajude', 141: 'ajudei', 142: 'ajudou', 143: 'alce', 144: 'alert\\tse', 145: 'alerta', 146: 'algum', 147: 'alguma', 148: 'algumas', 149: 'alguns', 150: 'alguÃ©m', 151: 'ali', 152: 'alimento', 153: 'alistou', 154: 'alive\\tande', 155: 'alive\\tela', 156: 'alive\\tele', 157: 'alive\\testou', 158: 'alive\\to', 159: 'alive\\tse', 160: 'alive\\tsintome', 161: 'alive\\ttom', 162: 'all', 163: 'all\\tisso', 164: 'all\\tleve', 165: 'all\\tÃ©', 166: 'almost', 167: 'alone\\tdeixame', 168: 'alone\\tele', 169: 'alone\\testamos', 170: 'alone\\testou', 171: 'alone\\teu', 172: 'alone\\tme', 173: 'alone\\tnÃ³s', 174: 'alone\\to', 175: 'alone\\ttom', 176: 'alone\\tvenha', 177: 'alone\\tvocÃª', 178: 'along\\tcante', 179: 'along\\tvem', 180: 'along\\tvenha', 181: 'already', 182: 'also', 183: 'alta', 184: 'altas', 185: 'alto', 186: 'altos', 187: 'always', 188: 'am', 189: 'am\\taqui', 190: 'am\\tsou', 191: 'ama', 192: 'amada', 193: 'amado', 194: 'amam', 195: 'amamos', 196: 'amando', 197: 'amanhÃ£', 198: 'amava', 199: 'amazed', 200: 'amazing\\tisso', 201: 'ambas', 202: 'ambos', 203: 'amei', 204: 'american\\teu', 205: 'americano', 206: 'amigÃ¡veis', 207: 'amigÃ¡vel', 208: 'amo', 209: 'amor', 210: 'amuse', 211: 'amÃ¡vamos', 212: 'amÃ¡vel', 213: 'an', 214: 'and', 215: 'anda', 216: 'andando', 217: 'andar', 218: 'ando', 219: 'angry\\tele', 220: 'angry\\testou', 221: 'angry\\teu', 222: 'angry\\tnÃ³s', 223: 'angry\\ttom', 224: 'annoying\\tque', 225: 'anos', 226: 'another\\ttoma', 227: 'another\\ttome', 228: 'another\\ttomem', 229: 'answer', 230: 'answered\\ttom', 231: 'any', 232: 'anybody', 233: 'anybody\\tpergunta', 234: 'anybody\\tpergunte', 235: 'anybody\\tperguntem', 236: 'anyone', 237: 'anything', 238: 'anytime\\tvenha', 239: 'anyway\\tfaÃ§a', 240: 'ao', 241: 'aparelho', 242: 'aplaudiram', 243: 'aplaudiu', 244: 'apologize\\tdesculpamos', 245: 'apologize\\tnÃ³s', 246: 'apologize\\tperdÃ£o', 247: 'apologize\\tpeÃ§o', 248: 'apontar', 249: 'aposentado', 250: 'aposentou', 251: 'aposta', 252: 'apples\\teu', 253: 'approve\\taprovam', 254: 'approve\\telas', 255: 'approve\\teles', 256: 'approved\\to', 257: 'approves\\ttom', 258: 'aprender', 259: 'aprenderei', 260: 'apressa', 261: 'apressar', 262: 'apresse', 263: 'apressemse', 264: 'apressese', 265: 'aprova', 266: 'aprovam', 267: 'aprovou', 268: 'aquecido', 269: 'aquela', 270: 'aquelas', 271: 'aquele', 272: 'aqueles', 273: 'aqui', 274: 'aquilo', 275: 'ar', 276: 'arabs\\tsomos', 277: 'are', 278: 'are\\taqui', 279: 'are\\teisnos', 280: 'are\\testamos', 281: 'areia', 282: 'arfou', 283: 'argue\\tnÃ£o', 284: 'arguing\\tpare', 285: 'arm', 286: 'armadilh', 287: 'armadilha', 288: 'armados', 289: 'armed\\tnÃ³s', 290: 'around\\tdÃ¡', 291: 'around\\tdÃª', 292: 'around\\tolhe', 293: 'around\\tpergunte', 294: 'around\\tperguntem', 295: 'arrebenta', 296: 'arrependo', 297: 'arrisque', 298: 'arrived\\ttom', 299: 'arrogant\\tque', 300: 'arrogante', 301: 'arrotei', 302: 'arrotou', 303: 'arroz', 304: 'arrumado', 305: 'art\\tisto', 306: 'arte', 307: 'as', 308: 'aside\\tafastese', 309: 'aside\\tse', 310: 'ask', 311: 'ask\\teu', 312: 'ask\\tnÃ£o', 313: 'asked\\tfaÃ§a', 314: 'asked\\tninguÃ©m', 315: 'asleep\\tele', 316: 'asleep\\testÃ¡', 317: 'asleep\\teu', 318: 'asleep\\to', 319: 'asleep\\ttom', 320: 'assim', 321: 'assinou', 322: 'assobiei', 323: 'assobio', 324: 'assobiou', 325: 'assusta', 326: 'assustado', 327: 'assustador', 328: 'assustam', 329: 'assustei', 330: 'at', 331: 'atacar', 332: 'atacaremos', 333: 'ate', 334: 'ate\\tacabei', 335: 'ate\\teu', 336: 'ate\\to', 337: 'ate\\tquem', 338: 'ate\\ttom', 339: 'atenÃ§Ã£o', 340: 'athletic\\teu', 341: 'athletic\\tsou', 342: 'atingido', 343: 'atirar', 344: 'atire', 345: 'atirem', 346: 'atirou', 347: 'atitude', 348: 'atlÃ©tico', 349: 'ator', 350: 'atrasado', 351: 'atrasados', 352: 'atrasar', 353: 'atrase', 354: 'attack\\tatacar', 355: 'attack\\tatacaremos', 356: 'attack\\tataque', 357: 'attack\\tataquem', 358: 'attack\\tiremos', 359: 'attack\\tnÃ³s', 360: 'attack\\tvamos', 361: 'atÃ©', 362: 'aulas', 363: 'autista', 364: 'autistic\\teu', 365: 'autistic\\tsou', 366: 'automÃ³vel', 367: 'aventure', 368: 'avoids', 369: 'awake\\testamos', 370: 'awake\\testou', 371: 'awake\\teu', 372: 'awake\\tfique', 373: 'awake\\tnÃ³s', 374: 'awake\\to', 375: 'awake\\tvocÃª', 376: 'away', 377: 'away\\tafastese', 378: 'away\\tcai', 379: 'away\\tele', 380: 'away\\tfique', 381: 'away\\tfora', 382: 'away\\tmantenhase', 383: 'away\\tnÃ£o', 384: 'away\\tquem', 385: 'away\\tse', 386: 'away\\ttire', 387: 'away\\ttom', 388: 'away\\tvÃ¡', 389: 'awesome\\tÃ©', 390: 'awful\\tque', 391: 'awkward\\tÃ©', 392: 'azul', 393: 'aÃ§Ãºcar', 394: 'aÃ\\xad', 395: 'babando', 396: 'back', 397: 'back\\tafastese', 398: 'back\\tbemvindo', 399: 'back\\telas', 400: 'back\\teles', 401: 'back\\teu', 402: 'back\\tfique', 403: 'back\\tolha', 404: 'back\\tolhe', 405: 'back\\tolhem', 406: 'back\\tpara', 407: 'back\\ttraga', 408: 'back\\ttragam', 409: 'back\\tvolte', 410: 'back\\tvoltei', 411: 'back\\tvoltem', 412: 'backup\\ttraga', 413: 'backup\\ttragam', 414: 'bad\\teles', 415: 'bad\\teu', 416: 'bad\\tisso', 417: 'bad\\tnos', 418: 'bad\\ttom', 419: 'bad\\tvocÃª', 420: 'bad\\tÃ©', 421: 'bag\\tpreciso', 422: 'bagunÃ§a', 423: 'baixo', 424: 'baker\\teu', 425: 'bald\\testou', 426: 'bald\\teu', 427: 'bald\\tnÃ£o', 428: 'bald\\to', 429: 'bald\\tsou', 430: 'banquete', 431: 'barba', 432: 'barbaric\\tque', 433: 'barbeou', 434: 'barbeouse', 435: 'bark\\tcachorros', 436: 'bark\\tcÃ£es', 437: 'bastante', 438: 'bastarÃ¡', 439: 'bateram', 440: 'bateu', 441: 'be', 442: 'be\\timpossÃ\\xadvel', 443: 'be\\tmentira', 444: 'be\\tnÃ£o', 445: 'beans\\teu', 446: 'beat', 447: 'beautiful\\tque', 448: 'beba', 449: 'bebe', 450: 'bebem', 451: 'bebi', 452: 'bebia', 453: 'bebo', 454: 'bed\\testou', 455: 'beef', 456: 'beef\\teu', 457: 'beer', 458: 'beer\\tcerveja', 459: 'beer\\teu', 460: 'beer\\tpega', 461: 'beer\\ttoma', 462: 'beer\\tvÃ¡', 463: 'begin\\tcomecemos', 464: 'begin\\tpodemos', 465: 'begin\\tposso', 466: 'begin\\tvamos', 467: 'begun\\tcomeÃ§ou', 468: 'beija', 469: 'beijaram', 470: 'beije', 471: 'beijei', 472: 'beijo', 473: 'beijou', 474: 'believe', 475: 'belong', 476: 'bem', 477: 'bemvinda', 478: 'bemvindo', 479: 'best\\tdÃª', 480: 'best\\teu', 481: 'best\\tfaÃ§a', 482: 'best\\tfiz', 483: 'bet\\tera', 484: 'bet\\teu', 485: 'better\\testamos', 486: 'better\\testou', 487: 'better\\teu', 488: 'better\\tnÃ³s', 489: 'beware', 490: 'big\\telas', 491: 'big\\teles', 492: 'big\\tisso', 493: 'big\\tisto', 494: 'big\\tvocÃª', 495: 'big\\tÃ©', 496: 'bipolar', 497: 'bipolar\\tsou', 498: 'bird\\talimente', 499: 'birds', 500: 'biruta', 501: 'biscoito', 502: 'bit', 503: 'bit\\tespera', 504: 'bit\\tespere', 505: 'bite\\teu', 506: 'bite\\tnÃ£o', 507: 'bizarre\\tÃ©', 508: 'bizarro', 509: 'blank\\testÃ¡', 510: 'bled\\to', 511: 'bleeding\\testou', 512: 'blefando', 513: 'bless', 514: 'blind\\tele', 515: 'blind\\teu', 516: 'blind\\tficou', 517: 'blind\\to', 518: 'blind\\tsou', 519: 'blind\\ttom', 520: 'blind\\tvocÃª', 521: 'blinked\\teu', 522: 'blinked\\ttom', 523: 'blog', 524: 'blog\\tele', 525: 'blood', 526: 'bloom\\tflores', 527: 'blue\\testou', 528: 'blue\\teu', 529: 'blue\\tgosto', 530: 'blue\\tÃ©', 531: 'bluffing\\testou', 532: 'blushed\\tela', 533: 'blushed\\ttom', 534: 'boa', 535: 'boato', 536: 'bobo', 537: 'boca', 538: 'bocejei', 539: 'bocejou', 540: 'body\\tqueime', 541: 'body\\tqueimem', 542: 'boi', 543: 'boia', 544: 'boil', 545: 'bolo', 546: 'bolsa', 547: 'bom', 548: 'boneca', 549: 'bonita', 550: 'bonitas', 551: 'bonitinha', 552: 'bonitinho', 553: 'bonito', 554: 'bonitos', 555: 'bonzinho', 556: 'book\\tele', 557: 'book\\tesse', 558: 'book\\teste', 559: 'book\\tisso', 560: 'book\\tvejo', 561: 'books\\teu', 562: 'bordo', 563: 'bored\\testou', 564: 'bored\\teu', 565: 'bored\\ttom', 566: 'bored\\tvocÃª', 567: 'boss\\tele', 568: 'bossy\\teu', 569: 'both\\teu', 570: 'both\\tpor', 571: 'bought', 572: 'box\\tabre', 573: 'box\\tfeche', 574: 'boy\\teu', 575: 'boy\\tseja', 576: 'boy\\tsou', 577: 'boys', 578: 'boys\\teles', 579: 'boys\\tsomos', 580: 'boys\\tsÃ£o', 581: 'braces\\tele', 582: 'branco', 583: 'brava', 584: 'brave\\teu', 585: 'brave\\tseja', 586: 'brave\\tvocÃª', 587: 'bravo', 588: 'bravos', 589: 'braÃ§o', 590: 'bread\\tcomo', 591: 'bread\\teu', 592: 'breve', 593: 'bribed', 594: 'brief\\tseja', 595: 'brincadeira', 596: 'brincando', 597: 'brincar', 598: 'bring', 599: 'brinque', 600: 'brinquedos', 601: 'broke', 602: 'broke\\tele', 603: 'broke\\testou', 604: 'broke\\teu', 605: 'broken\\testÃ¡', 606: 'broken\\ttÃ¡', 607: 'built', 608: 'buraco', 609: 'burn', 610: 'burn\\tisso', 611: 'burn\\tvai', 612: 'burned\\tqueimou', 613: 'burns\\ta', 614: 'burns\\to', 615: 'burped\\teu', 616: 'burped\\ttom', 617: 'burra', 618: 'burro', 619: 'burros', 620: 'bus\\teu', 621: 'bus\\tpegue', 622: 'bus\\tpeguem', 623: 'bus\\tvim', 624: 'bus\\tvÃ¡', 625: 'bus\\tÃ©', 626: 'buscar', 627: 'buscÃ¡lo', 628: 'bushed\\testou', 629: 'bushed\\teu', 630: 'busy', 631: 'busy\\tela', 632: 'busy\\telas', 633: 'busy\\tele', 634: 'busy\\teles', 635: 'busy\\testamos', 636: 'busy\\testou', 637: 'busy\\testÃ¡s', 638: 'busy\\testÃ¡vamos', 639: 'busy\\teu', 640: 'busy\\tnÃ³s', 641: 'busy\\ttom', 642: 'buy', 643: 'buying\\tnÃ³s', 644: 'buying\\tquem', 645: 'by', 646: 'by\\taguarde', 647: 'by\\taguardem', 648: 'by\\tum', 649: 'bÃ¡rbaro', 650: 'bÃªbada', 651: 'bÃªbado', 652: 'cab\\teu', 653: 'cachorro', 654: 'cachorros', 655: 'caia', 656: 'cain', 657: 'caiu', 658: 'caixa', 659: 'cake\\teu', 660: 'cake\\texperimente', 661: 'call', 662: 'call\\teles', 663: 'call\\tligarei', 664: 'call\\ttom', 665: 'call\\tvocÃª', 666: 'called', 667: 'called\\telas', 668: 'called\\teles', 669: 'called\\tligaram', 670: 'called\\ttom', 671: 'called\\tvocÃª', 672: 'calm', 673: 'calm\\tacalmese', 674: 'calm\\testamos', 675: 'calm\\tfique', 676: 'calm\\tnÃ³s', 677: 'calm\\ttom', 678: 'calma', 679: 'calmo', 680: 'calmos', 681: 'calor', 682: 'cama', 683: 'came', 684: 'came\\talguÃ©m', 685: 'came\\tela', 686: 'came\\tele', 687: 'came\\tninguÃ©m', 688: 'came\\tnÃ£o', 689: 'came\\tquem', 690: 'came\\ttom', 691: 'caminha', 692: 'caminhando', 693: 'caminhar', 694: 'caminharam', 695: 'caminho', 696: 'caminhou', 697: 'can', 698: 'canadense', 699: 'canadian\\teu', 700: 'cancel\\tcancelarei', 701: 'canceled\\tquem', 702: 'canceled\\ttom', 703: 'cancelou', 704: 'cancer\\tÃ©', 705: 'candy\\teu', 706: 'caneta', 707: 'cansada', 708: 'cansadas', 709: 'cansado', 710: 'cansados', 711: 'cant', 712: 'cantam', 713: 'cantando', 714: 'cantar', 715: 'cantaram', 716: 'cante', 717: 'cantei', 718: 'cantem', 719: 'cantes', 720: 'canto', 721: 'car', 722: 'car\\tele', 723: 'car\\teu', 724: 'car\\ttenho', 725: 'car\\tÃ©', 726: 'card\\tescolha', 727: 'care\\tcuidese', 728: 'care\\teu', 729: 'care\\tnÃ£o', 730: 'care\\tnÃ³s', 731: 'care\\tse', 732: 'care\\tte', 733: 'care\\ttoma', 734: 'care\\ttome', 735: 'careca', 736: 'cared\\tninguÃ©m', 737: 'careful\\tcuidado', 738: 'careful\\ttenha', 739: 'careful\\ttoma', 740: 'careful\\ttome', 741: 'carente', 742: 'cares\\tninguÃ©m', 743: 'cares\\tquem', 744: 'cares\\ttom', 745: 'careta', 746: 'carne', 747: 'carro', 748: 'carros', 749: 'carry', 750: 'cars\\tele', 751: 'cars\\teu', 752: 'carta', 753: 'casa', 754: 'casaco', 755: 'casada', 756: 'casado', 757: 'cash\\teu', 758: 'cash\\tquero', 759: 'castigo', 760: 'cat\\teste', 761: 'cat\\teu', 762: 'cat\\tÃ©', 763: 'catch', 764: 'cats\\tadoro', 765: 'cats\\tamo', 766: 'cats\\teu', 767: 'caught\\tfui', 768: 'cavando', 769: 'cavei', 770: 'caviar', 771: 'caviar\\teu', 772: 'cavou', 773: 'caÃ\\xad', 774: 'cd', 775: 'cd\\tÃ©', 776: 'cds', 777: 'cds\\tcompramos', 778: 'cedeu', 779: 'cedo', 780: 'cega', 781: 'cego', 782: 'certa', 783: 'certain\\testou', 784: 'certain\\ttenho', 785: 'certeza', 786: 'certo', 787: 'certos', 788: 'cerveja', 789: 'chamar', 790: 'chamarÃ£o', 791: 'chamou', 792: 'chance', 793: 'change\\tmudarei', 794: 'changed\\teu', 795: 'changed\\ttom', 796: 'chapado', 797: 'chapÃ©u', 798: 'chat\\tvamos', 799: 'chateada', 800: 'chateado', 801: 'chateados', 802: 'chatice', 803: 'chato', 804: 'chave', 805: 'cheat\\teles', 806: 'cheat\\tnÃ£o', 807: 'cheated', 808: 'cheated\\teu', 809: 'cheated\\to', 810: 'cheated\\tquem', 811: 'cheated\\tvocÃª', 812: 'cheated\\tvocÃªs', 813: 'cheats\\ttom', 814: 'check', 815: 'check\\tnÃ³s', 816: 'check\\tverificarei', 817: 'cheer', 818: 'cheered\\telas', 819: 'cheered\\teles', 820: 'cheered\\tquem', 821: 'cheered\\ttom', 822: 'cheers\\tsaÃºde', 823: 'cheese\\tsorria', 824: 'chegando', 825: 'chegou', 826: 'cheguei', 827: 'cheia', 828: 'cheio', 829: 'cheirada', 830: 'chess\\teu', 831: 'chess\\tgosto', 832: 'china', 833: 'china\\teu', 834: 'chinese\\teu', 835: 'chinese\\tsou', 836: 'chinÃªs', 837: 'chocado', 838: 'choose', 839: 'choque', 840: 'choram', 841: 'choramingar', 842: 'choramos', 843: 'chorando', 844: 'chorar', 845: 'chorava', 846: 'chore', 847: 'chorei', 848: 'chorem', 849: 'choro', 850: 'chorou', 851: 'chova', 852: 'chovendo', 853: 'chover', 854: 'chubby\\tsou', 855: 'chuckled\\ttom', 856: 'chutou', 857: 'chuvoso', 858: 'chÃ¡', 859: 'ci', 860: 'cidade', 861: 'cima', 862: 'cinco', 863: 'circle\\tdesenhe', 864: 'ciumenta', 865: 'ciumento', 866: 'clapped\\ttom', 867: 'claro', 868: 'clean', 869: 'clean\\testÃ¡', 870: 'clear\\testÃ¡', 871: 'clear\\tÃ©', 872: 'clever\\tque', 873: 'click', 874: 'client\\teu', 875: 'cliente', 876: 'clock\\tconserte', 877: 'close', 878: 'close\\testÃ¡', 879: 'close\\tfique', 880: 'closely\\tolhe', 881: 'closer\\tchega', 882: 'closer\\tchegue', 883: 'closer\\tcheguem', 884: 'closer\\tolhe', 885: 'closer\\tolhem', 886: 'cloudy\\testÃ¡', 887: 'clumsy\\tsou', 888: 'cnn', 889: 'cnn\\tligue', 890: 'coat\\tpegue', 891: 'cochilo', 892: 'cochilou', 893: 'coisa', 894: 'cola', 895: 'cold\\tera', 896: 'cold\\testamos', 897: 'cold\\testava', 898: 'cold\\testou', 899: 'cold\\testÃ¡vamos', 900: 'cold\\teu', 901: 'cold\\tfiquei', 902: 'cold\\tnÃ³s', 903: 'cold\\tsinto', 904: 'cold\\ttenho', 905: 'cole', 906: 'colem', 907: 'colidiram', 908: 'coloquei', 909: 'com', 910: 'comando', 911: 'come', 912: 'come\\taqui', 913: 'come\\tela', 914: 'come\\tele', 915: 'come\\testou', 916: 'come\\teu', 917: 'come\\tnÃ£o', 918: 'come\\to', 919: 'come\\tpode', 920: 'come\\tposso', 921: 'come\\ttom', 922: 'come\\tvem', 923: 'come\\tvenha', 924: 'come\\tvenham', 925: 'come\\tvocÃª', 926: 'come\\tvocÃªs', 927: 'comendo', 928: 'comentÃ¡rios', 929: 'comer', 930: 'comes\\tlÃ¡', 931: 'comeu', 932: 'comeÃ§a', 933: 'comeÃ§ar', 934: 'comeÃ§aremos', 935: 'comi', 936: 'comida', 937: 'comigo', 938: 'coming\\tele', 939: 'coming\\testou', 940: 'coming\\teu', 941: 'coming\\tnÃ³s', 942: 'coming\\tquem', 943: 'coming\\ttom', 944: 'command\\tassuma', 945: 'comment\\tsem', 946: 'como', 947: 'compartilhar', 948: 'compartilharemos', 949: 'comprando', 950: 'compraria', 951: 'compreende', 952: 'compreendo', 953: 'comprei', 954: 'concorda', 955: 'concordam', 956: 'concordamos', 957: 'concordo', 958: 'concordou', 959: 'confessed\\teu', 960: 'confessei', 961: 'confiante', 962: 'confiantes', 963: 'confident\\tsede', 964: 'confident\\tseja', 965: 'confident\\tsejam', 966: 'confident\\tsÃª', 967: 'confie', 968: 'confio', 969: 'confiÃ¡vel', 970: 'confused\\testou', 971: 'confuso', 972: 'congelando', 973: 'conhece', 974: 'conhecem', 975: 'conhecemos', 976: 'conhecer', 977: 'conheceu', 978: 'conheci', 979: 'conheÃ§o', 980: 'conheÃ§oo', 981: 'conosco', 982: 'consegue', 983: 'conseguimos', 984: 'conseguindo', 985: 'conseguiu', 986: 'consertei', 987: 'consertÃ¡lo', 988: 'consigo', 989: 'constrangedor', 990: 'construiu', 991: 'construÃ\\xad', 992: 'contact', 993: 'contar', 994: 'content\\tfique', 995: 'contente', 996: 'contigo', 997: 'contou', 998: 'contratado', 999: 'contratei', 1000: 'contributed\\teu', 1001: 'contribuÃ\\xad', 1002: 'controle', 1003: 'contrÃ¡rio', 1004: 'conversamos', 1005: 'conversar', 1006: 'convicto', 1007: 'convidado', 1008: 'cook\\tcozinharei', 1009: 'cook\\teu', 1010: 'cook\\tnÃ³s', 1011: 'cook\\to', 1012: 'cook\\tquem', 1013: 'cook\\tsou', 1014: 'cook\\ttom', 1015: 'cookie\\tcome', 1016: 'cooking\\testou', 1017: 'cool', 1018: 'cool\\telas', 1019: 'cool\\teles', 1020: 'cool\\teu', 1021: 'cool\\tfique', 1022: 'cool\\tfoi', 1023: 'cool\\tisso', 1024: 'cool\\tisto', 1025: 'cool\\tmantenha', 1026: 'cool\\tseja', 1027: 'cool\\tsou', 1028: 'cool\\tte', 1029: 'cool\\tÃ©', 1030: 'cops\\tvocÃªs', 1031: 'coragem', 1032: 'corajoso', 1033: 'corda', 1034: 'corou', 1035: 'corpo', 1036: 'corra', 1037: 'corram', 1038: 'corras', 1039: 'corre', 1040: 'correct\\teu', 1041: 'corredor', 1042: 'correm', 1043: 'correndo', 1044: 'correr', 1045: 'correu', 1046: 'corri', 1047: 'corria', 1048: 'cortei', 1049: 'cough', 1050: 'coughed\\tele', 1051: 'coughed\\teu', 1052: 'coughed\\ttom', 1053: 'could', 1054: 'count', 1055: 'courage\\ttenha', 1056: 'courage\\ttenham', 1057: 'course\\tclaro', 1058: 'course\\tpois', 1059: 'course\\tÃ©', 1060: 'cover\\tprotejamse', 1061: 'cozinhando', 1062: 'cozinhar', 1063: 'cozinharemos', 1064: 'cozinharÃ¡', 1065: 'cozinheira', 1066: 'cozinheiro', 1067: 'cozinho', 1068: 'coÃ§am', 1069: 'cranky\\tele', 1070: 'crashed\\telas', 1071: 'crashed\\teles', 1072: 'crashed\\ttom', 1073: 'crazy\\ta', 1074: 'crazy\\testou', 1075: 'crazy\\testÃ¡s', 1076: 'crazy\\tisso', 1077: 'crazy\\to', 1078: 'crazy\\tque', 1079: 'crazy\\tsomos', 1080: 'crazy\\tvocÃª', 1081: 'creative\\teu', 1082: 'creative\\tsede', 1083: 'creative\\tseja', 1084: 'creative\\tsejam', 1085: 'creative\\tsou', 1086: 'creative\\tsÃª', 1087: 'crescem', 1088: 'cresceram', 1089: 'crescerÃ£o', 1090: 'cresceu', 1091: 'crianÃ§as', 1092: 'criativa', 1093: 'criativas', 1094: 'criativo', 1095: 'criativos', 1096: 'cried', 1097: 'cried\\tela', 1098: 'cried\\teu', 1099: 'cried\\tmamÃ£e', 1100: 'cried\\ttodos', 1101: 'cried\\ttom', 1102: 'criei', 1103: 'cringed\\ttom', 1104: 'crio', 1105: 'cruel', 1106: 'cruel\\tele', 1107: 'cruel\\tisso', 1108: 'cry', 1109: 'cry\\teu', 1110: 'cry\\tnÃ£o', 1111: 'cry\\to', 1112: 'cry\\ttom', 1113: 'crying\\tele', 1114: 'crying\\tpare', 1115: 'crying\\tparem', 1116: 'crying\\ttom', 1117: 'cuff', 1118: 'cuida', 1119: 'cuidado', 1120: 'culhÃ£o', 1121: 'culpada', 1122: 'culpado', 1123: 'cultiva', 1124: 'curado', 1125: 'cured\\to', 1126: 'curiosa', 1127: 'curioso', 1128: 'curious\\teu', 1129: 'curious\\tque', 1130: 'curious\\tquero', 1131: 'curse\\tisso', 1132: 'curse\\tÃ©', 1133: 'curta', 1134: 'cut', 1135: 'cute\\ta', 1136: 'cute\\tas', 1137: 'cute\\tele', 1138: 'cute\\to', 1139: 'cute\\tos', 1140: 'cute\\tque', 1141: 'cute\\ttu', 1142: 'cute\\tvocÃª', 1143: 'cute\\tvocÃªs', 1144: 'cute\\tÃ©', 1145: 'cÃ¡', 1146: 'cÃ¢ncer', 1147: 'cÃ£es', 1148: 'cÃ£o', 1149: 'cÃ\\xadrculo', 1150: 'cÃ³cegas', 1151: 'da', 1152: 'damas', 1153: 'dance', 1154: 'dance\\tdancemos', 1155: 'dance\\tdanÃ§aremos', 1156: 'dance\\teu', 1157: 'dance\\tnÃ³s', 1158: 'dance\\tquer', 1159: 'danced\\tdanÃ§aram', 1160: 'danced\\telas', 1161: 'danced\\teles', 1162: 'danced\\teu', 1163: 'danced\\ttom', 1164: 'dancei', 1165: 'dances\\ttom', 1166: 'dancing\\tcontinue', 1167: 'dancing\\tcontinuem', 1168: 'dancing\\teu', 1169: 'danÃ§a', 1170: 'danÃ§ando', 1171: 'danÃ§ar', 1172: 'danÃ§aram', 1173: 'danÃ§aremos', 1174: 'danÃ§ou', 1175: 'daqui', 1176: 'daquilo', 1177: 'dar', 1178: 'dark\\testava', 1179: 'dark\\testÃ¡', 1180: 'dark\\tÃ©', 1181: 'date\\tÃ©', 1182: 'dating\\testamos', 1183: 'day\\tque', 1184: 'daÃ\\xad', 1185: 'de', 1186: 'dead\\tela', 1187: 'dead\\telas', 1188: 'dead\\teles', 1189: 'dead\\teu', 1190: 'dead\\tnÃ£o', 1191: 'deaf\\teu', 1192: 'deaf\\tnÃ£o', 1193: 'deaf\\to', 1194: 'deaf\\tsou', 1195: 'deaf\\ttom', 1196: 'deaf\\tvocÃª', 1197: 'decide\\tdecidirei', 1198: 'decide\\tdecidiremos', 1199: 'decide\\tnos', 1200: 'decide\\tnÃ³s', 1201: 'decided\\tdecidi', 1202: 'decided\\teu', 1203: 'decided\\ttom', 1204: 'decidi', 1205: 'decidir', 1206: 'decidiremos', 1207: 'decidiu', 1208: 'decisive\\tsou', 1209: 'decisivo', 1210: 'deep\\tqual', 1211: 'deep\\tquÃ£o', 1212: 'dei', 1213: 'deixa', 1214: 'deixe', 1215: 'deixei', 1216: 'dela', 1217: 'delas', 1218: 'dele', 1219: 'deles', 1220: 'delicioso', 1221: 'delicious\\tque', 1222: 'delÃ\\xadcia', 1223: 'demais', 1224: 'demiti', 1225: 'demitido', 1226: 'demito', 1227: 'demore', 1228: 'dentro', 1229: 'depois', 1230: 'depressa', 1231: 'desabrigado', 1232: 'desagradÃ¡vel', 1233: 'desajeitado', 1234: 'desapareceu', 1235: 'desastrado', 1236: 'descansando', 1237: 'descansar', 1238: 'desconfiado', 1239: 'describe', 1240: 'desculpamos', 1241: 'desculpe', 1242: 'desejo', 1243: 'desenhou', 1244: 'desista', 1245: 'desistimos', 1246: 'desistiram', 1247: 'desistiu', 1248: 'desisto', 1249: 'desliga', 1250: 'desligou', 1251: 'desmaiaram', 1252: 'desmaiei', 1253: 'desmaiou', 1254: 'desorganizada', 1255: 'desorganizado', 1256: 'despedida', 1257: 'despedido', 1258: 'despedidos', 1259: 'despediu', 1260: 'desperdÃ\\xadcio', 1261: 'despise', 1262: 'desprezo', 1263: 'desses', 1264: 'detesta', 1265: 'detesto', 1266: 'deu', 1267: 'deus', 1268: 'devagar', 1269: 'deve', 1270: 'deveria', 1271: 'deverÃ\\xadamos', 1272: 'deves', 1273: 'devia', 1274: 'dez', 1275: 'dezesseis', 1276: 'dia', 1277: 'diabetic\\teu', 1278: 'diabetic\\tsou', 1279: 'diabÃ©tico', 1280: 'did', 1281: 'didnt', 1282: 'die\\tdeixeme', 1283: 'die\\tdeixemme', 1284: 'die\\tele', 1285: 'die\\teu', 1286: 'die\\tlute', 1287: 'die\\tnÃ£o', 1288: 'die\\to', 1289: 'die\\ttodos', 1290: 'die\\ttom', 1291: 'die\\tvocÃª', 1292: 'died\\tela', 1293: 'died\\tninguÃ©m', 1294: 'died\\to', 1295: 'died\\tquantas', 1296: 'died\\tquantos', 1297: 'died\\tquem', 1298: 'died\\ttom', 1299: 'dies\\ttodo', 1300: 'dieta', 1301: 'dieting\\testou', 1302: 'difÃ\\xadcil', 1303: 'digging\\tcontinue', 1304: 'digging\\tcontinuem', 1305: 'dignidade', 1306: 'digo', 1307: 'dinheiro', 1308: 'direita', 1309: 'dirige', 1310: 'dirigi', 1311: 'dirigindo', 1312: 'dirigir', 1313: 'dirigiu', 1314: 'dirijo', 1315: 'dirty\\testÃ¡', 1316: 'dirty\\tisso', 1317: 'disagree\\tdiscordo', 1318: 'disagree\\teu', 1319: 'disagreed\\teu', 1320: 'discordei', 1321: 'discordo', 1322: 'discr', 1323: 'discreet\\tsede', 1324: 'discreet\\tseja', 1325: 'discreet\\tsejam', 1326: 'discreet\\tsÃª', 1327: 'discreta', 1328: 'discretas', 1329: 'discreto', 1330: 'discretos', 1331: 'discuta', 1332: 'discutam', 1333: 'discutas', 1334: 'discutir', 1335: 'dislÃ©xico', 1336: 'disparou', 1337: 'disperdÃ\\xadcio', 1338: 'disse', 1339: 'disseram', 1340: 'disso', 1341: 'distante', 1342: 'disto', 1343: 'distÃ¢ncia', 1344: 'diverte', 1345: 'diverti', 1346: 'divertida', 1347: 'divertido', 1348: 'divertimos', 1349: 'divertiu', 1350: 'dividir', 1351: 'divorced\\tsou', 1352: 'divorciado', 1353: 'diz', 1354: 'dizer', 1355: 'dizzy\\testamos', 1356: 'dizzy\\testou', 1357: 'dizzy\\teu', 1358: 'dizzy\\tnÃ³s', 1359: 'dj', 1360: 'dj\\tele', 1361: 'do', 1362: 'do\\tcomo', 1363: 'do\\tisso', 1364: 'doce', 1365: 'doctor\\teu', 1366: 'doctor\\tsou', 1367: 'doe', 1368: 'doem', 1369: 'doen', 1370: 'doend', 1371: 'doendo', 1372: 'doente', 1373: 'doentes', 1374: 'does', 1375: 'dog\\tcomo', 1376: 'dog\\tcuidado', 1377: 'dog\\teu', 1378: 'dog\\tvi', 1379: 'dogs', 1380: 'dogs\\teu', 1381: 'dogs\\tgosto', 1382: 'dogs\\todeio', 1383: 'doido', 1384: 'doing', 1385: 'dois', 1386: 'doll\\tÃ©', 1387: 'done', 1388: 'done\\tainda', 1389: 'done\\tbem', 1390: 'done\\tbom', 1391: 'done\\tbravo', 1392: 'done\\testÃ¡', 1393: 'done\\teu', 1394: 'done\\tjÃ¡', 1395: 'done\\tparabÃ©ns', 1396: 'done\\tpronto', 1397: 'done\\tterminamos', 1398: 'done\\tterminou', 1399: 'dont', 1400: 'donut\\teu', 1401: 'door\\tsegure', 1402: 'dor', 1403: 'dormem', 1404: 'dormi', 1405: 'dormindo', 1406: 'dormir', 1407: 'dormiu', 1408: 'dos', 1409: 'dou', 1410: 'doubt', 1411: 'doubts\\teu', 1412: 'down\\tabaixese', 1413: 'down\\tacalmate', 1414: 'down\\tacalmemse', 1415: 'down\\tacalmese', 1416: 'down\\tagora', 1417: 'down\\taquieta', 1418: 'down\\tassentese', 1419: 'down\\tdeitemse', 1420: 'down\\tdesliga', 1421: 'down\\tdesÃ§a', 1422: 'down\\tdevagar', 1423: 'down\\tescreva', 1424: 'down\\teu', 1425: 'down\\tfique', 1426: 'down\\tfiquem', 1427: 'down\\tnos', 1428: 'down\\tnÃ³s', 1429: 'down\\tse', 1430: 'down\\tsentese', 1431: 'down\\tte', 1432: 'down\\tvenha', 1433: 'down\\tvÃ¡', 1434: 'dozed', 1435: 'dozed\\ttom', 1436: 'drag\\tque', 1437: 'drank', 1438: 'draw', 1439: 'dream', 1440: 'dream\\teu', 1441: 'dreaming\\testou', 1442: 'dreams\\tbons', 1443: 'dressed\\tvistamse', 1444: 'dressed\\tvistase', 1445: 'drew', 1446: 'drink', 1447: 'drink\\tcoma', 1448: 'drink\\tcome', 1449: 'drink\\teu', 1450: 'drink\\tvocÃª', 1451: 'drink\\tvocÃªs', 1452: 'drinks\\ttom', 1453: 'drive', 1454: 'drive\\tdirigirei', 1455: 'drive\\tnÃ³s', 1456: 'drive\\tquem', 1457: 'drive\\ttom', 1458: 'drive\\tvamos', 1459: 'drive\\tvocÃª', 1460: 'drive\\tvocÃªs', 1461: 'drives\\ttom', 1462: 'driving\\tcontinue', 1463: 'drop', 1464: 'drove\\teu', 1465: 'drove\\ttom', 1466: 'drowned\\ttom', 1467: 'drowning\\testou', 1468: 'drunk\\tele', 1469: 'drunk\\testou', 1470: 'drunk\\teu', 1471: 'drunk\\to', 1472: 'drunk\\ttom', 1473: 'duas', 1474: 'dug', 1475: 'dumb\\teu', 1476: 'dura', 1477: 'duro', 1478: 'dusty\\testÃ¡', 1479: 'duvido', 1480: 'dying\\testamos', 1481: 'dying\\testou', 1482: 'dying\\teu', 1483: 'dying\\tquem', 1484: 'dying\\ttom', 1485: 'dying\\tvocÃª', 1486: 'dyslexic\\teu', 1487: 'dyslexic\\tsou', 1488: 'dÃ¡', 1489: 'dÃ³i', 1490: 'dÃ³lares', 1491: 'dÃºvida', 1492: 'dÃºvidas', 1493: 'e', 1494: 'early\\tcheguei', 1495: 'early\\testamos', 1496: 'early\\testou', 1497: 'early\\teu', 1498: 'early\\to', 1499: 'early\\ttom', 1500: 'early\\tvocÃª', 1501: 'ears\\tsou', 1502: 'easily\\teu', 1503: 'easter\\tfeliz', 1504: 'easy\\tacalmese', 1505: 'easy\\tisso', 1506: 'easy\\tisto', 1507: 'easy\\tpega', 1508: 'easy\\tpegue', 1509: 'easy\\ttenha', 1510: 'easy\\tÃ©', 1511: 'eat', 1512: 'eat\\teu', 1513: 'eat\\to', 1514: 'eat\\tposso', 1515: 'eat\\tvamos', 1516: 'eat\\tvocÃª', 1517: 'eaten\\teu', 1518: 'eating\\tele', 1519: 'eating\\testou', 1520: 'eating\\testÃ¡', 1521: 'eating\\teu', 1522: 'eating\\to', 1523: 'eating\\ttom', 1524: 'edit\\tclique', 1525: 'editar', 1526: 'educado', 1527: 'egg\\tcozinhe', 1528: 'egoÃ\\xadsta', 1529: 'eight\\tela', 1530: 'eight\\tele', 1531: 'ela', 1532: 'elas', 1533: 'ele', 1534: 'eles', 1535: 'elk\\tÃ©', 1536: 'else\\tmais', 1537: 'else\\to', 1538: 'em', 1539: 'embo', 1540: 'embora', 1541: 'emboscada', 1542: 'embriagado', 1543: 'emocionante', 1544: 'emperrada', 1545: 'emperrado', 1546: 'empoeirado', 1547: 'empregada', 1548: 'emprego', 1549: 'empty\\testava', 1550: 'empty\\testÃ¡', 1551: 'empurra', 1552: 'empurre', 1553: 'empurrei', 1554: 'empurrem', 1555: 'empurres', 1556: 'encara', 1557: 'encarar', 1558: 'encolheu', 1559: 'encontramos', 1560: 'encontrar', 1561: 'encontrei', 1562: 'encontro', 1563: 'ended\\to', 1564: 'enfermeiro', 1565: 'enfermo', 1566: 'enganei', 1567: 'enganou', 1568: 'english\\tele', 1569: 'engorde', 1570: 'engordou', 1571: 'engraÃ§ado', 1572: 'enjoy', 1573: 'enlisted\\ttom', 1574: 'enorme', 1575: 'enraiveceu', 1576: 'ensina', 1577: 'ensopado', 1578: 'entediada', 1579: 'entediado', 1580: 'entendi', 1581: 'entendo', 1582: 'entrar', 1583: 'entraram', 1584: 'entre', 1585: 'entrei', 1586: 'entrem', 1587: 'entrou', 1588: 'entÃ£o', 1589: 'envolvido', 1590: 'envy', 1591: 'equivocada', 1592: 'era', 1593: 'errada', 1594: 'errado', 1595: 'errei', 1596: 'escapando', 1597: 'escaparam', 1598: 'escaped\\teles', 1599: 'escaped\\tescaparam', 1600: 'escaped\\teu', 1601: 'escaped\\tquem', 1602: 'escaped\\ttom', 1603: 'escapei', 1604: 'escaping\\teu', 1605: 'escapou', 1606: 'escola', 1607: 'escondase', 1608: 'esconden', 1609: 'escondendo', 1610: 'escondete', 1611: 'escorregou', 1612: 'escorreguei', 1613: 'escrevendo', 1614: 'escreveu', 1615: 'escritor', 1616: 'escuro', 1617: 'escuta', 1618: 'escutei', 1619: 'escutou', 1620: 'esforÃ§a', 1621: 'espanhol', 1622: 'espaÃ§o', 1623: 'especial', 1624: 'especiali', 1625: 'especÃ\\xadfico', 1626: 'esperamos', 1627: 'esperando', 1628: 'esperanÃ§a', 1629: 'esperar', 1630: 'esperaram', 1631: 'esperaremos', 1632: 'espere', 1633: 'esperei', 1634: 'espero', 1635: 'esperou', 1636: 'esperta', 1637: 'esperto', 1638: 'espertos', 1639: 'espirituoso', 1640: 'espirrei', 1641: 'espirrou', 1642: 'espiÃ£o', 1643: 'esquecemos', 1644: 'esqueceu', 1645: 'esqueci', 1646: 'esquerda', 1647: 'esqueÃ§a', 1648: 'esquiando', 1649: 'esquiar', 1650: 'esquio', 1651: 'esquisito', 1652: 'essa', 1653: 'esse', 1654: 'esses', 1655: 'estamos', 1656: 'estaremos', 1657: 'estava', 1658: 'estou', 1659: 'estranho', 1660: 'estrela', 1661: 'estudando', 1662: 'estudar', 1663: 'estudei', 1664: 'estudo', 1665: 'estÃ¡', 1666: 'estÃ¡s', 1667: 'estÃ¡vamos', 1668: 'estÃ£o', 1669: 'estÃºpido', 1670: 'eu', 1671: 'even\\testamos', 1672: 'evening\\tboa', 1673: 'everybody\\tolÃ¡', 1674: 'everyone', 1675: 'evil\\ta', 1676: 'evil\\tcain', 1677: 'evil\\telas', 1678: 'evil\\teles', 1679: 'evil\\tnÃ£o', 1680: 'evita', 1681: 'evitame', 1682: 'ex', 1683: 'ex\\ttom', 1684: 'exalou', 1685: 'examine', 1686: 'exausto', 1687: 'excitado', 1688: 'excited\\testou', 1689: 'exciting\\tque', 1690: 'excuse', 1691: 'exercised\\teu', 1692: 'exercised\\tme', 1693: 'exercitei', 1694: 'exhaled\\ttom', 1695: 'exigente', 1696: 'exist\\tfantasmas', 1697: 'existe', 1698: 'existem', 1699: 'exists\\tdeus', 1700: 'experimentar', 1701: 'expert\\tpergunte', 1702: 'explain\\texplicarei', 1703: 'eyes', 1704: 'eyes\\teu', 1705: 'f', 1706: 'face\\tele', 1707: 'facho', 1708: 'facilmente', 1709: 'fail\\telas', 1710: 'fail\\teles', 1711: 'failed\\teu', 1712: 'failed\\tfalhamos', 1713: 'failed\\tfalhou', 1714: 'failed\\tnÃ³s', 1715: 'failed\\ttom', 1716: 'fainted\\tdesmaiei', 1717: 'fainted\\teu', 1718: 'fainted\\ttom', 1719: 'fainted\\tvocÃª', 1720: 'fainted\\tvocÃªs', 1721: 'fair\\taquilo', 1722: 'fair\\teu', 1723: 'fair\\tnÃ³s', 1724: 'fair\\tsede', 1725: 'fair\\tseja', 1726: 'fair\\tsejam', 1727: 'fair\\tsÃª', 1728: 'fair\\ttom', 1729: 'fair\\tvocÃª', 1730: 'fair\\tÃ©', 1731: 'faith\\ttenha', 1732: 'faith\\ttenham', 1733: 'fake\\tÃ©', 1734: 'fala', 1735: 'falando', 1736: 'falar', 1737: 'falarÃ¡', 1738: 'fale', 1739: 'falei', 1740: 'falem', 1741: 'fales', 1742: 'falhamos', 1743: 'falhou', 1744: 'falido', 1745: 'fall\\to', 1746: 'falo', 1747: 'falou', 1748: 'falta', 1749: 'faltando', 1750: 'faminto', 1751: 'famintos', 1752: 'famoso', 1753: 'famosos', 1754: 'famous\\testou', 1755: 'famous\\tsomos', 1756: 'famous\\ttom', 1757: 'fan\\ttom', 1758: 'fan\\ttu', 1759: 'fan\\tvocÃª', 1760: 'fantastic\\tfantÃ¡stico', 1761: 'far', 1762: 'far\\testÃ¡', 1763: 'far\\tisso', 1764: 'far\\tisto', 1765: 'far\\tÃ©', 1766: 'farei', 1767: 'faremos', 1768: 'faria', 1769: 'farmer\\teu', 1770: 'farmer\\tsou', 1771: 'farÃ¡', 1772: 'fast\\telas', 1773: 'fast\\tele', 1774: 'fast\\teles', 1775: 'fast\\teu', 1776: 'fast\\tfalo', 1777: 'fast\\tos', 1778: 'fast\\tsou', 1779: 'fast\\ttom', 1780: 'faster\\tdirija', 1781: 'faster\\teu', 1782: 'fasting\\testou', 1783: 'fasting\\teu', 1784: 'fat\\testou', 1785: 'fat\\teu', 1786: 'fat\\tnÃ£o', 1787: 'fat\\tsou', 1788: 'fat\\ttom', 1789: 'fat\\tvocÃª', 1790: 'father\\teu', 1791: 'father\\tsou', 1792: 'favor', 1793: 'favor\\tfaÃ§ame', 1794: 'faz', 1795: 'fazemos', 1796: 'fazendeiro', 1797: 'fazendo', 1798: 'fazer', 1799: 'fazÃªlo', 1800: 'faÃ§a', 1801: 'faÃ§o', 1802: 'fear', 1803: 'feast\\tque', 1804: 'fede', 1805: 'feed', 1806: 'feel', 1807: 'feet', 1808: 'feia', 1809: 'feias', 1810: 'feijÃ£o', 1811: 'feio', 1812: 'feios', 1813: 'feito', 1814: 'feliz', 1815: 'felizes', 1816: 'fell\\telas', 1817: 'fell\\teles', 1818: 'fell\\teu', 1819: 'fell\\tquem', 1820: 'fell\\tsentiram', 1821: 'fell\\ttom', 1822: 'felt', 1823: 'ferido', 1824: 'feriste', 1825: 'feriu', 1826: 'ferro', 1827: 'festejar', 1828: 'fez', 1829: 'ficar', 1830: 'ficarÃ¡', 1831: 'fico', 1832: 'ficou', 1833: 'fight', 1834: 'fight\\teu', 1835: 'fight\\tnÃ£o', 1836: 'fight\\tnÃ³s', 1837: 'fight\\tquem', 1838: 'fighting\\teu', 1839: 'filho', 1840: 'fill', 1841: 'find', 1842: 'fine', 1843: 'fine\\teles', 1844: 'fine\\testamos', 1845: 'fine\\testou', 1846: 'fine\\testÃ¡', 1847: 'fine\\teu', 1848: 'fine\\to', 1849: 'fine\\tvocÃª', 1850: 'fine\\tvou', 1851: 'finicky\\teu', 1852: 'finish', 1853: 'finished\\tjÃ¡', 1854: 'finished\\to', 1855: 'finished\\ttom', 1856: 'fique', 1857: 'fiquei', 1858: 'fire', 1859: 'fire\\tabrir', 1860: 'fire\\tfogo', 1861: 'fire\\tpreparar', 1862: 'fired', 1863: 'fired\\tdespediramme', 1864: 'fired\\testou', 1865: 'fired\\teu', 1866: 'fired\\tvocÃª', 1867: 'fired\\tvocÃªs', 1868: 'firme', 1869: 'first\\tas', 1870: 'first\\tele', 1871: 'first\\tpor', 1872: 'first\\tprimeiro', 1873: 'first\\tquem', 1874: 'first\\ttu', 1875: 'first\\tvocÃª', 1876: 'fish', 1877: 'fish\\teu', 1878: 'fish\\tisto', 1879: 'fish\\tvocÃª', 1880: 'fit\\testou', 1881: 'fit\\tvocÃª', 1882: 'fix', 1883: 'fixed', 1884: 'fiz', 1885: 'flies\\to', 1886: 'flinched\\ttom', 1887: 'floats\\ta', 1888: 'florescem', 1889: 'flowers', 1890: 'fly\\teu', 1891: 'fly\\tnÃ£o', 1892: 'fly\\tos', 1893: 'fly\\tpÃ¡ssaros', 1894: 'focado', 1895: 'foco', 1896: 'focused\\tmantenha', 1897: 'focused\\tmantenhase', 1898: 'focused\\tse', 1899: 'fofinha', 1900: 'fofinho', 1901: 'fofo', 1902: 'fogo', 1903: 'foi', 1904: 'foise', 1905: 'follow', 1906: 'follow\\tnÃ³s', 1907: 'fome', 1908: 'food\\teu', 1909: 'food\\tisso', 1910: 'food\\ttemos', 1911: 'food\\ttraga', 1912: 'food\\ttragam', 1913: 'food\\tÃ©', 1914: 'fool\\tele', 1915: 'fool\\teu', 1916: 'fool\\tnÃ£o', 1917: 'fooled', 1918: 'fools\\tvocÃªs', 1919: 'for', 1920: 'for\\tpara', 1921: 'fora', 1922: 'foram', 1923: 'forget', 1924: 'forget\\tnÃ£o', 1925: 'forgive', 1926: 'forgot', 1927: 'forgot\\tesquecemos', 1928: 'forgot\\tesqueci', 1929: 'forgot\\teu', 1930: 'forgot\\tme', 1931: 'forgot\\tnÃ³s', 1932: 'forgot\\ttom', 1933: 'forma', 1934: 'forte', 1935: 'fortes', 1936: 'forward\\tum', 1937: 'forward\\tvenha', 1938: 'forÃ§adamente', 1939: 'fought\\ttom', 1940: 'found', 1941: 'fracassarÃ£o', 1942: 'fracassei', 1943: 'fraco', 1944: 'fracos', 1945: 'francÃªs', 1946: 'franziu', 1947: 'free', 1948: 'free\\testamos', 1949: 'free\\testou', 1950: 'free\\testÃ¡', 1951: 'free\\teu', 1952: 'free\\tisso', 1953: 'free\\tliberte', 1954: 'free\\tlibertem', 1955: 'free\\tnÃ£o', 1956: 'free\\tsomos', 1957: 'free\\tvocÃª', 1958: 'free\\tÃ©', 1959: 'freezing\\testou', 1960: 'french\\tele', 1961: 'frente', 1962: 'frequentemente', 1963: 'frequÃªncia', 1964: 'friendly\\tsede', 1965: 'friendly\\tseja', 1966: 'friendly\\tsejam', 1967: 'friendly\\tsÃª', 1968: 'frio', 1969: 'frowned\\ttom', 1970: 'fruit\\teu', 1971: 'fruit\\tÃ©', 1972: 'fruta', 1973: 'frutas', 1974: 'fugindo', 1975: 'fugiu', 1976: 'fui', 1977: 'full\\testou', 1978: 'full\\teu', 1979: 'full\\tvocÃª', 1980: 'fuma', 1981: 'fumam', 1982: 'fumar', 1983: 'fume', 1984: 'fumem', 1985: 'fumo', 1986: 'fun\\ta', 1987: 'fun\\tdivertete', 1988: 'fun\\tdivirtamse', 1989: 'fun\\tdivirtase', 1990: 'fun\\teu', 1991: 'fun\\tfoi', 1992: 'fun\\tisso', 1993: 'fun\\tisto', 1994: 'fun\\tnÃ£o', 1995: 'fun\\tnÃ³s', 1996: 'fun\\tque', 1997: 'fun\\tserÃ¡', 1998: 'fun\\ttom', 1999: 'fun\\tÃ©', 2000: 'funciona', 2001: 'funcionando', 2002: 'funcionar', 2003: 'funcionou', 2004: 'fundo', 2005: 'funny\\tfoi', 2006: 'funny\\tisso', 2007: 'funny\\ttom', 2008: 'funny\\tvocÃª', 2009: 'furiosa', 2010: 'furioso', 2011: 'furious\\testou', 2012: 'fussy\\ttom', 2013: 'fÃ¡cil', 2014: 'fÃ£', 2015: 'fÃ©', 2016: 'gagueja', 2017: 'ganancioso', 2018: 'gananciosos', 2019: 'ganha', 2020: 'ganhando', 2021: 'ganhar', 2022: 'ganharam', 2023: 'ganharÃ¡', 2024: 'ganhe', 2025: 'ganhei', 2026: 'ganho', 2027: 'ganhou', 2028: 'garbage\\tÃ©', 2029: 'garota', 2030: 'garotas', 2031: 'garoto', 2032: 'gasped\\ttom', 2033: 'gata', 2034: 'gato', 2035: 'gatos', 2036: 'gave', 2037: 'gawking\\tnÃ£o', 2038: 'gelo', 2039: 'gemeu', 2040: 'generoso', 2041: 'genius\\teu', 2042: 'gente', 2043: 'gentil', 2044: 'gentileza', 2045: 'gentis', 2046: 'gently\\tfaz', 2047: 'get', 2048: 'ghosts', 2049: 'gift\\tÃ©', 2050: 'giggled\\tmaria', 2051: 'giggled\\tmary', 2052: 'giggled\\ttom', 2053: 'girl\\teu', 2054: 'girls\\teu', 2055: 'girls\\tgosto', 2056: 'girls\\toi', 2057: 'girls\\tolÃ¡', 2058: 'give', 2059: 'glad\\testou', 2060: 'glad\\ttom', 2061: 'gloated\\ttom', 2062: 'glue\\teu', 2063: 'go', 2064: 'go\\tcomo', 2065: 'go\\tdeixeme', 2066: 'go\\tdevo', 2067: 'go\\tei', 2068: 'go\\teu', 2069: 'go\\tnÃ£o', 2070: 'go\\tnÃ³s', 2071: 'go\\to', 2072: 'go\\tpara', 2073: 'go\\tposso', 2074: 'go\\tpra', 2075: 'go\\tquem', 2076: 'go\\tquero', 2077: 'go\\ttenho', 2078: 'go\\ttive', 2079: 'go\\ttom', 2080: 'go\\ttu', 2081: 'go\\tvai', 2082: 'go\\tvamos', 2083: 'go\\tvocÃª', 2084: 'go\\tvÃ¡', 2085: 'go\\tÃ©', 2086: 'god', 2087: 'god\\tconfia', 2088: 'god\\tconfie', 2089: 'going\\tcontinue', 2090: 'going\\testou', 2091: 'going\\teu', 2092: 'going\\tjÃ¡', 2093: 'going\\tnÃ³s', 2094: 'going\\ttom', 2095: 'going\\tvocÃª', 2096: 'going\\tvocÃªs', 2097: 'going\\tvoume', 2098: 'gold\\tisso', 2099: 'golf\\teu', 2100: 'golfe', 2101: 'gone\\telas', 2102: 'gone\\teles', 2103: 'gone\\to', 2104: 'gone\\ttom', 2105: 'good', 2106: 'good\\ta', 2107: 'good\\testava', 2108: 'good\\teu', 2109: 'good\\tisso', 2110: 'good\\tnÃ£o', 2111: 'good\\ttom', 2112: 'good\\tvocÃª', 2113: 'good\\tÃ©', 2114: 'goodbye\\tatÃ©', 2115: 'goodbye\\tdiga', 2116: 'goodbye\\ttchau', 2117: 'gorda', 2118: 'gordinho', 2119: 'gordo', 2120: 'gosta', 2121: 'gostamos', 2122: 'gostei', 2123: 'gosto', 2124: 'gostou', 2125: 'got', 2126: 'grab', 2127: 'grande', 2128: 'grandes', 2129: 'granizo', 2130: 'gratuito', 2131: 'grave', 2132: 'graÃ§a', 2133: 'great\\tque', 2134: 'greedy\\tnÃ³s', 2135: 'greedy\\ttom', 2136: 'grimaced\\ttom', 2137: 'grinned\\ttom', 2138: 'gritar', 2139: 'gritaram', 2140: 'gritaremos', 2141: 'grite', 2142: 'gritei', 2143: 'gritem', 2144: 'grites', 2145: 'gritou', 2146: 'groaned\\ttom', 2147: 'groggy\\ttom', 2148: 'grogue', 2149: 'gross\\tvocÃª', 2150: 'grosseiro', 2151: 'grounded\\testou', 2152: 'grow\\tas', 2153: 'grow\\telas', 2154: 'grow\\teles', 2155: 'grown\\tvocÃª', 2156: 'grown\\tvocÃªs', 2157: 'grows', 2158: 'grumbled\\ttom', 2159: 'grunhiu', 2160: 'grunted\\teu', 2161: 'grunted\\ttom', 2162: 'grÃ¡tis', 2163: 'grÃ¡vida', 2164: 'guerra', 2165: 'guess', 2166: 'guilty\\teu', 2167: 'guts\\tele', 2168: 'guys\\tolÃ¡', 2169: 'gÃªmeos', 2170: 'gÃªnio', 2171: 'had', 2172: 'hailing\\testÃ¡', 2173: 'hand\\tsegure', 2174: 'handled', 2175: 'hands', 2176: 'hang', 2177: 'happen\\tisso', 2178: 'happened\\taconteceu', 2179: 'happens\\tacontece', 2180: 'happens\\tisso', 2181: 'happy', 2182: 'happy\\tela', 2183: 'happy\\tele', 2184: 'happy\\testou', 2185: 'happy\\teu', 2186: 'happy\\tme', 2187: 'happy\\tnÃ³s', 2188: 'happy\\tsintome', 2189: 'happy\\tsomos', 2190: 'happy\\tsou', 2191: 'happy\\ttom', 2192: 'happy\\tvocÃª', 2193: 'hard\\ta', 2194: 'hard\\tele', 2195: 'hard\\tesforcese', 2196: 'hard\\testude', 2197: 'hard\\testudem', 2198: 'hard\\tisto', 2199: 'hard\\tmatemÃ¡tica', 2200: 'hard\\to', 2201: 'hard\\tse', 2202: 'has', 2203: 'hat\\tpreciso', 2204: 'hat\\ttraga', 2205: 'hate', 2206: 'hated\\tele', 2207: 'hates', 2208: 'have', 2209: 'have\\ttenho', 2210: 'he', 2211: 'he\\tonde', 2212: 'he\\tqual', 2213: 'he\\tquantos', 2214: 'he\\tquem', 2215: 'healthy\\testou', 2216: 'hear', 2217: 'hear\\tconsegue', 2218: 'heard', 2219: 'heavy\\ttom', 2220: 'hell\\ta', 2221: 'hello', 2222: 'hello\\talÃ´', 2223: 'hello\\tcumprimenta', 2224: 'hello\\tdiga', 2225: 'hello\\toi', 2226: 'hello\\tolÃ¡', 2227: 'help', 2228: 'help\\tajuda', 2229: 'help\\tajudaremos', 2230: 'help\\tdeixeme', 2231: 'help\\teu', 2232: 'help\\tisso', 2233: 'help\\tnÃ³s', 2234: 'help\\to', 2235: 'help\\tposso', 2236: 'help\\tprecisamos', 2237: 'help\\tpreciso', 2238: 'help\\tsocorro', 2239: 'help\\ttom', 2240: 'help\\ttraga', 2241: 'help\\ttragam', 2242: 'help\\tvocÃª', 2243: 'helped', 2244: 'helped\\teu', 2245: 'helped\\tisso', 2246: 'helped\\ttom', 2247: 'helps', 2248: 'helps\\tisso', 2249: 'her\\tamoa', 2250: 'her\\tele', 2251: 'her\\teu', 2252: 'her\\ttom', 2253: 'her\\tvocÃª', 2254: 'her\\tvocÃªs', 2255: 'her\\tÃ©', 2256: 'here', 2257: 'here\\tassina', 2258: 'here\\tassine', 2259: 'here\\tcomece', 2260: 'here\\telas', 2261: 'here\\teles', 2262: 'here\\tespere', 2263: 'here\\tesperem', 2264: 'here\\testamos', 2265: 'here\\testarei', 2266: 'here\\testava', 2267: 'here\\testou', 2268: 'here\\testÃ¡', 2269: 'here\\teu', 2270: 'here\\tfique', 2271: 'here\\tfiquem', 2272: 'here\\tmeu', 2273: 'here\\tmoro', 2274: 'here\\tnÃ³s', 2275: 'here\\tolhe', 2276: 'here\\tpare', 2277: 'here\\tquem', 2278: 'here\\tsenta', 2279: 'here\\tsentese', 2280: 'here\\ttem', 2281: 'here\\ttom', 2282: 'here\\ttraga', 2283: 'here\\tvem', 2284: 'here\\tvenha', 2285: 'here\\tvocÃª', 2286: 'here\\tvolte', 2287: 'here\\tÃ©', 2288: 'heres\\taqui', 2289: 'hero\\teu', 2290: 'hero\\tsou', 2291: 'heroic\\tele', 2292: 'heroico', 2293: 'heroÃ\\xadna', 2294: 'hers\\tisso', 2295: 'hers\\tÃ©', 2296: 'herÃ³i', 2297: 'hes', 2298: 'hey', 2299: 'hi', 2300: 'hi\\toi', 2301: 'hide\\tcorra', 2302: 'hide\\tcorre', 2303: 'hiding\\teu', 2304: 'hiding\\tnÃ³s', 2305: 'higher\\tmire', 2306: 'him', 2307: 'him\\tagarremno', 2308: 'him\\tagarreo', 2309: 'him\\tajudeo', 2310: 'him\\talgemeo', 2311: 'him\\tamoo', 2312: 'him\\tconfiamos', 2313: 'him\\tconheÃ§oo', 2314: 'him\\tela', 2315: 'him\\tesqueÃ§ao', 2316: 'him\\testou', 2317: 'him\\teu', 2318: 'him\\tignora', 2319: 'him\\tignore', 2320: 'him\\tignoreo', 2321: 'him\\tirei', 2322: 'him\\tliberteo', 2323: 'him\\tliguei', 2324: 'him\\tolhe', 2325: 'him\\tpegueo', 2326: 'him\\tsigamno', 2327: 'him\\tsigao', 2328: 'him\\ttom', 2329: 'him\\ttu', 2330: 'him\\tvocÃª', 2331: 'him\\tÃ©', 2332: 'hip', 2333: 'hipÃ³crita', 2334: 'hired', 2335: 'hired\\testou', 2336: 'hired\\teu', 2337: 'his', 2338: 'his\\tisto', 2339: 'his\\tÃ©', 2340: 'hit', 2341: 'hit\\tfui', 2342: 'hit\\ttom', 2343: 'hoj', 2344: 'hoje', 2345: 'hold', 2346: 'hole\\tele', 2347: 'hole\\teu', 2348: 'home', 2349: 'home\\tagora', 2350: 'home\\tbemvindo', 2351: 'home\\tbemvindos', 2352: 'home\\tcheguei', 2353: 'home\\tcorra', 2354: 'home\\tcorri', 2355: 'home\\telas', 2356: 'home\\teles', 2357: 'home\\testou', 2358: 'home\\testÃ¡', 2359: 'home\\testÃ¡s', 2360: 'home\\teu', 2361: 'home\\tligue', 2362: 'home\\tnÃ£o', 2363: 'home\\ttem', 2364: 'home\\ttom', 2365: 'home\\tvenha', 2366: 'home\\tvocÃª', 2367: 'home\\tvocÃªs', 2368: 'home\\tvÃ¡', 2369: 'home\\tvÃ£o', 2370: 'homeless\\tsou', 2371: 'homem', 2372: 'homens', 2373: 'homesick\\testou', 2374: 'honest\\teu', 2375: 'honesto', 2376: 'hope', 2377: 'hope\\teu', 2378: 'hope\\ttemos', 2379: 'hora', 2380: 'horas', 2381: 'horrible\\testou', 2382: 'horrible\\teu', 2383: 'horrible\\tsou', 2384: 'horror', 2385: 'horrÃ\\xadvel', 2386: 'hot\\tela', 2387: 'hot\\testamos', 2388: 'hot\\testou', 2389: 'hot\\testÃ¡', 2390: 'hot\\tnÃ³s', 2391: 'hot\\ttenho', 2392: 'how', 2393: 'hows', 2394: 'hug', 2395: 'hug\\teu', 2396: 'hug\\tpreciso', 2397: 'huge\\tÃ©', 2398: 'hugged', 2399: 'hugged\\telas', 2400: 'hugged\\teles', 2401: 'human\\teu', 2402: 'human\\tsou', 2403: 'humano', 2404: 'humble\\tsou', 2405: 'humilde', 2406: 'hung', 2407: 'hungry\\testamos', 2408: 'hungry\\testou', 2409: 'hungry\\teu', 2410: 'hungry\\tnÃ³s', 2411: 'hungry\\tquem', 2412: 'hungry\\tquero', 2413: 'hungry\\ttenho', 2414: 'hungry\\ttom', 2415: 'hurry', 2416: 'hurry\\tapressemonos', 2417: 'hurry\\tapressemse', 2418: 'hurry\\tapressese', 2419: 'hurry\\tdespachate', 2420: 'hurry\\tdespachemse', 2421: 'hurry\\tdevemos', 2422: 'hurry\\tdevo', 2423: 'hurry\\tpor', 2424: 'hurry\\tvamos', 2425: 'hurt', 2426: 'hurt\\talguÃ©m', 2427: 'hurt\\tas', 2428: 'hurt\\tdoemme', 2429: 'hurt\\testou', 2430: 'hurt\\tmeus', 2431: 'hurt\\tos', 2432: 'hurt\\ttom', 2433: 'hurt\\tvocÃª', 2434: 'hurt\\tvocÃªs', 2435: 'hurts\\tisso', 2436: 'hurts\\tminha', 2437: 'hurts\\to', 2438: 'hÃ¡', 2439: 'i', 2440: 'i\\tonde', 2441: 'i\\tquem', 2442: 'ice\\teu', 2443: 'ice\\tvou', 2444: 'icky\\tisso', 2445: 'id', 2446: 'idad', 2447: 'idade', 2448: 'idea\\tnÃ£o', 2449: 'idea\\tque', 2450: 'ideia', 2451: 'idiot\\tidiota', 2452: 'idiot\\tseu', 2453: 'idiota', 2454: 'idoso', 2455: 'ienes', 2456: 'ignore', 2457: 'ignored', 2458: 'ignorou', 2459: 'ill', 2460: 'ill\\tele', 2461: 'ill\\teu', 2462: 'ill\\to', 2463: 'im', 2464: 'imediatamente', 2465: 'imitaÃ§Ã£o', 2466: 'impiedoso', 2467: 'importa', 2468: 'importamos', 2469: 'importo', 2470: 'importou', 2471: 'impostos', 2472: 'improvised\\teu', 2473: 'improvisei', 2474: 'in', 2475: 'in\\tconte', 2476: 'in\\tdeixeme', 2477: 'in\\tdeixenos', 2478: 'in\\tdeixeo', 2479: 'in\\telas', 2480: 'in\\tele', 2481: 'in\\teles', 2482: 'in\\tentra', 2483: 'in\\tentre', 2484: 'in\\tentrem', 2485: 'in\\testÃ£o', 2486: 'in\\teu', 2487: 'in\\tfaÃ§ao', 2488: 'in\\tmandeo', 2489: 'in\\tmary', 2490: 'in\\tnÃ£o', 2491: 'in\\to', 2492: 'in\\tposso', 2493: 'in\\ttom', 2494: 'in\\ttragao', 2495: 'in\\tvocÃª', 2496: 'in\\tvou', 2497: 'inalou', 2498: 'included\\teu', 2499: 'incluÃ\\xaddo', 2500: 'incomum', 2501: 'incrÃ\\xadvel', 2502: 'indo', 2503: 'inferno', 2504: 'inglÃªs', 2505: 'ingÃªnua', 2506: 'ingÃªnuo', 2507: 'inhaled\\ttom', 2508: 'injured\\testou', 2509: 'injusto', 2510: 'innocent\\teu', 2511: 'inocente', 2512: 'insano', 2513: 'insensÃ\\xadvel', 2514: 'inside\\tdÃª', 2515: 'inside\\tentra', 2516: 'inside\\tentre', 2517: 'inside\\tentrem', 2518: 'inside\\testamos', 2519: 'inside\\testou', 2520: 'inside\\teu', 2521: 'inside\\tnÃ³s', 2522: 'inside\\tvai', 2523: 'inside\\tvÃ¡', 2524: 'insisted\\ttom', 2525: 'insistente', 2526: 'insistiu', 2527: 'inteligente', 2528: 'intrigado', 2529: 'intrometido', 2530: 'invejo', 2531: 'inventei', 2532: 'invited\\teu', 2533: 'involved\\testou', 2534: 'involved\\teu', 2535: 'inÃºtil', 2536: 'ir', 2537: 'irei', 2538: 'iremos', 2539: 'iron', 2540: 'irÃ¡', 2541: 'is', 2542: 'is\\taqui', 2543: 'is\\tlÃ¡', 2544: 'is\\tsÃ£o', 2545: 'isnt', 2546: 'isso', 2547: 'isto', 2548: 'it', 2549: 'it\\ta', 2550: 'it\\tachei', 2551: 'it\\tagora', 2552: 'it\\tcadÃª', 2553: 'it\\tcomo', 2554: 'it\\tde', 2555: 'it\\tdeixa', 2556: 'it\\tdeixe', 2557: 'it\\tdesista', 2558: 'it\\tela', 2559: 'it\\tele', 2560: 'it\\tencontrei', 2561: 'it\\tencontreia', 2562: 'it\\tentendeu', 2563: 'it\\tentendi', 2564: 'it\\tesquece', 2565: 'it\\tesqueci', 2566: 'it\\tesqueÃ§a', 2567: 'it\\tesqueÃ§am', 2568: 'it\\testou', 2569: 'it\\testÃ¡', 2570: 'it\\teu', 2571: 'it\\tfaremos', 2572: 'it\\tfica', 2573: 'it\\tfique', 2574: 'it\\tfiquem', 2575: 'it\\tgostamos', 2576: 'it\\tirei', 2577: 'it\\tlargue', 2578: 'it\\tlembre', 2579: 'it\\tme', 2580: 'it\\tmemorize', 2581: 'it\\tnÃ£o', 2582: 'it\\tnÃ³s', 2583: 'it\\to', 2584: 'it\\tonde', 2585: 'it\\tpara', 2586: 'it\\tpare', 2587: 'it\\tpegaa', 2588: 'it\\tpegao', 2589: 'it\\tpeguea', 2590: 'it\\tpeguemna', 2591: 'it\\tpeguemno', 2592: 'it\\tpegueo', 2593: 'it\\tperdi', 2594: 'it\\tperdio', 2595: 'it\\tpodemos', 2596: 'it\\tposso', 2597: 'it\\tpreciso', 2598: 'it\\tprovea', 2599: 'it\\tproveo', 2600: 'it\\tquem', 2601: 'it\\tqueremos', 2602: 'it\\tquero', 2603: 'it\\tquÃ£o', 2604: 'it\\tsaquei', 2605: 'it\\tsaudades', 2606: 'it\\tse', 2607: 'it\\tsegura', 2608: 'it\\tsegure', 2609: 'it\\tsolte', 2610: 'it\\tsolteo', 2611: 'it\\tsubstitua', 2612: 'it\\ttentao', 2613: 'it\\ttodos', 2614: 'it\\ttom', 2615: 'it\\tvamos', 2616: 'it\\tvaza', 2617: 'it\\tvocÃª', 2618: 'it\\tvocÃªs', 2619: 'it\\tvÃ¡', 2620: 'it\\tÃ©', 2621: 'itch\\tmeus', 2622: 'itd', 2623: 'itll', 2624: 'its', 2625: 'its\\tsÃ£o', 2626: 'ive', 2627: 'japanese\\teu', 2628: 'japanese\\tsou', 2629: 'japonesa', 2630: 'japonÃªs', 2631: 'jaw', 2632: 'jazz', 2633: 'jazz\\teu', 2634: 'jealous\\teu', 2635: 'jealous\\tsou', 2636: 'jeito', 2637: 'jejuando', 2638: 'jejum', 2639: 'jerk\\tque', 2640: 'jesus', 2641: 'job\\tarrume', 2642: 'job\\tbom', 2643: 'job\\teu', 2644: 'job\\tpreciso', 2645: 'job\\tÃ©', 2646: 'jogar', 2647: 'jogue', 2648: 'join', 2649: 'joke\\tÃ©', 2650: 'jokes\\teu', 2651: 'joking\\testamos', 2652: 'joking\\testou', 2653: 'joking\\teu', 2654: 'jovem', 2655: 'jovens', 2656: 'jump\\teu', 2657: 'jump\\tnÃ£o', 2658: 'jump\\tpule', 2659: 'jump\\tpulem', 2660: 'jumped\\teu', 2661: 'jumped\\ttom', 2662: 'junto', 2663: 'jurou', 2664: 'just', 2665: 'justa', 2666: 'justas', 2667: 'justo', 2668: 'justos', 2669: 'jÃ¡', 2670: 'keep', 2671: 'key\\ttraga', 2672: 'kicked', 2673: 'kid', 2674: 'kidding\\teu', 2675: 'kidding\\tsem', 2676: 'kidding\\tsÃ©rio', 2677: 'kids\\telas', 2678: 'kids\\teles', 2679: 'kids\\tsÃ£o', 2680: 'kill', 2681: 'kind\\tele', 2682: 'kind\\tsede', 2683: 'kind\\tseja', 2684: 'kind\\tsejam', 2685: 'kind\\tsÃª', 2686: 'kind\\ttom', 2687: 'kind\\tvocÃª', 2688: 'kiss', 2689: 'kissed', 2690: 'kissed\\teles', 2691: 'klutz\\teu', 2692: 'klutz\\tsou', 2693: 'kneeled\\ttom', 2694: 'knew', 2695: 'knew\\ttom', 2696: 'know', 2697: 'know\\telas', 2698: 'know\\tele', 2699: 'know\\teles', 2700: 'know\\teu', 2701: 'know\\tnÃ³s', 2702: 'know\\tsei', 2703: 'know\\ttodos', 2704: 'know\\ttom', 2705: 'knows', 2706: 'knows\\tninguÃ©m', 2707: 'knows\\tquem', 2708: 'knows\\ttom', 2709: 'ladies', 2710: 'ladrÃ£o', 2711: 'lar', 2712: 'last\\teu', 2713: 'lasts\\to', 2714: 'late\\teles', 2715: 'late\\testamos', 2716: 'late\\testou', 2717: 'late\\testÃ¡', 2718: 'late\\teu', 2719: 'late\\tnÃ£o', 2720: 'late\\to', 2721: 'late\\ttarde', 2722: 'late\\ttom', 2723: 'late\\tvocÃª', 2724: 'late\\tvou', 2725: 'late\\tÃ©', 2726: 'latem', 2727: 'later\\tligue', 2728: 'later\\tme', 2729: 'laugh\\tnÃ£o', 2730: 'laugh\\to', 2731: 'laughed\\tele', 2732: 'laughed\\teles', 2733: 'laughed\\teu', 2734: 'laughed\\tnÃ³s', 2735: 'laughed\\tri', 2736: 'laughed\\ttom', 2737: 'lavei', 2738: 'law\\teu', 2739: 'law\\tÃ©', 2740: 'lazy\\tele', 2741: 'lazy\\tnÃ£o', 2742: 'lazy\\tnÃ³s', 2743: 'lazy\\ttom', 2744: 'leal', 2745: 'learn\\taprenderei', 2746: 'learn\\teu', 2747: 'learn\\tirei', 2748: 'learn\\tvou', 2749: 'leave', 2750: 'leave\\tabandone', 2751: 'leave\\tdeixame', 2752: 'leave\\tdeixeme', 2753: 'leave\\testou', 2754: 'leave\\teu', 2755: 'leave\\tnÃ£o', 2756: 'leave\\tsaia', 2757: 'leave\\tvamos', 2758: 'leave\\tvou', 2759: 'leave\\tvÃ¡', 2760: 'leaving\\testou', 2761: 'leciono', 2762: 'left', 2763: 'left\\tele', 2764: 'left\\teles', 2765: 'left\\teu', 2766: 'left\\ttom', 2767: 'left\\tvire', 2768: 'leg', 2769: 'legais', 2770: 'legal', 2771: 'legs', 2772: 'lei', 2773: 'leio', 2774: 'leite', 2775: 'lembrados', 2776: 'lembramos', 2777: 'lembro', 2778: 'lendo', 2779: 'lento', 2780: 'ler', 2781: 'let', 2782: 'lets', 2783: 'levantar', 2784: 'levante', 2785: 'levantei', 2786: 'levantou', 2787: 'leve', 2788: 'lhe', 2789: 'liar\\tele', 2790: 'liar\\teu', 2791: 'liar\\tnÃ£o', 2792: 'licenÃ§a', 2793: 'lidei', 2794: 'lie\\taquilo', 2795: 'lie\\teu', 2796: 'lie\\tisso', 2797: 'lie\\tnÃ£o', 2798: 'lied', 2799: 'lied\\teles', 2800: 'lied\\tninguÃ©m', 2801: 'lied\\ttom', 2802: 'lies\\tele', 2803: 'lies\\ttom', 2804: 'life', 2805: 'life\\tÃ©', 2806: 'lifes', 2807: 'lift', 2808: 'liga', 2809: 'ligada', 2810: 'ligar', 2811: 'ligaram', 2812: 'lighten', 2813: 'ligo', 2814: 'ligou', 2815: 'ligue', 2816: 'liguei', 2817: 'like', 2818: 'liked', 2819: 'likes', 2820: 'limpo', 2821: 'lip\\teu', 2822: 'lips\\tleio', 2823: 'listen', 2824: 'listen\\tescuta', 2825: 'listen\\tescutai', 2826: 'listen\\tescute', 2827: 'listen\\tescutem', 2828: 'listen\\tescutemme', 2829: 'listen\\touÃ§a', 2830: 'listen\\touÃ§ame', 2831: 'listen\\tvamos', 2832: 'listened\\tquem', 2833: 'listened\\ttom', 2834: 'listens\\ttom', 2835: 'litter\\tnÃ£o', 2836: 'live', 2837: 'live\\tele', 2838: 'live\\teu', 2839: 'livre', 2840: 'livres', 2841: 'livro', 2842: 'livros', 2843: 'lixo', 2844: 'lobo', 2845: 'locked\\testÃ¡', 2846: 'logo', 2847: 'lonely\\teu', 2848: 'lonely\\to', 2849: 'long\\tnÃ£o', 2850: 'longe', 2851: 'look', 2852: 'look\\tcomo', 2853: 'look\\tdÃ¡', 2854: 'look\\tdÃª', 2855: 'look\\teu', 2856: 'look\\tolha', 2857: 'look\\tolhe', 2858: 'look\\tveja', 2859: 'look\\tvÃª', 2860: 'looked\\teu', 2861: 'looked\\ttom', 2862: 'looking\\tfique', 2863: 'looks', 2864: 'lose\\teu', 2865: 'lose\\ttom', 2866: 'lose\\tvocÃª', 2867: 'lose\\tvou', 2868: 'loser\\tque', 2869: 'losers\\tsomos', 2870: 'losing\\testou', 2871: 'lost', 2872: 'lost\\teles', 2873: 'lost\\tenganÃ¡monos', 2874: 'lost\\testamos', 2875: 'lost\\testou', 2876: 'lost\\teu', 2877: 'lost\\tme', 2878: 'lost\\tnÃ³s', 2879: 'lost\\to', 2880: 'lost\\tperdemos', 2881: 'lost\\tse', 2882: 'lost\\ttom', 2883: 'lost\\tvaite', 2884: 'lost\\tvocÃª', 2885: 'lost\\tvocÃªs', 2886: 'lost\\tvÃ¡se', 2887: 'lot\\teu', 2888: 'lot\\tleio', 2889: 'lot\\tmuito', 2890: 'lot\\ttrabalho', 2891: 'lot\\tÃ©', 2892: 'louco', 2893: 'loucura', 2894: 'love', 2895: 'love\\testou', 2896: 'love\\tisso', 2897: 'love\\to', 2898: 'love\\tque', 2899: 'loved', 2900: 'loved\\teu', 2901: 'loved\\tsou', 2902: 'lovely\\tque', 2903: 'loves', 2904: 'loyal\\teu', 2905: 'luck\\tdesejeme', 2906: 'lucky\\tela', 2907: 'lucky\\teu', 2908: 'lucky\\to', 2909: 'lucky\\ttive', 2910: 'lucky\\ttom', 2911: 'lucky\\tvocÃª', 2912: 'lugar', 2913: 'lunch\\talmoÃ§amos', 2914: 'lutando', 2915: 'lutar', 2916: 'lutaremos', 2917: 'lutarÃ¡', 2918: 'lute', 2919: 'lutem', 2920: 'lutes', 2921: 'lutou', 2922: 'lying\\tele', 2923: 'lying\\testou', 2924: 'lying\\to', 2925: 'lying\\tpare', 2926: 'lying\\ttom', 2927: 'lying\\tvocÃª', 2928: 'lÃ¡', 2929: 'lÃ¡bio', 2930: 'lÃ¡bios', 2931: 'lÃ¡stima', 2932: 'm', 2933: 'machuca', 2934: 'machucada', 2935: 'machucado', 2936: 'machucaram', 2937: 'machucou', 2938: 'mad\\testou', 2939: 'mad\\testÃ¡s', 2940: 'mad\\teu', 2941: 'mad\\tfiquei', 2942: 'mad\\tnÃ£o', 2943: 'mad\\ttom', 2944: 'mad\\tvocÃª', 2945: 'made', 2946: 'madeira', 2947: 'maduros', 2948: 'magic\\tfoi', 2949: 'magro', 2950: 'maid\\tele', 2951: 'mais', 2952: 'make', 2953: 'mal', 2954: 'malas', 2955: 'maldiÃ§Ã£o', 2956: 'malucas', 2957: 'maluco', 2958: 'malucos', 2959: 'malvado', 2960: 'mama', 2961: 'man\\teu', 2962: 'man\\tsou', 2963: 'manage\\tconseguirei', 2964: 'managing\\testou', 2965: 'manda', 2966: 'mandona', 2967: 'mandÃ£o', 2968: 'mandÃ\\xadbula', 2969: 'maneira', 2970: 'mantenha', 2971: 'many', 2972: 'map\\teu', 2973: 'map\\tpreciso', 2974: 'mapa', 2975: 'maravilha', 2976: 'maria', 2977: 'married\\teu', 2978: 'married\\tsou', 2979: 'marry', 2980: 'mary', 2981: 'mary\\teu', 2982: 'mary\\tquero', 2983: 'mary\\ttom', 2984: 'massa', 2985: 'matarei', 2986: 'matemÃ¡tica', 2987: 'math', 2988: 'math\\teu', 2989: 'matters\\timporta', 2990: 'matters\\tisso', 2991: 'mature\\testamos', 2992: 'mature\\teu', 2993: 'mature\\tnÃ³s', 2994: 'mature\\tsomos', 2995: 'maturo', 2996: 'mau', 2997: 'maus', 2998: 'may', 2999: 'maybe\\teu', 3000: 'maÃ§Ã£s', 3001: 'me', 3002: 'me\\tagora', 3003: 'me\\tajudame', 3004: 'me\\tajudeme', 3005: 'me\\tajudemme', 3006: 'me\\tassustoume', 3007: 'me\\tbeijeme', 3008: 'me\\tcase', 3009: 'me\\tcom', 3010: 'me\\tconfia', 3011: 'me\\tconfie', 3012: 'me\\tdance', 3013: 'me\\tdanÃ§a', 3014: 'me\\tdeixeme', 3015: 'me\\tdesculpa', 3016: 'me\\tdesculpe', 3017: 'me\\tdigame', 3018: 'me\\tdigamme', 3019: 'me\\tdizme', 3020: 'me\\tdÃªme', 3021: 'me\\tei', 3022: 'me\\tela', 3023: 'me\\telas', 3024: 'me\\tele', 3025: 'me\\teles', 3026: 'me\\tescrevame', 3027: 'me\\tesqueÃ§ame', 3028: 'me\\tesse', 3029: 'me\\teste', 3030: 'me\\tfale', 3031: 'me\\tfaz', 3032: 'me\\tfaÃ§a', 3033: 'me\\tfaÃ§am', 3034: 'me\\tficame', 3035: 'me\\tisso', 3036: 'me\\tme', 3037: 'me\\tmostreme', 3038: 'me\\tnÃ£o', 3039: 'me\\to', 3040: 'me\\tobserveme', 3041: 'me\\tperdoeme', 3042: 'me\\tperdÃ£o', 3043: 'me\\tpor', 3044: 'me\\tquem', 3045: 'me\\trespondame', 3046: 'me\\trespondamme', 3047: 'me\\treze', 3048: 'me\\trezem', 3049: 'me\\tsegurese', 3050: 'me\\tsentese', 3051: 'me\\tsigame', 3052: 'me\\tsigamme', 3053: 'me\\tsolteme', 3054: 'me\\tsou', 3055: 'me\\tsurpreendeume', 3056: 'me\\ttom', 3057: 'me\\tvem', 3058: 'me\\tvenha', 3059: 'me\\tvocÃª', 3060: 'me\\tvocÃªs', 3061: 'me\\tvote', 3062: 'me\\tvÃ¡', 3063: 'me\\tÃ©', 3064: 'mean', 3065: 'mean\\tnÃ£o', 3066: 'means', 3067: 'meat', 3068: 'meat\\teu', 3069: 'meat\\tnÃ³s', 3070: 'medo', 3071: 'melhor', 3072: 'memorize', 3073: 'men', 3074: 'men\\tnÃ³s', 3075: 'men\\tsomos', 3076: 'meninas', 3077: 'menino', 3078: 'meninos', 3079: 'mente', 3080: 'menti', 3081: 'mentindo', 3082: 'mentir', 3083: 'mentira', 3084: 'mentiram', 3085: 'mentiroso', 3086: 'mentiu', 3087: 'merciless\\tseja', 3088: 'mesmo', 3089: 'mess\\teu', 3090: 'mess\\tque', 3091: 'met', 3092: 'met\\tacabamos', 3093: 'meu', 3094: 'meus', 3095: 'mexa', 3096: 'mexendo', 3097: 'mexer', 3098: 'milk\\tbebi', 3099: 'milk\\teu', 3100: 'mim', 3101: 'mind', 3102: 'mind\\tesquece', 3103: 'mind\\tesqueÃ§a', 3104: 'mind\\teu', 3105: 'mind\\tnÃ£o', 3106: 'mind\\ttudo', 3107: 'mind\\tvocÃª', 3108: 'mine\\ta', 3109: 'mine\\teles', 3110: 'mine\\tera', 3111: 'mine\\tessa', 3112: 'mine\\tesse', 3113: 'mine\\teu', 3114: 'mine\\tisso', 3115: 'mine\\tisto', 3116: 'mine\\tmary', 3117: 'mine\\tpegue', 3118: 'mine\\tpreciso', 3119: 'mine\\tquero', 3120: 'mine\\tsÃ£o', 3121: 'mine\\tvocÃª', 3122: 'mine\\tÃ©', 3123: 'minha', 3124: 'minhas', 3125: 'minta', 3126: 'minto', 3127: 'minuto', 3128: 'miss', 3129: 'missed', 3130: 'missed\\tvocÃª', 3131: 'missed\\tvocÃªs', 3132: 'missing\\testÃ¡', 3133: 'mistaken\\testou', 3134: 'moaned\\ttom', 3135: 'mocked', 3136: 'modo', 3137: 'molhada', 3138: 'molhado', 3139: 'momento', 3140: 'monday\\tÃ©', 3141: 'money', 3142: 'money\\teu', 3143: 'money\\tnecessito', 3144: 'money\\tpreciso', 3145: 'monge', 3146: 'monk\\tsou', 3147: 'moramos', 3148: 'morder', 3149: 'mordeu', 3150: 'mordo', 3151: 'more\\tconteme', 3152: 'more\\tele', 3153: 'more\\tfale', 3154: 'more\\tprecisamos', 3155: 'more\\tpreciso', 3156: 'morning\\tbom', 3157: 'moro', 3158: 'morra', 3159: 'morram', 3160: 'morre', 3161: 'morremos', 3162: 'morrendo', 3163: 'morrer', 3164: 'morrera', 3165: 'morreram', 3166: 'morrerÃ¡', 3167: 'morreu', 3168: 'morta', 3169: 'mortas', 3170: 'morto', 3171: 'mortos', 3172: 'mosca', 3173: 'mostra', 3174: 'mostre', 3175: 'mova', 3176: 'move', 3177: 'move\\tnÃ£o', 3178: 'moved\\ttom', 3179: 'moving\\tcontinue', 3180: 'moving\\tmexase', 3181: 'moving\\tpara', 3182: 'moving\\tpare', 3183: 'moving\\tparem', 3184: 'mudei', 3185: 'mudou', 3186: 'muito', 3187: 'mulher', 3188: 'mulheres', 3189: 'mundo', 3190: 'music\\teu', 3191: 'muslim\\tsou', 3192: 'must', 3193: 'muÃ§ulmana', 3194: 'muÃ§ulmano', 3195: 'my', 3196: 'myself\\teu', 3197: 'mÃ¡', 3198: 'mÃ¡gico', 3199: 'mÃ¡s', 3200: 'mÃ£o', 3201: 'mÃ£os', 3202: 'mÃ£ozinha', 3203: 'mÃ©dico', 3204: 'mÃºsica', 3205: 'na', 3206: 'nada', 3207: 'nadam', 3208: 'nadar', 3209: 'nadaram', 3210: 'nadei', 3211: 'nadou', 3212: 'nailed', 3213: 'naive\\teu', 3214: 'naive\\ttom', 3215: 'naked\\testou', 3216: 'naked\\to', 3217: 'namorando', 3218: 'nap\\teu', 3219: 'nap\\tpreciso', 3220: 'nap\\ttire', 3221: 'nariz', 3222: 'nasty\\to', 3223: 'navio', 3224: 'neat\\ttom', 3225: 'necessidades', 3226: 'necessito', 3227: 'need', 3228: 'needed', 3229: 'needs', 3230: 'needs\\teu', 3231: 'needy\\tvocÃª', 3232: 'nela', 3233: 'nele', 3234: 'nem', 3235: 'nenhum', 3236: 'nervosa', 3237: 'nervoso', 3238: 'nervous\\testou', 3239: 'nervous\\teu', 3240: 'neutral\\teu', 3241: 'neutro', 3242: 'nevando', 3243: 'nevar', 3244: 'neve', 3245: 'never', 3246: 'new\\talgo', 3247: 'new\\talguma', 3248: 'new\\teles', 3249: 'new\\tisso', 3250: 'new\\to', 3251: 'new\\tquais', 3252: 'new\\tque', 3253: 'new\\tvocÃª', 3254: 'new\\tvocÃªs', 3255: 'news\\ttenho', 3256: 'next\\tquem', 3257: 'next\\tsou', 3258: 'next\\ttom', 3259: 'next\\tvocÃª', 3260: 'nice', 3261: 'nice\\tela', 3262: 'nice\\tele', 3263: 'nice\\tfoi', 3264: 'nice\\tisso', 3265: 'nice\\tisto', 3266: 'nice\\tque', 3267: 'nice\\tseja', 3268: 'nice\\tsejam', 3269: 'nice\\tÃ©', 3270: 'night\\tboa', 3271: 'night\\tera', 3272: 'night\\tque', 3273: 'ninguÃ©m', 3274: 'no', 3275: 'no\\tah', 3276: 'no\\teles', 3277: 'no\\teu', 3278: 'no\\tisso', 3279: 'no\\tnÃ£o', 3280: 'no\\ttom', 3281: 'nobody', 3282: 'nodded\\ttom', 3283: 'noite', 3284: 'nojento', 3285: 'normal', 3286: 'normal\\teu', 3287: 'normal\\tisso', 3288: 'norteamericano', 3289: 'nos', 3290: 'nose', 3291: 'nosso', 3292: 'nosy\\ttom', 3293: 'not', 3294: 'not\\tespero', 3295: 'not\\teu', 3296: 'not\\tpor', 3297: 'notei', 3298: 'nothing\\teu', 3299: 'nothing\\tisso', 3300: 'noticed\\teu', 3301: 'noticed\\tnotei', 3302: 'noticed\\ttom', 3303: 'notou', 3304: 'novamente', 3305: 'novas', 3306: 'novidade', 3307: 'novidades', 3308: 'novo', 3309: 'novos', 3310: 'now', 3311: 'now\\tagora', 3312: 'now\\tcomece', 3313: 'now\\tele', 3314: 'now\\testou', 3315: 'now\\teu', 3316: 'now\\tfique', 3317: 'now\\tsaia', 3318: 'now\\tvai', 3319: 'now\\tvÃ¡', 3320: 'ntt', 3321: 'ntt\\teu', 3322: 'nu', 3323: 'nublado', 3324: 'numb\\ttom', 3325: 'nunca', 3326: 'nurse\\teu', 3327: 'nuts\\testÃ¡', 3328: 'nuts\\testÃ¡s', 3329: 'nuts\\ttom', 3330: 'nuts\\tvocÃª', 3331: 'nÃ', 3332: 'nÃ£o', 3333: 'nÃ³s', 3334: 'o', 3335: 'obedecer', 3336: 'obedeceram', 3337: 'obedeceremos', 3338: 'obedeceu', 3339: 'obese\\testou', 3340: 'obeso', 3341: 'obey\\teu', 3342: 'obey\\tnÃ³s', 3343: 'obey\\tobedecerei', 3344: 'obeyed\\teles', 3345: 'obeyed\\ttom', 3346: 'objection\\tsem', 3347: 'objective\\tseja', 3348: 'objective\\tsejam', 3349: 'objetiva', 3350: 'objetivo', 3351: 'objetivos', 3352: 'objeto', 3353: 'objeÃ§Ãµes', 3354: 'obrigada', 3355: 'obrigado', 3356: 'ocd\\teu', 3357: 'ocd\\ttom', 3358: 'ocupada', 3359: 'ocupadas', 3360: 'ocupado', 3361: 'ocupados', 3362: 'odd\\tfoi', 3363: 'odd\\tisso', 3364: 'odd\\ttom', 3365: 'odd\\tÃ©', 3366: 'odeia', 3367: 'odeiam', 3368: 'odeio', 3369: 'odiado', 3370: 'odiamos', 3371: 'of', 3372: 'ofendido', 3373: 'off', 3374: 'off\\tacalmese', 3375: 'off\\tdesligao', 3376: 'off\\tdesligue', 3377: 'off\\tdesliguea', 3378: 'off\\tdesligueo', 3379: 'off\\tele', 3380: 'off\\to', 3381: 'off\\tpara', 3382: 'off\\trecua', 3383: 'off\\trecue', 3384: 'off\\trecuem', 3385: 'off\\ttira', 3386: 'off\\ttire', 3387: 'off\\ttirem', 3388: 'offended\\testou', 3389: 'often', 3390: 'oh', 3391: 'oito', 3392: 'ok', 3393: 'ok\\testamos', 3394: 'ok\\testou', 3395: 'ok\\testÃ¡', 3396: 'ok\\teu', 3397: 'ok\\tfoi', 3398: 'ok\\tnÃ³s', 3399: 'ok\\to', 3400: 'ok\\ttom', 3401: 'ok\\ttudo', 3402: 'ok\\ttÃ¡', 3403: 'ok\\tvai', 3404: 'ok\\tvamos', 3405: 'ok\\tvocÃª', 3406: 'ok\\tvocÃªs', 3407: 'okay\\testou', 3408: 'okay\\testÃ¡', 3409: 'okay\\teu', 3410: 'okay\\ttudo', 3411: 'okay\\tvocÃª', 3412: 'old', 3413: 'old\\ta', 3414: 'old\\tas', 3415: 'old\\tela', 3416: 'old\\tele', 3417: 'old\\teles', 3418: 'old\\testais', 3419: 'old\\teu', 3420: 'old\\to', 3421: 'old\\tos', 3422: 'old\\tsintome', 3423: 'old\\tsois', 3424: 'old\\ttom', 3425: 'old\\ttu', 3426: 'old\\tvocÃª', 3427: 'old\\tvocÃªs', 3428: 'old\\tvÃ³s', 3429: 'old\\tÃ©', 3430: 'old\\tÃ©s', 3431: 'olhada', 3432: 'olhadela', 3433: 'olhando', 3434: 'olhar', 3435: 'olhe', 3436: 'olhei', 3437: 'olhos', 3438: 'olhou', 3439: 'olÃ¡', 3440: 'ombros', 3441: 'omen\\tÃ©', 3442: 'on', 3443: 'on\\ta', 3444: 'on\\taguarde', 3445: 'on\\taguardem', 3446: 'on\\tah', 3447: 'on\\tavante', 3448: 'on\\tcoloqueo', 3449: 'on\\tdirija', 3450: 'on\\texperimentea', 3451: 'on\\texperimenteo', 3452: 'on\\tliguea', 3453: 'on\\tligueo', 3454: 'on\\tmexase', 3455: 'on\\tqual', 3456: 'on\\tvamos', 3457: 'on\\tvenha', 3458: 'on\\tvÃ¡', 3459: 'once', 3460: 'once\\tvenha', 3461: 'one', 3462: 'one\\tele', 3463: 'one\\tescolha', 3464: 'one\\teu', 3465: 'one\\tnÃ£o', 3466: 'one\\tnÃ³s', 3467: 'one\\tquero', 3468: 'online', 3469: 'online\\testou', 3470: 'online\\teu', 3471: 'open', 3472: 'open\\testÃ¡', 3473: 'opera\\teu', 3474: 'or', 3475: 'orei', 3476: 'os', 3477: 'otÃ¡rios', 3478: 'ou', 3479: 'our', 3480: 'ouro', 3481: 'ours\\tisso', 3482: 'ours\\tÃ©', 3483: 'out', 3484: 'out\\tafastese', 3485: 'out\\tagora', 3486: 'out\\tajuda', 3487: 'out\\tajude', 3488: 'out\\tajudei', 3489: 'out\\tajudeme', 3490: 'out\\tatenÃ§Ã£o', 3491: 'out\\tchame', 3492: 'out\\tcuidado', 3493: 'out\\tdeixame', 3494: 'out\\tdeixeme', 3495: 'out\\tele', 3496: 'out\\testou', 3497: 'out\\teu', 3498: 'out\\tfora', 3499: 'out\\tindique', 3500: 'out\\tme', 3501: 'out\\tnÃ£o', 3502: 'out\\tnÃ³s', 3503: 'out\\to', 3504: 'out\\tpara', 3505: 'out\\tpare', 3506: 'out\\tparem', 3507: 'out\\tpreencha', 3508: 'out\\tpreste', 3509: 'out\\tsai', 3510: 'out\\tsaia', 3511: 'out\\tsaiam', 3512: 'out\\ttom', 3513: 'out\\tvamos', 3514: 'outra', 3515: 'outro', 3516: 'outside\\tespera', 3517: 'outside\\tespere', 3518: 'outside\\tesperem', 3519: 'outside\\tvenha', 3520: 'ouvidos', 3521: 'ouvindo', 3522: 'ouvir', 3523: 'ouÃ§o', 3524: 'over', 3525: 'over\\tacabou', 3526: 'over\\tchega', 3527: 'over\\tchegue', 3528: 'over\\trecomece', 3529: 'over\\tvem', 3530: 'over\\tvenha', 3531: 'over\\tvenham', 3532: 'over\\tvire', 3533: 'overslept\\tdormi', 3534: 'overslept\\tdormimos', 3535: 'overslept\\teu', 3536: 'ovo', 3537: 'owe', 3538: 'own\\tpegue', 3539: 'p', 3540: 'pa', 3541: 'paciÃªncia', 3542: 'packing\\testou', 3543: 'padeiro', 3544: 'pagando', 3545: 'pagar', 3546: 'pagarei', 3547: 'pagarÃ¡', 3548: 'pago', 3549: 'pagou', 3550: 'paguei', 3551: 'pai', 3552: 'paid', 3553: 'paid\\teu', 3554: 'paid\\to', 3555: 'pain\\testou', 3556: 'pain\\tque', 3557: 'paint\\teu', 3558: 'pale\\tele', 3559: 'pale\\ttom', 3560: 'panic\\tnÃ£o', 3561: 'panicked\\teu', 3562: 'panicked\\tquem', 3563: 'panicked\\ttom', 3564: 'papo', 3565: 'para', 3566: 'parada', 3567: 'parado', 3568: 'parafuso', 3569: 'parar', 3570: 'pararam', 3571: 'pardon', 3572: 'pare', 3573: 'parece', 3574: 'parecida', 3575: 'parecido', 3576: 'parei', 3577: 'pareÃ§o', 3578: 'paris', 3579: 'paris\\testou', 3580: 'paris\\teu', 3581: 'parou', 3582: 'part\\teu', 3583: 'parte', 3584: 'partida', 3585: 'partir', 3586: 'party\\tvamos', 3587: 'pasmado', 3588: 'pass\\tnÃ³s', 3589: 'passaram', 3590: 'passaremos', 3591: 'passed\\tanos', 3592: 'passed\\tpassaram', 3593: 'passo', 3594: 'passou', 3595: 'pathetic\\tque', 3596: 'patient\\ttenha', 3597: 'patinar', 3598: 'patrÃ£o', 3599: 'patÃ©tico', 3600: 'pay', 3601: 'pay\\ta', 3602: 'pay\\teu', 3603: 'pay\\tnÃ³s', 3604: 'pay\\tquem', 3605: 'pay\\ttom', 3606: 'paying\\tnÃ³s', 3607: 'paying\\tquem', 3608: 'paz', 3609: 'pbs', 3610: 'pbs\\tassistimos', 3611: 'pedi', 3612: 'pegar', 3613: 'pegarei', 3614: 'pego', 3615: 'pegou', 3616: 'peguei', 3617: 'peixe', 3618: 'peixes', 3619: 'pelada', 3620: 'pelado', 3621: 'pelo', 3622: 'pen\\teu', 3623: 'pen\\ttenho', 3624: 'pena', 3625: 'pensando', 3626: 'pequena', 3627: 'pequeno', 3628: 'perco', 3629: 'perdedor', 3630: 'perdedora', 3631: 'perdedores', 3632: 'perdemos', 3633: 'perdendo', 3634: 'perder', 3635: 'perderam', 3636: 'perdeu', 3637: 'perdi', 3638: 'perdida', 3639: 'perdidas', 3640: 'perdido', 3641: 'perdidos', 3642: 'perdoe', 3643: 'perdÃ£o', 3644: 'perfect\\testÃ¡', 3645: 'perfect\\tisso', 3646: 'perfect\\tisto', 3647: 'perfect\\tperfeito', 3648: 'perfect\\tÃ©', 3649: 'perfeito', 3650: 'pergunta', 3651: 'perguntar', 3652: 'perguntarei', 3653: 'pergunte', 3654: 'perguntei', 3655: 'perguntou', 3656: 'perguntÃ¡lo', 3657: 'perna', 3658: 'pernas', 3659: 'perth', 3660: 'perth\\testou', 3661: 'perto', 3662: 'pesado', 3663: 'pesca', 3664: 'pessoal', 3665: 'pessoas', 3666: 'phoned', 3667: 'phoned\\teu', 3668: 'phoned\\tquem', 3669: 'phoned\\ttom', 3670: 'phony\\tque', 3671: 'piada', 3672: 'piadas', 3673: 'piano', 3674: 'piano\\teu', 3675: 'pick', 3676: 'picky\\tsou', 3677: 'picky\\ttom', 3678: 'piedosa', 3679: 'piedoso', 3680: 'pig\\ttom', 3681: 'pilot\\tsou', 3682: 'piloto', 3683: 'pior', 3684: 'piscou', 3685: 'pisquei', 3686: 'pity\\tpena', 3687: 'pity\\tque', 3688: 'pity\\tÃ©', 3689: 'pizza', 3690: 'pizza\\teu', 3691: 'planos', 3692: 'plans\\teu', 3693: 'plant\\tÃ©', 3694: 'planta', 3695: 'plantas', 3696: 'plants', 3697: 'play', 3698: 'play\\tjoguemos', 3699: 'play\\tvamos', 3700: 'please', 3701: 'please\\tcarne', 3702: 'please\\tchÃ¡', 3703: 'please\\tdiga', 3704: 'please\\tfalem', 3705: 'please\\tpeixe', 3706: 'po', 3707: 'pobre', 3708: 'pobres', 3709: 'pode', 3710: 'podeis', 3711: 'podem', 3712: 'podemos', 3713: 'poderia', 3714: 'podes', 3715: 'poet\\tele', 3716: 'poet\\tsou', 3717: 'poeta', 3718: 'point', 3719: 'poison\\tÃ©', 3720: 'policiais', 3721: 'polite\\to', 3722: 'polite\\ttom', 3723: 'pontuais', 3724: 'pontual', 3725: 'poor\\tnÃ£o', 3726: 'poor\\tsomos', 3727: 'poor\\tsou', 3728: 'popular', 3729: 'popular\\teu', 3730: 'por', 3731: 'porco', 3732: 'porqu', 3733: 'porquÃª', 3734: 'porta', 3735: 'posso', 3736: 'possui', 3737: 'possÃ\\xadvel', 3738: 'pouco', 3739: 'pra', 3740: 'pray', 3741: 'pray\\to', 3742: 'prayed\\teu', 3743: 'prayed\\ttom', 3744: 'precipitado', 3745: 'precisa', 3746: 'precisam', 3747: 'precisamos', 3748: 'precisava', 3749: 'preciso', 3750: 'pregnant\\teu', 3751: 'preguiÃ§osas', 3752: 'preguiÃ§oso', 3753: 'preguiÃ§osos', 3754: 'preocupada', 3755: 'preocupe', 3756: 'prepaid\\ttom', 3757: 'preparada', 3758: 'preparadas', 3759: 'preparado', 3760: 'preparados', 3761: 'preparamos', 3762: 'prepared\\testeja', 3763: 'prepared\\testejam', 3764: 'prepared\\testou', 3765: 'preparou', 3766: 'presa', 3767: 'presente', 3768: 'preso', 3769: 'pressÃ¡gio', 3770: 'prestÃ\\xadgio', 3771: 'pretty\\tÃ©', 3772: 'primavera', 3773: 'primeira', 3774: 'primeiro', 3775: 'pro\\tsou', 3776: 'pro\\ttom', 3777: 'problem\\tnÃ£o', 3778: 'problem\\tproblema', 3779: 'problem\\tsem', 3780: 'problema', 3781: 'process', 3782: 'processar', 3783: 'processou', 3784: 'profissional', 3785: 'profundidade', 3786: 'prometemos', 3787: 'prometeram', 3788: 'prometeu', 3789: 'prometo', 3790: 'promise\\teu', 3791: 'promised\\tnÃ³s', 3792: 'promised\\tprometemos', 3793: 'promised\\tprometi', 3794: 'promised\\ttom', 3795: 'promised\\tvocÃª', 3796: 'promised\\tvocÃªs', 3797: 'pront', 3798: 'pronta', 3799: 'pronto', 3800: 'prontos', 3801: 'proof\\teu', 3802: 'prova', 3803: 'provar', 3804: 'prudent\\teu', 3805: 'prudent\\tsou', 3806: 'prudente', 3807: 'prÃ³prio', 3808: 'prÃ³xima', 3809: 'prÃ³ximo', 3810: 'pude', 3811: 'pular', 3812: 'pule', 3813: 'pulei', 3814: 'pulou', 3815: 'punctual\\tseja', 3816: 'punctual\\tsejam', 3817: 'punctual\\tsou', 3818: 'purist\\tsou', 3819: 'purista', 3820: 'push', 3821: 'push\\tnÃ£o', 3822: 'pushy\\ttom', 3823: 'put', 3824: 'puzzled\\testou', 3825: 'puzzled\\teu', 3826: 'pÃ¡lido', 3827: 'pÃ¡scoa', 3828: 'pÃ¡ssaro', 3829: 'pÃ¡ssaros', 3830: 'pÃ¢nico', 3831: 'pÃ£o', 3832: 'pÃ©', 3833: 'pÃ©s', 3834: 'quadrado', 3835: 'quadril', 3836: 'qualquer', 3837: 'quando', 3838: 'quarenta', 3839: 'quase', 3840: 'que', 3841: 'quebrado', 3842: 'quebrou', 3843: 'quebrouo', 3844: 'queima', 3845: 'queimar', 3846: 'quem', 3847: 'quente', 3848: 'quer', 3849: 'querem', 3850: 'queremos', 3851: 'quero', 3852: 'questions\\talguma', 3853: 'quick\\tvem', 3854: 'quick\\tvenha', 3855: 'quickly\\teu', 3856: 'quickly\\tvem', 3857: 'quickly\\tvenha', 3858: 'quickly\\tvenham', 3859: 'quiet', 3860: 'quiet\\tfica', 3861: 'quiet\\tfique', 3862: 'quiet\\ttudo', 3863: 'quieta', 3864: 'quietly\\tsaia', 3865: 'quieto', 3866: 'quiser', 3867: 'quit\\teu', 3868: 'quit\\tfomos', 3869: 'quit\\tnÃ³s', 3870: 'quit\\tquem', 3871: 'quit\\ttom', 3872: 'quit\\tvamos', 3873: 'quites', 3874: 'quÃª', 3875: 'racional', 3876: 'rain\\tpode', 3877: 'rain\\ttalvez', 3878: 'rained\\testava', 3879: 'rainy\\to', 3880: 'raised', 3881: 'raiva', 3882: 'ran', 3883: 'ran\\tele', 3884: 'ran\\teu', 3885: 'ran\\to', 3886: 'ran\\tquem', 3887: 'ran\\ttom', 3888: 'ranzinza', 3889: 'raramente', 3890: 'rarely', 3891: 'rash\\tnÃ£o', 3892: 'rational\\teu', 3893: 'ratos', 3894: 'rats\\teu', 3895: 'razoÃ¡vel', 3896: 'razÃ£o', 3897: 'read', 3898: 'read\\tele', 3899: 'read\\teu', 3900: 'read\\tleio', 3901: 'reading\\tcontinue', 3902: 'reading\\tele', 3903: 'reading\\tpare', 3904: 'ready\\testou', 3905: 'ready\\testÃ¡', 3906: 'ready\\tnos', 3907: 'ready\\tnÃ³s', 3908: 'ready\\ttom', 3909: 'real', 3910: 'real\\tacorda', 3911: 'real\\tcai', 3912: 'real\\tisso', 3913: 'real\\tÃ©', 3914: 'realista', 3915: 'realistic\\tseja', 3916: 'really\\tmesmo', 3917: 'really\\tsÃ©rio', 3918: 'really\\tÃ©', 3919: 'realmente', 3920: 'reasonable\\tseja', 3921: 'recess', 3922: 'recinto', 3923: 'recordo', 3924: 'recovered\\teu', 3925: 'recreio', 3926: 'recuou', 3927: 'recuperei', 3928: 'recusaram', 3929: 'recusou', 3930: 'red\\tisso', 3931: 'red\\to', 3932: 'red\\tÃ©', 3933: 'redor', 3934: 'reforÃ§o', 3935: 'reforÃ§os', 3936: 'refused\\teles', 3937: 'refused\\ttom', 3938: 'regras', 3939: 'regret', 3940: 'rei', 3941: 'relax\\tapenas', 3942: 'relax\\tei', 3943: 'relax\\tpor', 3944: 'relax\\trelaxa', 3945: 'relax\\trelaxe', 3946: 'relax\\ttente', 3947: 'relaxa', 3948: 'relaxada', 3949: 'relaxado', 3950: 'relaxar', 3951: 'relaxe', 3952: 'relaxed\\testou', 3953: 'relaxed\\ttom', 3954: 'relaxem', 3955: 'relaxou', 3956: 'release', 3957: 'reliable\\teu', 3958: 'relÃ³gio', 3959: 'remember', 3960: 'remember\\teu', 3961: 'remember\\tlembramos', 3962: 'remember\\tlembro', 3963: 'remember\\tme', 3964: 'remember\\tnÃ³s', 3965: 'rendo', 3966: 'renunciei', 3967: 'renunciou', 3968: 'replace', 3969: 'reply\\to', 3970: 'repugnante', 3971: 'reservado', 3972: 'reserved\\teu', 3973: 'resign\\teu', 3974: 'resigned\\tele', 3975: 'resigned\\teu', 3976: 'resigned\\tquem', 3977: 'resigned\\ttom', 3978: 'resmungou', 3979: 'resmunguei', 3980: 'respeito', 3981: 'respond\\tnÃ£o', 3982: 'responda', 3983: 'respondam', 3984: 'respondas', 3985: 'responde', 3986: 'respondeu', 3987: 'rest\\tdeixe', 3988: 'rest\\tdescansa', 3989: 'rest\\tdescanse', 3990: 'rest\\tdescansem', 3991: 'rest\\ttente', 3992: 'resting\\testou', 3993: 'retired\\testou', 3994: 'retired\\teu', 3995: 'retired\\tsou', 3996: 'retired\\ttom', 3997: 'rever', 3998: 'review\\tvamos', 3999: 'revisar', 4000: 'rezei', 4001: 'rezou', 4002: 'ri', 4003: 'ria', 4004: 'riam', 4005: 'rias', 4006: 'rica', 4007: 'rice\\tele', 4008: 'rice\\teu', 4009: 'rice\\tgosto', 4010: 'rich\\tele', 4011: 'rich\\teu', 4012: 'rich\\tnÃ£o', 4013: 'rich\\tnÃ³s', 4014: 'rich\\ttom', 4015: 'rich\\tvocÃª', 4016: 'rico', 4017: 'ricos', 4018: 'ridÃ\\xadculo', 4019: 'right\\tdobre', 4020: 'right\\tele', 4021: 'right\\testamos', 4022: 'right\\teu', 4023: 'right\\texatamente', 4024: 'right\\tnÃ³s', 4025: 'right\\tquem', 4026: 'right\\ttenho', 4027: 'right\\ttom', 4028: 'right\\tvire', 4029: 'right\\tvocÃª', 4030: 'rimos', 4031: 'rindo', 4032: 'riram', 4033: 'risada', 4034: 'risadinha', 4035: 'risadinhas', 4036: 'risk', 4037: 'riu', 4038: 'riuse', 4039: 'robbed\\teu', 4040: 'rock', 4041: 'rock\\teu', 4042: 'rocks\\ttom', 4043: 'roma', 4044: 'romantic\\tque', 4045: 'rome\\testou', 4046: 'rome\\teu', 4047: 'romÃ¢ntico', 4048: 'ronca', 4049: 'ronco', 4050: 'room\\ttemos', 4051: 'rope\\tsegure', 4052: 'rope\\tsegurem', 4053: 'rosa', 4054: 'rose\\testou', 4055: 'rose\\tvejo', 4056: 'rosquinha', 4057: 'rosto', 4058: 'roubado', 4059: 'roubo', 4060: 'roubou', 4061: 'rude', 4062: 'rude\\tnÃ£o', 4063: 'rude\\ttom', 4064: 'ruim', 4065: 'ruins', 4066: 'rules\\teu', 4067: 'rumor\\tÃ©', 4068: 'run', 4069: 'run\\tcorra', 4070: 'run\\tcorram', 4071: 'run\\tcorre', 4072: 'run\\teu', 4073: 'run\\tnÃ£o', 4074: 'run\\tvocÃª', 4075: 'runner\\teu', 4076: 'running\\tcontinue', 4077: 'runs', 4078: 'runs\\tela', 4079: 'runs\\tele', 4080: 'rush', 4081: 'ruthless\\tseja', 4082: 'ruÃ\\xaddo', 4083: 'rÃ¡pi', 4084: 'rÃ¡pida', 4085: 'rÃ¡pidas', 4086: 'rÃ¡pido', 4087: 'rÃ¡pidos', 4088: 'sabe', 4089: 'sabem', 4090: 'sabemos', 4091: 'saber', 4092: 'saberÃ£o', 4093: 'sabia', 4094: 'sabÃ\\xadamos', 4095: 'sad\\tagora', 4096: 'sad\\testou', 4097: 'sad\\teu', 4098: 'sad\\tfoi', 4099: 'sad\\tnÃ£o', 4100: 'sad\\tnÃ³s', 4101: 'sad\\to', 4102: 'sad\\ttom', 4103: 'sad\\tvocÃª', 4104: 'sad\\tvocÃªs', 4105: 'sad\\tÃ©', 4106: 'safe', 4107: 'safe\\teles', 4108: 'safe\\testamos', 4109: 'safe\\teu', 4110: 'safe\\tnÃ³s', 4111: 'safe\\to', 4112: 'safe\\ttom', 4113: 'safe\\ttu', 4114: 'safe\\tvocÃª', 4115: 'safe\\tÃ©', 4116: 'safely\\tdirija', 4117: 'sai', 4118: 'saia', 4119: 'said', 4120: 'sair', 4121: 'saiu', 4122: 'salvei', 4123: 'salvos', 4124: 'salvou', 4125: 'sand\\teu', 4126: 'sangrando', 4127: 'sangrava', 4128: 'sangue', 4129: 'sapatos', 4130: 'sat', 4131: 'saudade', 4132: 'saudades', 4133: 'saudÃ¡vel', 4134: 'save', 4135: 'saved', 4136: 'saved\\testamos', 4137: 'saved\\tnÃ³s', 4138: 'saw', 4139: 'say', 4140: 'say\\tfaÃ§a', 4141: 'say\\tfaÃ§am', 4142: 'say\\tnÃ£o', 4143: 'says\\tfaz', 4144: 'says\\tfaÃ§a', 4145: 'says\\tfaÃ§am', 4146: 'saÃ\\xad', 4147: 'saÃ\\xadmos', 4148: 'saÃ\\xadram', 4149: 'saÃºde', 4150: 'scare', 4151: 'scared', 4152: 'scared\\testou', 4153: 'scared\\teu', 4154: 'scared\\tfiquei', 4155: 'scares', 4156: 'scary\\ttom', 4157: 'scary\\ttu', 4158: 'scary\\tvocÃª', 4159: 'school\\tcomo', 4160: 'school\\tvai', 4161: 'school\\tvÃ¡', 4162: 'scream\\tnÃ£o', 4163: 'scream\\tnÃ³s', 4164: 'screamed\\teu', 4165: 'screamed\\tgritei', 4166: 'screamed\\ttom', 4167: 'se', 4168: 'seat\\tsentate', 4169: 'seat\\tsentemse', 4170: 'seat\\tsentese', 4171: 'sec\\tsÃ³', 4172: 'secret\\tÃ©', 4173: 'security\\tchame', 4174: 'security\\tchamem', 4175: 'sede', 4176: 'sedento', 4177: 'see', 4178: 'see\\tdeixame', 4179: 'see\\tdeixeme', 4180: 'see\\tdeixemme', 4181: 'see\\tdÃ¡', 4182: 'see\\testou', 4183: 'see\\teu', 4184: 'see\\tnÃ£o', 4185: 'see\\tnÃ³s', 4186: 'see\\tvejamos', 4187: 'see\\tvenha', 4188: 'see\\tveremos', 4189: 'see\\tvocÃª', 4190: 'seems', 4191: 'seen', 4192: 'seen\\tela', 4193: 'segredo', 4194: 'seguiremos', 4195: 'segunda', 4196: 'segundafeira', 4197: 'segundinho', 4198: 'seguranÃ§a', 4199: 'seguro', 4200: 'seguros', 4201: 'sei', 4202: 'seize', 4203: 'seja', 4204: 'sejas', 4205: 'selfish\\teu', 4206: 'sell', 4207: 'sells', 4208: 'sem', 4209: 'sempre', 4210: 'senhor', 4211: 'senhora', 4212: 'senhoras', 4213: 'senhores', 4214: 'sensible\\ttem', 4215: 'sensible\\ttende', 4216: 'sensible\\ttenha', 4217: 'sensible\\ttenham', 4218: 'senso', 4219: 'senta', 4220: 'sentamos', 4221: 'sentar', 4222: 'sente', 4223: 'sentei', 4224: 'sentemse', 4225: 'sentese', 4226: 'senti', 4227: 'sentila', 4228: 'sentilo', 4229: 'sentimos', 4230: 'sentindo', 4231: 'sentiram', 4232: 'sentiu', 4233: 'ser', 4234: 'seria', 4235: 'serious\\testou', 4236: 'serious\\teu', 4237: 'serious\\tfale', 4238: 'serious\\tfica', 4239: 'serious\\tfique', 4240: 'serious\\tfiquem', 4241: 'serious\\tisto', 4242: 'seriously\\tmesmo', 4243: 'seriously\\tsÃ©rio', 4244: 'seriously\\tÃ©', 4245: 'serve', 4246: 'servir', 4247: 'set', 4248: 'sete', 4249: 'settle', 4250: 'seu', 4251: 'sexist\\tele', 4252: 'sexista', 4253: 'sexy', 4254: 'sexy\\ttom', 4255: 'shall', 4256: 'shame\\tque', 4257: 'shame\\tÃ©', 4258: 'share\\ta', 4259: 'share\\tcompartilharemos', 4260: 'share\\tnÃ³s', 4261: 'shaved\\tele', 4262: 'shaved\\tse', 4263: 'shaved\\ttom', 4264: 'she', 4265: 'she\\tcadÃª', 4266: 'she\\tonde', 4267: 'she\\tquem', 4268: 'shes', 4269: 'ship\\tabandonar', 4270: 'ship\\tÃ©', 4271: 'shock\\tque', 4272: 'shocked\\teu', 4273: 'shoes\\teu', 4274: 'shoes\\tvendo', 4275: 'shoot\\teu', 4276: 'shoot\\tirei', 4277: 'shoot\\tnÃ£o', 4278: 'shoot\\tnÃ³s', 4279: 'shoot\\tvou', 4280: 'short\\ta', 4281: 'short\\to', 4282: 'shot', 4283: 'shot\\tbelo', 4284: 'should', 4285: 'shout\\tnÃ£o', 4286: 'shouted\\teu', 4287: 'shoved', 4288: 'show', 4289: 'shrugged\\ttom', 4290: 'shut', 4291: 'shy\\teu', 4292: 'shy\\tnÃ£o', 4293: 'shy\\tsomos', 4294: 'shy\\tsou', 4295: 'shy\\ttom', 4296: 'shy\\ttu', 4297: 'shy\\tvocÃª', 4298: 'shy\\tvocÃªs', 4299: 'si', 4300: 'sick\\tele', 4301: 'sick\\testou', 4302: 'sick\\teu', 4303: 'sick\\tnÃ³s', 4304: 'sick\\to', 4305: 'sick\\tsintome', 4306: 'sick\\tsou', 4307: 'sick\\ttom', 4308: 'sick\\tvocÃª', 4309: 'sick\\tvocÃªs', 4310: 'siga', 4311: 'sigam', 4312: 'sighed\\ttom', 4313: 'sign', 4314: 'sign\\tvocÃª', 4315: 'silly\\teu', 4316: 'silly\\tme', 4317: 'sim', 4318: 'similar\\tÃ©', 4319: 'simpatizo', 4320: 'simpÃ¡tica', 4321: 'simpÃ¡tico', 4322: 'sincere\\teu', 4323: 'sincero', 4324: 'sing', 4325: 'sing\\tcantarei', 4326: 'sing\\tcantaremos', 4327: 'sing\\tcante', 4328: 'sing\\tcantem', 4329: 'sing\\tdeixeme', 4330: 'sing\\teu', 4331: 'sing\\tnÃ£o', 4332: 'sing\\to', 4333: 'sing\\tos', 4334: 'sing\\tpor', 4335: 'sing\\tposso', 4336: 'sing\\tpÃ¡ssaros', 4337: 'sing\\ttom', 4338: 'sing\\tvocÃªs', 4339: 'singing\\tcontinue', 4340: 'singing\\tpare', 4341: 'single\\testou', 4342: 'single\\tsou', 4343: 'sinto', 4344: 'sit', 4345: 'sit\\tpoderÃ\\xadamos', 4346: 'sit\\tpor', 4347: 'skate\\tvocÃª', 4348: 'ski\\tesquio', 4349: 'ski\\teu', 4350: 'skiing\\ttom', 4351: 'skinny\\tele', 4352: 'sleep', 4353: 'sleep\\tdeixeme', 4354: 'sleep\\teu', 4355: 'sleep\\tnÃ£o', 4356: 'sleep\\tos', 4357: 'sleep\\ttente', 4358: 'sleep\\tvÃ¡', 4359: 'sleeping\\testou', 4360: 'sleepy\\testou', 4361: 'sleepy\\teu', 4362: 'sleepy\\tnÃ³s', 4363: 'slept', 4364: 'slept\\tninguÃ©m', 4365: 'slipped\\tela', 4366: 'slipped\\teu', 4367: 'slipped\\ttom', 4368: 'slow', 4369: 'slow\\ttom', 4370: 'slower\\tfale', 4371: 'slowly\\tande', 4372: 'slowly\\tcoma', 4373: 'slowly\\tdirija', 4374: 'slowly\\ttrabalhe', 4375: 'small\\teu', 4376: 'small\\ttom', 4377: 'small\\ttu', 4378: 'small\\tvocÃª', 4379: 'smart\\tele', 4380: 'smart\\teu', 4381: 'smart\\tnÃ³s', 4382: 'smart\\ttom', 4383: 'smart\\tvocÃª', 4384: 'smell', 4385: 'smile\\tnÃ£o', 4386: 'smile\\tpor', 4387: 'smile\\tsorria', 4388: 'smile\\tsorriam', 4389: 'smiled\\tdei', 4390: 'smiled\\tela', 4391: 'smiled\\tele', 4392: 'smiled\\teles', 4393: 'smiled\\teu', 4394: 'smiled\\ttom', 4395: 'smiling\\tcontinue', 4396: 'smoke\\teu', 4397: 'smoke\\tnÃ£o', 4398: 'smoke\\tposso', 4399: 'smoke\\tvocÃª', 4400: 'smoke\\tvocÃªs', 4401: 'smokes\\ttom', 4402: 'smoking\\tpara', 4403: 'smoking\\tpare', 4404: 'smoking\\tparem', 4405: 'sneaky\\tsou', 4406: 'sneezed\\teu', 4407: 'sneezed\\ttom', 4408: 'snore\\teu', 4409: 'snore\\tvocÃª', 4410: 'snores\\ttom', 4411: 'snow\\teu', 4412: 'snow\\tpode', 4413: 'snowed\\tnevou', 4414: 'snowing\\testÃ¡', 4415: 'so', 4416: 'so\\tacho', 4417: 'so\\tacredito', 4418: 'so\\tassim', 4419: 'so\\tespero', 4420: 'so\\teu', 4421: 'so\\tnÃ£o', 4422: 'so\\tnÃ³s', 4423: 'so\\trealmente', 4424: 'so\\tsuponho', 4425: 'soaked\\teu', 4426: 'sober\\testou', 4427: 'sobrevivemos', 4428: 'sobreviveram', 4429: 'sobreviveu', 4430: 'sobrevivi', 4431: 'socapa', 4432: 'sociable\\teu', 4433: 'sociÃ¡vel', 4434: 'sofreu', 4435: 'sois', 4436: 'solitÃ¡rio', 4437: 'solta', 4438: 'solteiro', 4439: 'solto', 4440: 'some', 4441: 'some\\texperimenta', 4442: 'some\\texperimente', 4443: 'some\\texperimentem', 4444: 'some\\tpega', 4445: 'some\\tpegue', 4446: 'some\\tpeguem', 4447: 'some\\ttemos', 4448: 'some\\tvenha', 4449: 'someone', 4450: 'somos', 4451: 'son\\teu', 4452: 'son\\ttenho', 4453: 'sonhando', 4454: 'sonho', 4455: 'sonhos', 4456: 'sono', 4457: 'soon\\tatÃ©', 4458: 'soon\\tvenha', 4459: 'sopa', 4460: 'sorrateiro', 4461: 'sorri', 4462: 'sorria', 4463: 'sorrias', 4464: 'sorrindo', 4465: 'sorriram', 4466: 'sorriso', 4467: 'sorriu', 4468: 'sorry\\tdesculpa', 4469: 'sorry\\tdesculpe', 4470: 'sorry\\teu', 4471: 'sorry\\tlamento', 4472: 'sorry\\tme', 4473: 'sorry\\tnos', 4474: 'sorry\\tsinto', 4475: 'sorte', 4476: 'sortuda', 4477: 'sortudo', 4478: 'sou', 4479: 'soup\\teu', 4480: 'soup\\tquero', 4481: 'sozinha', 4482: 'sozinho', 4483: 'sozinhos', 4484: 'space\\teu', 4485: 'space\\tpreciso', 4486: 'spanish\\tele', 4487: 'speak', 4488: 'speak\\tdeixeme', 4489: 'speak\\tnÃ£o', 4490: 'speak\\ttom', 4491: 'speaking\\teu', 4492: 'special\\teu', 4493: 'specific\\tseja', 4494: 'split\\tvamos', 4495: 'spoke\\tele', 4496: 'spoke\\tquem', 4497: 'spoke\\ttom', 4498: 'spring\\tÃ©', 4499: 'spy\\to', 4500: 'square\\teu', 4501: 'stand', 4502: 'stand\\tnÃ³s', 4503: 'standing\\teu', 4504: 'star\\testou', 4505: 'star\\tvejo', 4506: 'stare\\tnÃ£o', 4507: 'staring\\tpare', 4508: 'start', 4509: 'start\\tcomecemos', 4510: 'start\\tcomeÃ§arei', 4511: 'start\\tiremos', 4512: 'start\\tnÃ³s', 4513: 'start\\tpodemos', 4514: 'start\\tquem', 4515: 'start\\tvamos', 4516: 'start\\tvocÃª', 4517: 'started\\tcomece', 4518: 'started\\tcomecem', 4519: 'started\\tcomeÃ§a', 4520: 'starve\\tnÃ³s', 4521: 'starved\\testou', 4522: 'starving\\testou', 4523: 'starving\\teu', 4524: 'stay', 4525: 'stay\\ta', 4526: 'stay\\tas', 4527: 'stay\\teu', 4528: 'stay\\tficarei', 4529: 'stay\\tfique', 4530: 'stay\\to', 4531: 'stay\\tos', 4532: 'stay\\tpode', 4533: 'stay\\tpodeis', 4534: 'stay\\tpodem', 4535: 'stay\\tpodes', 4536: 'stay\\tpor', 4537: 'stay\\ttom', 4538: 'stay\\ttu', 4539: 'stay\\tvocÃª', 4540: 'stay\\tvocÃªs', 4541: 'stay\\tvÃ³s', 4542: 'stayed\\teu', 4543: 'stayed\\to', 4544: 'stayed\\tquem', 4545: 'stayed\\ttom', 4546: 'staying\\teu', 4547: 'steal\\tisso', 4548: 'steal\\tÃ©', 4549: 'step', 4550: 'still', 4551: 'still\\tacalmese', 4552: 'still\\tfique', 4553: 'still\\tnÃ£o', 4554: 'still\\ttudo', 4555: 'stink\\tvocÃª', 4556: 'stinks\\tisso', 4557: 'stinks\\tisto', 4558: 'stole', 4559: 'stoned\\ttom', 4560: 'stood', 4561: 'stood\\to', 4562: 'stood\\tquem', 4563: 'stop', 4564: 'stop\\teu', 4565: 'stop\\tnÃ£o', 4566: 'stop\\tpare', 4567: 'stop\\tparem', 4568: 'stop\\tpor', 4569: 'stopped\\teles', 4570: 'stopped\\teu', 4571: 'stopped\\to', 4572: 'stopped\\tquem', 4573: 'stopped\\ttom', 4574: 'strange\\tque', 4575: 'strange\\tÃ©', 4576: 'strong\\tele', 4577: 'strong\\teu', 4578: 'strong\\tnÃ³s', 4579: 'strong\\tsou', 4580: 'strong\\ttom', 4581: 'stuck\\temperrei', 4582: 'stuck\\testou', 4583: 'stuck\\testÃ¡', 4584: 'stuck\\teu', 4585: 'studied\\teu', 4586: 'study', 4587: 'study\\teu', 4588: 'studying\\tele', 4589: 'studying\\testou', 4590: 'studying\\teu', 4591: 'stuffed\\testou', 4592: 'stunned\\teu', 4593: 'stupid\\tela', 4594: 'stupid\\tele', 4595: 'stupid\\teu', 4596: 'stupid\\tisso', 4597: 'stutters\\ttom', 4598: 'sua', 4599: 'suaram', 4600: 'suava', 4601: 'subornei', 4602: 'succeeded\\tnÃ³s', 4603: 'sue', 4604: 'sue\\tnÃ³s', 4605: 'sued', 4606: 'sugar\\teu', 4607: 'sugar\\tpreciso', 4608: 'suits', 4609: 'sujo', 4610: 'suppose', 4611: 'surdo', 4612: 'sure\\tagora', 4613: 'sure\\teu', 4614: 'sure\\ttem', 4615: 'sure\\ttenho', 4616: 'sure\\tvocÃª', 4617: 'sure\\tvocÃªs', 4618: 'surf\\teu', 4619: 'surfar', 4620: 'surrender\\teu', 4621: 'surrender\\tme', 4622: 'surrender\\tnÃ³s', 4623: 'survived\\teu', 4624: 'survived\\tnÃ³s', 4625: 'survived\\tquem', 4626: 'survived\\tsobrevivemos', 4627: 'survived\\ttom', 4628: 'survived\\tvocÃª', 4629: 'survived\\tvocÃªs', 4630: 'sushi', 4631: 'sushi\\teu', 4632: 'suspirou', 4633: 'suÃ\\xadÃ§o', 4634: 'swam\\telas', 4635: 'swam\\teles', 4636: 'swam\\to', 4637: 'swam\\tquem', 4638: 'swam\\ttom', 4639: 'sweated\\teles', 4640: 'sweated\\ttom', 4641: 'sweet', 4642: 'sweet\\testÃ¡', 4643: 'sweet\\tisso', 4644: 'sweet\\tÃ©', 4645: 'swim\\ta', 4646: 'swim\\tcachorros', 4647: 'swim\\tdeixeme', 4648: 'swim\\tela', 4649: 'swim\\tele', 4650: 'swim\\teu', 4651: 'swim\\tnademos', 4652: 'swim\\tnÃ³s', 4653: 'swim\\to', 4654: 'swim\\tpode', 4655: 'swim\\tpodem', 4656: 'swim\\tpodes', 4657: 'swim\\tsabe', 4658: 'swim\\tsabemos', 4659: 'swim\\tsabes', 4660: 'swim\\ttom', 4661: 'swim\\tvamos', 4662: 'swim\\tvocÃª', 4663: 'swim\\tvocÃªs', 4664: 'swims\\ttom', 4665: 'swiss\\tele', 4666: 'swore\\ttom', 4667: 'sympathize\\teu', 4668: 'sympathize\\tme', 4669: 'sÃ¡bio', 4670: 'sÃ£o', 4671: 'sÃ©ria', 4672: 'sÃ©rias', 4673: 'sÃ©rio', 4674: 'sÃ©rios', 4675: 'sÃ³', 4676: 'sÃ³brio', 4677: 't', 4678: 'tailandÃªs', 4679: 'take', 4680: 'talk', 4681: 'talk\\tnÃ£o', 4682: 'talk\\tpodemos', 4683: 'talk\\ttom', 4684: 'talked\\tconversamos', 4685: 'talked\\teu', 4686: 'talked\\tnÃ³s', 4687: 'talked\\tquem', 4688: 'talked\\ttom', 4689: 'talking\\tcontinue', 4690: 'talking\\tpara', 4691: 'talking\\tpare', 4692: 'talks', 4693: 'talks\\to', 4694: 'tall\\telas', 4695: 'tall\\tele', 4696: 'tall\\teles', 4697: 'tall\\teu', 4698: 'tall\\tmary', 4699: 'tall\\tnÃ£o', 4700: 'tall\\to', 4701: 'tall\\tsou', 4702: 'tall\\tvocÃª', 4703: 'taller\\teu', 4704: 'talvez', 4705: 'tambÃ©m', 4706: 'tanque', 4707: 'tarde', 4708: 'taste', 4709: 'taxes\\teu', 4710: 'taxes\\tpago', 4711: 'te', 4712: 'tea', 4713: 'tea\\tbeba', 4714: 'tea\\tele', 4715: 'tea\\teu', 4716: 'teach', 4717: 'teaches\\ttom', 4718: 'telefonei', 4719: 'telefonou', 4720: 'televisÃ£o', 4721: 'tell', 4722: 'tell\\teu', 4723: 'tem', 4724: 'temo', 4725: 'tempo', 4726: 'ten\\tconte', 4727: 'tenho', 4728: 'tenta', 4729: 'tentamos', 4730: 'tentando', 4731: 'tentar', 4732: 'tentaram', 4733: 'tentarei', 4734: 'tentaremos', 4735: 'tentava', 4736: 'tente', 4737: 'tentei', 4738: 'tento', 4739: 'tentou', 4740: 'terminamos', 4741: 'terminei', 4742: 'terminou', 4743: 'terrific\\texcelente', 4744: 'terrific\\tfantÃ¡stico', 4745: 'terrific\\tgenial', 4746: 'terrific\\tmagnÃ\\xadfico', 4747: 'terrÃ\\xadvel', 4748: 'testa', 4749: 'teu', 4750: 'thai\\teu', 4751: 'thai\\tsou', 4752: 'thank', 4753: 'thanks', 4754: 'thanks\\tmuito', 4755: 'thanks\\tobrigada', 4756: 'thanks\\tobrigado', 4757: 'that', 4758: 'that\\tapenas', 4759: 'that\\tcurto', 4760: 'that\\tele', 4761: 'that\\tescuto', 4762: 'that\\testou', 4763: 'that\\teu', 4764: 'that\\tfarei', 4765: 'that\\tfique', 4766: 'that\\tgosto', 4767: 'that\\tignore', 4768: 'that\\tignoreo', 4769: 'that\\tnÃ£o', 4770: 'that\\tnÃ³s', 4771: 'that\\to', 4772: 'that\\todeio', 4773: 'that\\tolhe', 4774: 'that\\tolhem', 4775: 'that\\tpare', 4776: 'that\\tparem', 4777: 'that\\tquem', 4778: 'that\\tquero', 4779: 'that\\tsabemos', 4780: 'that\\tsabÃ\\xadamos', 4781: 'that\\ttom', 4782: 'that\\tverifica', 4783: 'that\\tverifique', 4784: 'that\\tverifiquem', 4785: 'thatll', 4786: 'thats', 4787: 'the', 4788: 'them', 4789: 'them\\ta', 4790: 'them\\tdetenhamnos', 4791: 'them\\tdetenhanos', 4792: 'them\\tele', 4793: 'them\\teu', 4794: 'them\\texamineos', 4795: 'them\\tfique', 4796: 'them\\tignoreas', 4797: 'them\\tignoremnas', 4798: 'them\\tignoremnos', 4799: 'them\\tignoreos', 4800: 'them\\tnÃ³s', 4801: 'them\\tpergunte', 4802: 'them\\ttom', 4803: 'then', 4804: 'there', 4805: 'there\\tcoloqueo', 4806: 'there\\testÃ¡', 4807: 'there\\teu', 4808: 'there\\tfica', 4809: 'there\\tfique', 4810: 'there\\tnÃ³s', 4811: 'there\\to', 4812: 'there\\tolha', 4813: 'there\\tpare', 4814: 'there\\tquem', 4815: 'there\\tsenta', 4816: 'there\\tsentese', 4817: 'there\\ttom', 4818: 'there\\ttu', 4819: 'there\\tvocÃª', 4820: 'there\\tvocÃªs', 4821: 'theres', 4822: 'these\\teu', 4823: 'these\\tgosto', 4824: 'these\\tleve', 4825: 'these\\tquero', 4826: 'they', 4827: 'they\\tquem', 4828: 'theyll', 4829: 'theyre', 4830: 'theyve', 4831: 'thief\\tele', 4832: 'thin\\teu', 4833: 'thin\\tsou', 4834: 'thin\\ttom', 4835: 'think', 4836: 'thinking\\testou', 4837: 'thirsty\\testou', 4838: 'thirsty\\ttenho', 4839: 'thirty\\teu', 4840: 'thirty\\to', 4841: 'thirty\\ttenho', 4842: 'this', 4843: 'this\\tassine', 4844: 'this\\tassinem', 4845: 'this\\tcarregue', 4846: 'this\\tcheire', 4847: 'this\\tconserta', 4848: 'this\\tconsigo', 4849: 'this\\teu', 4850: 'this\\texperimenta', 4851: 'this\\texperimente', 4852: 'this\\texperimentem', 4853: 'this\\tfique', 4854: 'this\\tleia', 4855: 'this\\tleiam', 4856: 'this\\tnÃ£o', 4857: 'this\\tnÃ³s', 4858: 'this\\tobservem', 4859: 'this\\tolhe', 4860: 'this\\tpegue', 4861: 'this\\tposso', 4862: 'this\\tprove', 4863: 'this\\tprovem', 4864: 'this\\tquero', 4865: 'this\\tsabÃ\\xadamos', 4866: 'this\\tsegure', 4867: 'this\\tsinta', 4868: 'this\\ttermine', 4869: 'this\\ttom', 4870: 'this\\tuse', 4871: 'this\\tveja', 4872: 'this\\tverifica', 4873: 'this\\tverifique', 4874: 'this\\tverifiquem', 4875: 'this\\tvocÃª', 4876: 'this\\tvou', 4877: 'thisll', 4878: 'threw', 4879: 'thrilling\\tque', 4880: 'through\\tacabei', 4881: 'through\\tterminei', 4882: 'ti', 4883: 'ticklish\\teu', 4884: 'tidy\\testou', 4885: 'tidy\\teu', 4886: 'tight\\taguenta', 4887: 'tight\\tdurma', 4888: 'tight\\tperaÃ\\xad', 4889: 'tight\\tsegurem', 4890: 'tight\\tsegurese', 4891: 'time', 4892: 'time\\tdÃªlhe', 4893: 'time\\testÃ¡', 4894: 'time\\teu', 4895: 'time\\tnÃ³s', 4896: 'time\\tprecisamos', 4897: 'time\\tquero', 4898: 'time\\ttemos', 4899: 'time\\ttom', 4900: 'timid\\teu', 4901: 'timid\\tnÃ³s', 4902: 'timid\\ttom', 4903: 'timid\\ttu', 4904: 'timid\\tvocÃª', 4905: 'timing\\tna', 4906: 'tinha', 4907: 'tinta', 4908: 'tipo', 4909: 'tired\\tele', 4910: 'tired\\testamos', 4911: 'tired\\testou', 4912: 'tired\\teu', 4913: 'tired\\tme', 4914: 'tired\\tnÃ³s', 4915: 'tired\\to', 4916: 'tired\\tsentiase', 4917: 'tired\\tsintome', 4918: 'tired\\ttom', 4919: 'tired\\ttu', 4920: 'tired\\tvocÃª', 4921: 'tired\\tvocÃªs', 4922: 'tiro', 4923: 'tive', 4924: 'to', 4925: 'toc', 4926: 'tocante', 4927: 'tocar', 4928: 'toco', 4929: 'today\\teu', 4930: 'today\\tnÃ³s', 4931: 'todo', 4932: 'todos', 4933: 'tola', 4934: 'told', 4935: 'tolerant\\tsede', 4936: 'tolerant\\tseja', 4937: 'tolerant\\tsejam', 4938: 'tolerant\\tsÃª', 4939: 'tolerante', 4940: 'tolerantes', 4941: 'tolo', 4942: 'tom', 4943: 'tom\\tabrace', 4944: 'tom\\tabracem', 4945: 'tom\\tacerta', 4946: 'tom\\tacorda', 4947: 'tom\\tacorde', 4948: 'tom\\tacredito', 4949: 'tom\\tadorÃ¡vamos', 4950: 'tom\\tajuda', 4951: 'tom\\tajude', 4952: 'tom\\tajudem', 4953: 'tom\\tajudeme', 4954: 'tom\\tajudenos', 4955: 'tom\\talerte', 4956: 'tom\\tali', 4957: 'tom\\tamÃ¡vamos', 4958: 'tom\\tapressese', 4959: 'tom\\taquele', 4960: 'tom\\tavise', 4961: 'tom\\tavisem', 4962: 'tom\\tbeije', 4963: 'tom\\tcomo', 4964: 'tom\\tconfie', 4965: 'tom\\tconheÃ§o', 4966: 'tom\\tconta', 4967: 'tom\\tcontacte', 4968: 'tom\\tcontactem', 4969: 'tom\\tconte', 4970: 'tom\\tcontem', 4971: 'tom\\tcontratei', 4972: 'tom\\tconversa', 4973: 'tom\\tdeixe', 4974: 'tom\\tdescreva', 4975: 'tom\\tdescrevam', 4976: 'tom\\tele', 4977: 'tom\\tencontre', 4978: 'tom\\tencontrem', 4979: 'tom\\tera', 4980: 'tom\\tescreva', 4981: 'tom\\tesqueÃ§a', 4982: 'tom\\tesqueÃ§am', 4983: 'tom\\tesse', 4984: 'tom\\teste', 4985: 'tom\\testou', 4986: 'tom\\teu', 4987: 'tom\\tfale', 4988: 'tom\\tfaÃ§a', 4989: 'tom\\tfaÃ§am', 4990: 'tom\\tfaÃ§o', 4991: 'tom\\tfoi', 4992: 'tom\\tgostamos', 4993: 'tom\\tgostei', 4994: 'tom\\tgosto', 4995: 'tom\\tignora', 4996: 'tom\\tignore', 4997: 'tom\\timpeÃ§a', 4998: 'tom\\timpeÃ§am', 4999: 'tom\\tinterrompa', 5000: 'tom\\tinterrompam', 5001: 'tom\\tirei', 5002: 'tom\\tleva', 5003: 'tom\\tleve', 5004: 'tom\\tligue', 5005: 'tom\\tnÃ£o', 5006: 'tom\\tnÃ³s', 5007: 'tom\\todeio', 5008: 'tom\\toi', 5009: 'tom\\tolha', 5010: 'tom\\tolÃ¡', 5011: 'tom\\tpare', 5012: 'tom\\tparem', 5013: 'tom\\tpegue', 5014: 'tom\\tpeguem', 5015: 'tom\\tperdoem', 5016: 'tom\\tpergunta', 5017: 'tom\\tperguntarei', 5018: 'tom\\tpergunte', 5019: 'tom\\tperguntem', 5020: 'tom\\tpeÃ§a', 5021: 'tom\\tprecisamos', 5022: 'tom\\tprossiga', 5023: 'tom\\tquem', 5024: 'tom\\tqueremos', 5025: 'tom\\tquero', 5026: 'tom\\tresponda', 5027: 'tom\\tsalve', 5028: 'tom\\tsentimos', 5029: 'tom\\tsiga', 5030: 'tom\\tsigam', 5031: 'tom\\tsinto', 5032: 'tom\\tsolte', 5033: 'tom\\tsou', 5034: 'tom\\tum', 5035: 'tom\\tvai', 5036: 'tom\\tvi', 5037: 'tom\\tvocÃª', 5038: 'tom\\tvote', 5039: 'tom\\tvou', 5040: 'tom\\tvÃ¡', 5041: 'tom\\tÃ©', 5042: 'tomei', 5043: 'tomll', 5044: 'tomo', 5045: 'tomorrow\\tvenha', 5046: 'tomorrow\\tvenham', 5047: 'toms', 5048: 'toms\\tera', 5049: 'toms\\tisso', 5050: 'toms\\tÃ©', 5051: 'tomÃ¡s', 5052: 'tonto', 5053: 'tontos', 5054: 'too', 5055: 'too\\teu', 5056: 'too\\thomens', 5057: 'too\\tposso', 5058: 'took', 5059: 'tossi', 5060: 'tossiu', 5061: 'touching\\tque', 5062: 'town\\tdeixe', 5063: 'town\\tdeixem', 5064: 'toys\\tcomprem', 5065: 'toys\\tele', 5066: 'trabalh', 5067: 'trabalha', 5068: 'trabalhando', 5069: 'trabalhar', 5070: 'trabalho', 5071: 'trabalhou', 5072: 'tragic\\tque', 5073: 'trancado', 5074: 'tranquilos', 5075: 'trap\\tcoloquei', 5076: 'trap\\teu', 5077: 'trap\\tÃ©', 5078: 'trapacearam', 5079: 'trapaceei', 5080: 'trapaceia', 5081: 'trapaceou', 5082: 'trapped\\testou', 5083: 'trapped\\teu', 5084: 'trash\\tisso', 5085: 'trash\\tÃ©', 5086: 'trick\\tÃ©', 5087: 'tried\\tela', 5088: 'tried\\teles', 5089: 'tried\\teu', 5090: 'tried\\tnÃ³s', 5091: 'tried\\to', 5092: 'tried\\ttentei', 5093: 'tried\\tvocÃª', 5094: 'tried\\tvocÃªs', 5095: 'tries', 5096: 'tries\\tele', 5097: 'tries\\to', 5098: 'trinta', 5099: 'tripped\\teu', 5100: 'tripped\\ttom', 5101: 'tripped\\ttropecei', 5102: 'trips\\tamo', 5103: 'triste', 5104: 'tristes', 5105: 'tropecei', 5106: 'true\\tisso', 5107: 'true\\tisto', 5108: 'true\\tÃ©', 5109: 'truque', 5110: 'trust', 5111: 'try', 5112: 'try\\ta', 5113: 'try\\tagora', 5114: 'try\\tdeixeme', 5115: 'try\\tdevemos', 5116: 'try\\teu', 5117: 'try\\tnÃ³s', 5118: 'try\\ttentamos', 5119: 'try\\ttentarei', 5120: 'try\\ttentaremos', 5121: 'try\\ttento', 5122: 'try\\ttom', 5123: 'try\\tvou', 5124: 'trying\\tcontinue', 5125: 'trying\\testou', 5126: 'trying\\teu', 5127: 'trying\\tpare', 5128: 'trying\\tparem', 5129: 'trÃ¡gico', 5130: 'trÃ¡s', 5131: 'tu', 5132: 'tua', 5133: 'tuas', 5134: 'tudo', 5135: 'turn', 5136: 'turned\\tele', 5137: 'tusso', 5138: 'tv', 5139: 'tv\\tisso', 5140: 'tv\\tisto', 5141: 'tv\\to', 5142: 'tv\\ttom', 5143: 'tv\\tÃ©', 5144: 'tvs', 5145: 'twice\\teu', 5146: 'twins\\teu', 5147: 'twins\\tnÃ³s', 5148: 'twins\\tsomos', 5149: 'two\\teu', 5150: 'type\\tele', 5151: 'tÃ¡xi', 5152: 'tÃ£o', 5153: 'tÃªm', 5154: 'tÃ\\xadmida', 5155: 'tÃ\\xadmidas', 5156: 'tÃ\\xadmido', 5157: 'tÃ\\xadmidos', 5158: 'ufo\\teu', 5159: 'ugly\\teles', 5160: 'ugly\\testou', 5161: 'ugly\\teu', 5162: 'ugly\\tnÃ£o', 5163: 'ugly\\tsou', 5164: 'ugly\\ttom', 5165: 'ugly\\ttu', 5166: 'ugly\\tvocÃª', 5167: 'ugly\\tvocÃªs', 5168: 'um', 5169: 'uma', 5170: 'unbelievable\\tinacreditÃ¡vel', 5171: 'unbelievable\\tincrÃ\\xadvel', 5172: 'understand\\tcompreendo', 5173: 'understand\\tentendo', 5174: 'understand\\teu', 5175: 'understood\\tentendi', 5176: 'understood\\teu', 5177: 'unfair\\tisso', 5178: 'unsure\\to', 5179: 'untidy\\tsou', 5180: 'unusual\\tÃ©', 5181: 'up', 5182: 'up\\tabra', 5183: 'up\\tacorda', 5184: 'up\\tacorde', 5185: 'up\\tacordem', 5186: 'up\\tacordeos', 5187: 'up\\tapanheo', 5188: 'up\\tapressate', 5189: 'up\\tapressese', 5190: 'up\\tcala', 5191: 'up\\tcalate', 5192: 'up\\tcale', 5193: 'up\\tcalem', 5194: 'up\\tcalemse', 5195: 'up\\tcalese', 5196: 'up\\tcontinua', 5197: 'up\\tcontinue', 5198: 'up\\tde', 5199: 'up\\tdepressa', 5200: 'up\\te', 5201: 'up\\tela', 5202: 'up\\telas', 5203: 'up\\tele', 5204: 'up\\teles', 5205: 'up\\tencha', 5206: 'up\\tescute', 5207: 'up\\tescutem', 5208: 'up\\tespere', 5209: 'up\\tesperem', 5210: 'up\\testou', 5211: 'up\\teu', 5212: 'up\\tfala', 5213: 'up\\tfale', 5214: 'up\\tfalem', 5215: 'up\\tlavate', 5216: 'up\\tlave', 5217: 'up\\tlavemse', 5218: 'up\\tlavese', 5219: 'up\\tlevantate', 5220: 'up\\tlevantemse', 5221: 'up\\tlevanteo', 5222: 'up\\tlevantese', 5223: 'up\\tlimpe', 5224: 'up\\tnÃ£o', 5225: 'up\\tnÃ³s', 5226: 'up\\to', 5227: 'up\\tpegue', 5228: 'up\\tpegueo', 5229: 'up\\trelaxa', 5230: 'up\\trelaxe', 5231: 'up\\tse', 5232: 'up\\ttom', 5233: 'up\\ttudo', 5234: 'up\\tvou', 5235: 'up\\tÃ¢nimo', 5236: 'upset\\testamos', 5237: 'upset\\testou', 5238: 'upset\\teu', 5239: 'upset\\tnÃ³s', 5240: 'upset\\to', 5241: 'upset\\ttom', 5242: 'upset\\ttu', 5243: 'upset\\tvocÃª', 5244: 'upstairs\\tsobe', 5245: 'upstairs\\tsuba', 5246: 'upstairs\\tsubam', 5247: 'urgent\\tÃ©', 5248: 'urgente', 5249: 'us', 5250: 'us\\tajude', 5251: 'us\\tajudemnos', 5252: 'us\\tajudenos', 5253: 'us\\tcante', 5254: 'us\\tconverse', 5255: 'us\\tdeixenos', 5256: 'us\\tela', 5257: 'us\\telas', 5258: 'us\\tele', 5259: 'us\\teles', 5260: 'us\\tfale', 5261: 'us\\tfique', 5262: 'us\\tfiquem', 5263: 'us\\tjuntese', 5264: 'us\\tligue', 5265: 'us\\tnos', 5266: 'us\\tnÃ£o', 5267: 'us\\tolhe', 5268: 'us\\tperdoenos', 5269: 'us\\tsigamnos', 5270: 'us\\tsiganos', 5271: 'us\\ttom', 5272: 'us\\tvenha', 5273: 'us\\tvenham', 5274: 'usa', 5275: 'usar', 5276: 'use', 5277: 'used', 5278: 'useless\\teu', 5279: 'uso', 5280: 'usou', 5281: 'vai', 5282: 'valente', 5283: 'vamos', 5284: 'vangloriouse', 5285: 'vanished\\tquem', 5286: 'vanished\\ttom', 5287: 'vazio', 5288: 've', 5289: 'vegan\\teu', 5290: 'vegana', 5291: 'vegano', 5292: 'veio', 5293: 'velha', 5294: 'velhas', 5295: 'velho', 5296: 'velhos', 5297: 'vem', 5298: 'vemos', 5299: 'vencemos', 5300: 'vencendo', 5301: 'vencer', 5302: 'venceram', 5303: 'venceremos', 5304: 'venceu', 5305: 'venci', 5306: 'vende', 5307: 'vendo', 5308: 'veneno', 5309: 'venha', 5310: 'venham', 5311: 'venhas', 5312: 'venho', 5313: 'ventando', 5314: 'venÃ§a', 5315: 'ver', 5316: 'verda', 5317: 'verdade', 5318: 'verei', 5319: 'veremos', 5320: 'vergonha', 5321: 'verificaremos', 5322: 'vermelha', 5323: 'vermelho', 5324: 'very', 5325: 'verÃ¡', 5326: 'verÃ\\xaddico', 5327: 'vet\\ttom', 5328: 'veterano', 5329: 'vez', 5330: 'vezes', 5331: 'vi', 5332: 'viagem', 5333: 'viagens', 5334: 'vice\\tÃ©', 5335: 'viciado', 5336: 'vida', 5337: 'vim', 5338: 'vindo', 5339: 'vinho', 5340: 'vir', 5341: 'viram', 5342: 'virei', 5343: 'virÃ¡', 5344: 'vista', 5345: 'viu', 5346: 'viva', 5347: 'viver', 5348: 'vivo', 5349: 'vo', 5350: 'voa', 5351: 'voador', 5352: 'voam', 5353: 'voar', 5354: 'vocÃ', 5355: 'vocÃª', 5356: 'vocÃªs', 5357: 'volta', 5358: 'voltar', 5359: 'voltaram', 5360: 'voltarei', 5361: 'volte', 5362: 'volto', 5363: 'vomited\\to', 5364: 'vomitei', 5365: 'vomitou', 5366: 'vos', 5367: 'votaram', 5368: 'vote', 5369: 'vote\\teu', 5370: 'vote\\ttom', 5371: 'vote\\tvocÃª', 5372: 'voted\\telas', 5373: 'voted\\teles', 5374: 'voted\\ttom', 5375: 'voted\\tvotaram', 5376: 'votei', 5377: 'votou', 5378: 'vou', 5379: 'vÃ¡', 5380: 'vÃ£o', 5381: 'vÃª', 5382: 'vÃ\\xadcio', 5383: 'vÃ³s', 5384: 'wait', 5385: 'wait\\tesperarei', 5386: 'wait\\tesperaremos', 5387: 'wait\\tespere', 5388: 'wait\\tesperem', 5389: 'wait\\teu', 5390: 'wait\\tirei', 5391: 'wait\\tnÃ£o', 5392: 'wait\\tnÃ³s', 5393: 'wait\\tpor', 5394: 'wait\\ttom', 5395: 'wait\\tvou', 5396: 'waited\\teles', 5397: 'waited\\tesperamos', 5398: 'waited\\tesperaram', 5399: 'waited\\teu', 5400: 'waited\\tnÃ³s', 5401: 'waited\\ttom', 5402: 'waiting\\testou', 5403: 'waiting\\teu', 5404: 'wake', 5405: 'walk', 5406: 'walk\\tandarei', 5407: 'walk\\tele', 5408: 'walk\\teu', 5409: 'walk\\to', 5410: 'walk\\tpoderÃ\\xadamos', 5411: 'walk\\ttom', 5412: 'walked\\tcaminharam', 5413: 'walked\\telas', 5414: 'walked\\teles', 5415: 'walked\\ttom', 5416: 'walking\\tcontinue', 5417: 'walking\\tcontinuem', 5418: 'walks', 5419: 'walks\\tela', 5420: 'walks\\ttom', 5421: 'want', 5422: 'wants', 5423: 'war', 5424: 'warm\\tmantenha', 5425: 'warn', 5426: 'wary\\ttom', 5427: 'was', 5428: 'wash', 5429: 'washed', 5430: 'wasnt', 5431: 'waste\\tque', 5432: 'waste\\tÃ©', 5433: 'watch', 5434: 'water\\teu', 5435: 'waved\\teu', 5436: 'waved\\ttom', 5437: 'way\\tde', 5438: 'way\\timpossÃ\\xadvel', 5439: 'way\\tsem', 5440: 'way\\tvem', 5441: 'we', 5442: 'we\\tonde', 5443: 'we\\tquem', 5444: 'weak\\teles', 5445: 'weak\\teu', 5446: 'weak\\tnÃ³s', 5447: 'weak\\ttom', 5448: 'weak\\ttu', 5449: 'weak\\tvocÃª', 5450: 'wealthy\\teu', 5451: 'weird\\tfoi', 5452: 'weird\\tisso', 5453: 'weird\\to', 5454: 'welcome', 5455: 'welcome\\tbemvinda', 5456: 'welcome\\tbemvindo', 5457: 'welcome\\tseja', 5458: 'well', 5459: 'well\\tele', 5460: 'well\\testou', 5461: 'well\\testÃ¡', 5462: 'well\\teu', 5463: 'well\\to', 5464: 'well\\ttudo', 5465: 'well\\tvocÃª', 5466: 'well\\tvocÃªs', 5467: 'went', 5468: 'went\\teu', 5469: 'wept\\tjesus', 5470: 'were', 5471: 'wet\\tele', 5472: 'wet\\testava', 5473: 'wet\\testou', 5474: 'wet\\testÃ¡', 5475: 'wet\\teu', 5476: 'wet\\ttom', 5477: 'weve', 5478: 'what', 5479: 'what\\te', 5480: 'whats', 5481: 'where', 5482: 'whiff\\tdÃª', 5483: 'while\\tespere', 5484: 'while\\tesperem', 5485: 'while\\tfique', 5486: 'whining\\tpare', 5487: 'whistled\\teu', 5488: 'whistled\\ttom', 5489: 'white\\tera', 5490: 'white\\tÃ©', 5491: 'who', 5492: 'who\\tquem', 5493: 'wholl', 5494: 'whos', 5495: 'whose', 5496: 'why', 5497: 'why\\tdiga', 5498: 'why\\teu', 5499: 'why\\tposso', 5500: 'why\\tsabemos', 5501: 'will', 5502: 'win\\teu', 5503: 'win\\tganhamos', 5504: 'win\\tnÃ³s', 5505: 'win\\to', 5506: 'win\\tquem', 5507: 'win\\ttom', 5508: 'win\\tvencemos', 5509: 'win\\tvenceremos', 5510: 'win\\tvou', 5511: 'windy\\testÃ¡', 5512: 'wine\\tele', 5513: 'wine\\teu', 5514: 'wine\\ttemos', 5515: 'wine\\ttraga', 5516: 'wine\\ttragam', 5517: 'winked\\teu', 5518: 'winked\\to', 5519: 'winning\\testou', 5520: 'wins\\ttodo', 5521: 'wise\\teu', 5522: 'wise\\ttom', 5523: 'wise\\ttu', 5524: 'wise\\tvocÃª', 5525: 'wish', 5526: 'wish\\tfaÃ§a', 5527: 'with', 5528: 'without', 5529: 'witty\\ttom', 5530: 'woke', 5531: 'wolf\\tisso', 5532: 'wolf\\tÃ©', 5533: 'woman\\teu', 5534: 'woman\\tque', 5535: 'woman\\tsou', 5536: 'women\\teu', 5537: 'won\\tadivinha', 5538: 'won\\telas', 5539: 'won\\teles', 5540: 'won\\teu', 5541: 'won\\tganhei', 5542: 'won\\tnÃ³s', 5543: 'won\\tquem', 5544: 'won\\ttom', 5545: 'won\\tvencemos', 5546: 'won\\tvocÃª', 5547: 'won\\tvocÃªs', 5548: 'wonderful\\tmagnÃ\\xadfico', 5549: 'wonderful\\tmaravilhoso', 5550: 'wonderful\\tque', 5551: 'wont', 5552: 'wood', 5553: 'work', 5554: 'work\\testou', 5555: 'work\\teu', 5556: 'work\\tfuncionou', 5557: 'work\\tisso', 5558: 'work\\tpoderia', 5559: 'work\\tvai', 5560: 'work\\tvou', 5561: 'work\\tÃ©', 5562: 'worked\\taquilo', 5563: 'worked\\tfuncionou', 5564: 'worked\\ttom', 5565: 'working\\tcontinue', 5566: 'working\\tcontinuem', 5567: 'working\\testou', 5568: 'working\\testÃ¡', 5569: 'working\\teu', 5570: 'works\\tfunciona', 5571: 'works\\tisto', 5572: 'works\\ttom', 5573: 'worn', 5574: 'worried\\testou', 5575: 'worry\\tnÃ£o', 5576: 'worse\\tfoi', 5577: 'wow\\tnossa', 5578: 'wow\\tuau', 5579: 'wow\\twow', 5580: 'write', 5581: 'writer\\teu', 5582: 'writing\\tcontinue', 5583: 'writing\\tcontinuem', 5584: 'wrong\\tela', 5585: 'wrong\\testou', 5586: 'wrong\\testÃ¡', 5587: 'wrong\\teu', 5588: 'wrong\\to', 5589: 'wrong\\tquem', 5590: 'wrong\\tvocÃª', 5591: 'wrote', 5592: 'xadrez', 5593: 'yawned\\teu', 5594: 'yawned\\ttom', 5595: 'years', 5596: 'yelled\\teles', 5597: 'yelled\\tquem', 5598: 'yelled\\ttom', 5599: 'yelling\\tpara', 5600: 'yelling\\tpare', 5601: 'yen\\tcusta', 5602: 'yes\\tdisse', 5603: 'yes\\teu', 5604: 'yes\\to', 5605: 'yes\\ttom', 5606: 'you', 5607: 'you\\ta', 5608: 'you\\tacredito', 5609: 'you\\tamavaa', 5610: 'you\\tamavao', 5611: 'you\\tamavate', 5612: 'you\\tameia', 5613: 'you\\tameio', 5614: 'you\\tameite', 5615: 'you\\tamo', 5616: 'you\\tamoa', 5617: 'you\\tamoo', 5618: 'you\\tamote', 5619: 'you\\tamovos', 5620: 'you\\tatÃ©', 5621: 'you\\tcomo', 5622: 'you\\tconfio', 5623: 'you\\tdeus', 5624: 'you\\te', 5625: 'you\\tele', 5626: 'you\\teles', 5627: 'you\\tencontrei', 5628: 'you\\testamos', 5629: 'you\\testou', 5630: 'you\\teu', 5631: 'you\\tgosto', 5632: 'you\\tirei', 5633: 'you\\tisso', 5634: 'you\\tnos', 5635: 'you\\tnÃ£o', 5636: 'you\\tnÃ³s', 5637: 'you\\tobrigada', 5638: 'you\\tobrigado', 5639: 'you\\touvimos', 5640: 'you\\tposso', 5641: 'you\\tpreciso', 5642: 'you\\tque', 5643: 'you\\tquem', 5644: 'you\\tquerote', 5645: 'you\\tsaudades', 5646: 'you\\tsaÃºde', 5647: 'you\\tsenti', 5648: 'you\\tsentimos', 5649: 'you\\tsinto', 5650: 'you\\tte', 5651: 'you\\ttenho', 5652: 'you\\ttom', 5653: 'you\\tvocÃª', 5654: 'you\\tvocÃªs', 5655: 'you\\tvou', 5656: 'you\\tÃ©', 5657: 'youll', 5658: 'young\\tela', 5659: 'young\\tele', 5660: 'young\\teu', 5661: 'young\\tnÃ³s', 5662: 'young\\to', 5663: 'young\\tsomos', 5664: 'young\\tsou', 5665: 'young\\tvocÃª', 5666: 'young\\tvocÃªs', 5667: 'your', 5668: 'youre', 5669: 'yours\\teu', 5670: 'yours\\tisso', 5671: 'yours\\tsou', 5672: 'yours\\tÃ©', 5673: 'yourself\\tseja', 5674: 'yourself\\tservete', 5675: 'yourself\\tsirvamse', 5676: 'yourself\\tsirvase', 5677: 'youve', 5678: 'zombou', 5679: 'Ã', 5680: 'Ã¡gua', 5681: 'Ã¡rabes', 5682: 'Ã©', 5683: 'Ã©s', 5684: 'Ã³pera', 5685: 'Ã³vni', 5686: 'Ã´nibus', 5687: 'Ãºltimo'}\n",
      "{'': 0, 'a': 1, 'abaixado': 2, 'abaixados': 3, 'abandon': 4, 'abenÃ§oe': 5, 'aberto': 6, 'aboard\\ttodos': 7, 'about': 8, 'above\\tvejam': 9, 'abrace': 10, 'abraÃ§ar': 11, 'abraÃ§aram': 12, 'abraÃ§o': 13, 'abraÃ§ou': 14, 'abraÃ§Ã¡lo': 15, 'abrir': 16, 'absurd\\tque': 17, 'absurdo': 18, 'acabou': 19, 'acalma': 20, 'acalme': 21, 'acalmese': 22, 'acenei': 23, 'acenou': 24, 'achamos': 25, 'achar': 26, 'achei': 27, 'acho': 28, 'acidente': 29, 'acima': 30, 'acontece': 31, 'acontecer': 32, 'acorda': 33, 'acordada': 34, 'acordado': 35, 'acordados': 36, 'acorde': 37, 'acordei': 38, 'acordo': 39, 'acordou': 40, 'acredito': 41, 'act': 42, 'act\\tnÃ³s': 43, 'action\\ttome': 44, 'action\\ttomem': 45, 'actor\\tsou': 46, 'addicted\\testou': 47, 'adentro': 48, 'adeus': 49, 'adiantada': 50, 'adiantado': 51, 'adiantados': 52, 'adiante': 53, 'admire': 54, 'admiro': 55, 'admit': 56, 'admito': 57, 'adoeceu': 58, 'adopted\\teu': 59, 'adora': 60, 'adorable\\tele': 61, 'adormeceu': 62, 'adormecido': 63, 'adoro': 64, 'adorÃ¡vel': 65, 'adotado': 66, 'adult\\teu': 67, 'adulta': 68, 'adulto': 69, 'adultos': 70, 'adults\\tnÃ³s': 71, 'afastado': 72, 'afaste': 73, 'afastou': 74, 'afastouse': 75, 'afogando': 76, 'afogou': 77, 'afraid\\testamos': 78, 'afraid\\testou': 79, 'afraid\\teu': 80, 'afraid\\ttom': 81, 'after': 82, 'again\\tcomece': 83, 'again\\tcomecem': 84, 'again\\tcomeÃ§a': 85, 'again\\tconfira': 86, 'again\\teu': 87, 'again\\tfaz': 88, 'again\\tfaÃ§a': 89, 'again\\tmais': 90, 'again\\tobrigado': 91, 'again\\tolhe': 92, 'again\\toutra': 93, 'again\\tpensa': 94, 'again\\tpense': 95, 'again\\tpergunte': 96, 'again\\trepita': 97, 'again\\ttenta': 98, 'again\\ttente': 99, 'again\\ttentem': 100, 'again\\tverifique': 101, 'age\\taja': 102, 'age\\tele': 103, 'age\\ttenho': 104, 'agir': 105, 'agora': 106, 'agradÃ¡vel': 107, 'agree\\tconcordam': 108, 'agree\\tconcordamos': 109, 'agree\\telas': 110, 'agree\\teles': 111, 'agree\\testou': 112, 'agree\\testÃ¡': 113, 'agree\\teu': 114, 'agree\\tnÃ£o': 115, 'agree\\tnÃ³s': 116, 'agree\\tvocÃª': 117, 'agree\\tvocÃªs': 118, 'agreed\\tconcordamos': 119, 'agreed\\teu': 120, 'agreed\\tnÃ³s': 121, 'agreed\\ttom': 122, 'agrees\\ttom': 123, 'agressivo': 124, 'agricultor': 125, 'ahead': 126, 'ahead\\tcontinua': 127, 'ahead\\tcontinue': 128, 'ahead\\tcontinuem': 129, 'ahead\\tvÃ¡': 130, 'aim': 131, 'ainda': 132, 'aint': 133, 'air\\teu': 134, 'ajoelhou': 135, 'ajuda': 136, 'ajudar': 137, 'ajudaremos': 138, 'ajudarÃ¡': 139, 'ajude': 140, 'ajudei': 141, 'ajudou': 142, 'alce': 143, 'alert\\tse': 144, 'alerta': 145, 'algum': 146, 'alguma': 147, 'algumas': 148, 'alguns': 149, 'alguÃ©m': 150, 'ali': 151, 'alimento': 152, 'alistou': 153, 'alive\\tande': 154, 'alive\\tela': 155, 'alive\\tele': 156, 'alive\\testou': 157, 'alive\\to': 158, 'alive\\tse': 159, 'alive\\tsintome': 160, 'alive\\ttom': 161, 'all': 162, 'all\\tisso': 163, 'all\\tleve': 164, 'all\\tÃ©': 165, 'almost': 166, 'alone\\tdeixame': 167, 'alone\\tele': 168, 'alone\\testamos': 169, 'alone\\testou': 170, 'alone\\teu': 171, 'alone\\tme': 172, 'alone\\tnÃ³s': 173, 'alone\\to': 174, 'alone\\ttom': 175, 'alone\\tvenha': 176, 'alone\\tvocÃª': 177, 'along\\tcante': 178, 'along\\tvem': 179, 'along\\tvenha': 180, 'already': 181, 'also': 182, 'alta': 183, 'altas': 184, 'alto': 185, 'altos': 186, 'always': 187, 'am': 188, 'am\\taqui': 189, 'am\\tsou': 190, 'ama': 191, 'amada': 192, 'amado': 193, 'amam': 194, 'amamos': 195, 'amando': 196, 'amanhÃ£': 197, 'amava': 198, 'amazed': 199, 'amazing\\tisso': 200, 'ambas': 201, 'ambos': 202, 'amei': 203, 'american\\teu': 204, 'americano': 205, 'amigÃ¡veis': 206, 'amigÃ¡vel': 207, 'amo': 208, 'amor': 209, 'amuse': 210, 'amÃ¡vamos': 211, 'amÃ¡vel': 212, 'an': 213, 'and': 214, 'anda': 215, 'andando': 216, 'andar': 217, 'ando': 218, 'angry\\tele': 219, 'angry\\testou': 220, 'angry\\teu': 221, 'angry\\tnÃ³s': 222, 'angry\\ttom': 223, 'annoying\\tque': 224, 'anos': 225, 'another\\ttoma': 226, 'another\\ttome': 227, 'another\\ttomem': 228, 'answer': 229, 'answered\\ttom': 230, 'any': 231, 'anybody': 232, 'anybody\\tpergunta': 233, 'anybody\\tpergunte': 234, 'anybody\\tperguntem': 235, 'anyone': 236, 'anything': 237, 'anytime\\tvenha': 238, 'anyway\\tfaÃ§a': 239, 'ao': 240, 'aparelho': 241, 'aplaudiram': 242, 'aplaudiu': 243, 'apologize\\tdesculpamos': 244, 'apologize\\tnÃ³s': 245, 'apologize\\tperdÃ£o': 246, 'apologize\\tpeÃ§o': 247, 'apontar': 248, 'aposentado': 249, 'aposentou': 250, 'aposta': 251, 'apples\\teu': 252, 'approve\\taprovam': 253, 'approve\\telas': 254, 'approve\\teles': 255, 'approved\\to': 256, 'approves\\ttom': 257, 'aprender': 258, 'aprenderei': 259, 'apressa': 260, 'apressar': 261, 'apresse': 262, 'apressemse': 263, 'apressese': 264, 'aprova': 265, 'aprovam': 266, 'aprovou': 267, 'aquecido': 268, 'aquela': 269, 'aquelas': 270, 'aquele': 271, 'aqueles': 272, 'aqui': 273, 'aquilo': 274, 'ar': 275, 'arabs\\tsomos': 276, 'are': 277, 'are\\taqui': 278, 'are\\teisnos': 279, 'are\\testamos': 280, 'areia': 281, 'arfou': 282, 'argue\\tnÃ£o': 283, 'arguing\\tpare': 284, 'arm': 285, 'armadilh': 286, 'armadilha': 287, 'armados': 288, 'armed\\tnÃ³s': 289, 'around\\tdÃ¡': 290, 'around\\tdÃª': 291, 'around\\tolhe': 292, 'around\\tpergunte': 293, 'around\\tperguntem': 294, 'arrebenta': 295, 'arrependo': 296, 'arrisque': 297, 'arrived\\ttom': 298, 'arrogant\\tque': 299, 'arrogante': 300, 'arrotei': 301, 'arrotou': 302, 'arroz': 303, 'arrumado': 304, 'art\\tisto': 305, 'arte': 306, 'as': 307, 'aside\\tafastese': 308, 'aside\\tse': 309, 'ask': 310, 'ask\\teu': 311, 'ask\\tnÃ£o': 312, 'asked\\tfaÃ§a': 313, 'asked\\tninguÃ©m': 314, 'asleep\\tele': 315, 'asleep\\testÃ¡': 316, 'asleep\\teu': 317, 'asleep\\to': 318, 'asleep\\ttom': 319, 'assim': 320, 'assinou': 321, 'assobiei': 322, 'assobio': 323, 'assobiou': 324, 'assusta': 325, 'assustado': 326, 'assustador': 327, 'assustam': 328, 'assustei': 329, 'at': 330, 'atacar': 331, 'atacaremos': 332, 'ate': 333, 'ate\\tacabei': 334, 'ate\\teu': 335, 'ate\\to': 336, 'ate\\tquem': 337, 'ate\\ttom': 338, 'atenÃ§Ã£o': 339, 'athletic\\teu': 340, 'athletic\\tsou': 341, 'atingido': 342, 'atirar': 343, 'atire': 344, 'atirem': 345, 'atirou': 346, 'atitude': 347, 'atlÃ©tico': 348, 'ator': 349, 'atrasado': 350, 'atrasados': 351, 'atrasar': 352, 'atrase': 353, 'attack\\tatacar': 354, 'attack\\tatacaremos': 355, 'attack\\tataque': 356, 'attack\\tataquem': 357, 'attack\\tiremos': 358, 'attack\\tnÃ³s': 359, 'attack\\tvamos': 360, 'atÃ©': 361, 'aulas': 362, 'autista': 363, 'autistic\\teu': 364, 'autistic\\tsou': 365, 'automÃ³vel': 366, 'aventure': 367, 'avoids': 368, 'awake\\testamos': 369, 'awake\\testou': 370, 'awake\\teu': 371, 'awake\\tfique': 372, 'awake\\tnÃ³s': 373, 'awake\\to': 374, 'awake\\tvocÃª': 375, 'away': 376, 'away\\tafastese': 377, 'away\\tcai': 378, 'away\\tele': 379, 'away\\tfique': 380, 'away\\tfora': 381, 'away\\tmantenhase': 382, 'away\\tnÃ£o': 383, 'away\\tquem': 384, 'away\\tse': 385, 'away\\ttire': 386, 'away\\ttom': 387, 'away\\tvÃ¡': 388, 'awesome\\tÃ©': 389, 'awful\\tque': 390, 'awkward\\tÃ©': 391, 'azul': 392, 'aÃ§Ãºcar': 393, 'aÃ\\xad': 394, 'babando': 395, 'back': 396, 'back\\tafastese': 397, 'back\\tbemvindo': 398, 'back\\telas': 399, 'back\\teles': 400, 'back\\teu': 401, 'back\\tfique': 402, 'back\\tolha': 403, 'back\\tolhe': 404, 'back\\tolhem': 405, 'back\\tpara': 406, 'back\\ttraga': 407, 'back\\ttragam': 408, 'back\\tvolte': 409, 'back\\tvoltei': 410, 'back\\tvoltem': 411, 'backup\\ttraga': 412, 'backup\\ttragam': 413, 'bad\\teles': 414, 'bad\\teu': 415, 'bad\\tisso': 416, 'bad\\tnos': 417, 'bad\\ttom': 418, 'bad\\tvocÃª': 419, 'bad\\tÃ©': 420, 'bag\\tpreciso': 421, 'bagunÃ§a': 422, 'baixo': 423, 'baker\\teu': 424, 'bald\\testou': 425, 'bald\\teu': 426, 'bald\\tnÃ£o': 427, 'bald\\to': 428, 'bald\\tsou': 429, 'banquete': 430, 'barba': 431, 'barbaric\\tque': 432, 'barbeou': 433, 'barbeouse': 434, 'bark\\tcachorros': 435, 'bark\\tcÃ£es': 436, 'bastante': 437, 'bastarÃ¡': 438, 'bateram': 439, 'bateu': 440, 'be': 441, 'be\\timpossÃ\\xadvel': 442, 'be\\tmentira': 443, 'be\\tnÃ£o': 444, 'beans\\teu': 445, 'beat': 446, 'beautiful\\tque': 447, 'beba': 448, 'bebe': 449, 'bebem': 450, 'bebi': 451, 'bebia': 452, 'bebo': 453, 'bed\\testou': 454, 'beef': 455, 'beef\\teu': 456, 'beer': 457, 'beer\\tcerveja': 458, 'beer\\teu': 459, 'beer\\tpega': 460, 'beer\\ttoma': 461, 'beer\\tvÃ¡': 462, 'begin\\tcomecemos': 463, 'begin\\tpodemos': 464, 'begin\\tposso': 465, 'begin\\tvamos': 466, 'begun\\tcomeÃ§ou': 467, 'beija': 468, 'beijaram': 469, 'beije': 470, 'beijei': 471, 'beijo': 472, 'beijou': 473, 'believe': 474, 'belong': 475, 'bem': 476, 'bemvinda': 477, 'bemvindo': 478, 'best\\tdÃª': 479, 'best\\teu': 480, 'best\\tfaÃ§a': 481, 'best\\tfiz': 482, 'bet\\tera': 483, 'bet\\teu': 484, 'better\\testamos': 485, 'better\\testou': 486, 'better\\teu': 487, 'better\\tnÃ³s': 488, 'beware': 489, 'big\\telas': 490, 'big\\teles': 491, 'big\\tisso': 492, 'big\\tisto': 493, 'big\\tvocÃª': 494, 'big\\tÃ©': 495, 'bipolar': 496, 'bipolar\\tsou': 497, 'bird\\talimente': 498, 'birds': 499, 'biruta': 500, 'biscoito': 501, 'bit': 502, 'bit\\tespera': 503, 'bit\\tespere': 504, 'bite\\teu': 505, 'bite\\tnÃ£o': 506, 'bizarre\\tÃ©': 507, 'bizarro': 508, 'blank\\testÃ¡': 509, 'bled\\to': 510, 'bleeding\\testou': 511, 'blefando': 512, 'bless': 513, 'blind\\tele': 514, 'blind\\teu': 515, 'blind\\tficou': 516, 'blind\\to': 517, 'blind\\tsou': 518, 'blind\\ttom': 519, 'blind\\tvocÃª': 520, 'blinked\\teu': 521, 'blinked\\ttom': 522, 'blog': 523, 'blog\\tele': 524, 'blood': 525, 'bloom\\tflores': 526, 'blue\\testou': 527, 'blue\\teu': 528, 'blue\\tgosto': 529, 'blue\\tÃ©': 530, 'bluffing\\testou': 531, 'blushed\\tela': 532, 'blushed\\ttom': 533, 'boa': 534, 'boato': 535, 'bobo': 536, 'boca': 537, 'bocejei': 538, 'bocejou': 539, 'body\\tqueime': 540, 'body\\tqueimem': 541, 'boi': 542, 'boia': 543, 'boil': 544, 'bolo': 545, 'bolsa': 546, 'bom': 547, 'boneca': 548, 'bonita': 549, 'bonitas': 550, 'bonitinha': 551, 'bonitinho': 552, 'bonito': 553, 'bonitos': 554, 'bonzinho': 555, 'book\\tele': 556, 'book\\tesse': 557, 'book\\teste': 558, 'book\\tisso': 559, 'book\\tvejo': 560, 'books\\teu': 561, 'bordo': 562, 'bored\\testou': 563, 'bored\\teu': 564, 'bored\\ttom': 565, 'bored\\tvocÃª': 566, 'boss\\tele': 567, 'bossy\\teu': 568, 'both\\teu': 569, 'both\\tpor': 570, 'bought': 571, 'box\\tabre': 572, 'box\\tfeche': 573, 'boy\\teu': 574, 'boy\\tseja': 575, 'boy\\tsou': 576, 'boys': 577, 'boys\\teles': 578, 'boys\\tsomos': 579, 'boys\\tsÃ£o': 580, 'braces\\tele': 581, 'branco': 582, 'brava': 583, 'brave\\teu': 584, 'brave\\tseja': 585, 'brave\\tvocÃª': 586, 'bravo': 587, 'bravos': 588, 'braÃ§o': 589, 'bread\\tcomo': 590, 'bread\\teu': 591, 'breve': 592, 'bribed': 593, 'brief\\tseja': 594, 'brincadeira': 595, 'brincando': 596, 'brincar': 597, 'bring': 598, 'brinque': 599, 'brinquedos': 600, 'broke': 601, 'broke\\tele': 602, 'broke\\testou': 603, 'broke\\teu': 604, 'broken\\testÃ¡': 605, 'broken\\ttÃ¡': 606, 'built': 607, 'buraco': 608, 'burn': 609, 'burn\\tisso': 610, 'burn\\tvai': 611, 'burned\\tqueimou': 612, 'burns\\ta': 613, 'burns\\to': 614, 'burped\\teu': 615, 'burped\\ttom': 616, 'burra': 617, 'burro': 618, 'burros': 619, 'bus\\teu': 620, 'bus\\tpegue': 621, 'bus\\tpeguem': 622, 'bus\\tvim': 623, 'bus\\tvÃ¡': 624, 'bus\\tÃ©': 625, 'buscar': 626, 'buscÃ¡lo': 627, 'bushed\\testou': 628, 'bushed\\teu': 629, 'busy': 630, 'busy\\tela': 631, 'busy\\telas': 632, 'busy\\tele': 633, 'busy\\teles': 634, 'busy\\testamos': 635, 'busy\\testou': 636, 'busy\\testÃ¡s': 637, 'busy\\testÃ¡vamos': 638, 'busy\\teu': 639, 'busy\\tnÃ³s': 640, 'busy\\ttom': 641, 'buy': 642, 'buying\\tnÃ³s': 643, 'buying\\tquem': 644, 'by': 645, 'by\\taguarde': 646, 'by\\taguardem': 647, 'by\\tum': 648, 'bÃ¡rbaro': 649, 'bÃªbada': 650, 'bÃªbado': 651, 'cab\\teu': 652, 'cachorro': 653, 'cachorros': 654, 'caia': 655, 'cain': 656, 'caiu': 657, 'caixa': 658, 'cake\\teu': 659, 'cake\\texperimente': 660, 'call': 661, 'call\\teles': 662, 'call\\tligarei': 663, 'call\\ttom': 664, 'call\\tvocÃª': 665, 'called': 666, 'called\\telas': 667, 'called\\teles': 668, 'called\\tligaram': 669, 'called\\ttom': 670, 'called\\tvocÃª': 671, 'calm': 672, 'calm\\tacalmese': 673, 'calm\\testamos': 674, 'calm\\tfique': 675, 'calm\\tnÃ³s': 676, 'calm\\ttom': 677, 'calma': 678, 'calmo': 679, 'calmos': 680, 'calor': 681, 'cama': 682, 'came': 683, 'came\\talguÃ©m': 684, 'came\\tela': 685, 'came\\tele': 686, 'came\\tninguÃ©m': 687, 'came\\tnÃ£o': 688, 'came\\tquem': 689, 'came\\ttom': 690, 'caminha': 691, 'caminhando': 692, 'caminhar': 693, 'caminharam': 694, 'caminho': 695, 'caminhou': 696, 'can': 697, 'canadense': 698, 'canadian\\teu': 699, 'cancel\\tcancelarei': 700, 'canceled\\tquem': 701, 'canceled\\ttom': 702, 'cancelou': 703, 'cancer\\tÃ©': 704, 'candy\\teu': 705, 'caneta': 706, 'cansada': 707, 'cansadas': 708, 'cansado': 709, 'cansados': 710, 'cant': 711, 'cantam': 712, 'cantando': 713, 'cantar': 714, 'cantaram': 715, 'cante': 716, 'cantei': 717, 'cantem': 718, 'cantes': 719, 'canto': 720, 'car': 721, 'car\\tele': 722, 'car\\teu': 723, 'car\\ttenho': 724, 'car\\tÃ©': 725, 'card\\tescolha': 726, 'care\\tcuidese': 727, 'care\\teu': 728, 'care\\tnÃ£o': 729, 'care\\tnÃ³s': 730, 'care\\tse': 731, 'care\\tte': 732, 'care\\ttoma': 733, 'care\\ttome': 734, 'careca': 735, 'cared\\tninguÃ©m': 736, 'careful\\tcuidado': 737, 'careful\\ttenha': 738, 'careful\\ttoma': 739, 'careful\\ttome': 740, 'carente': 741, 'cares\\tninguÃ©m': 742, 'cares\\tquem': 743, 'cares\\ttom': 744, 'careta': 745, 'carne': 746, 'carro': 747, 'carros': 748, 'carry': 749, 'cars\\tele': 750, 'cars\\teu': 751, 'carta': 752, 'casa': 753, 'casaco': 754, 'casada': 755, 'casado': 756, 'cash\\teu': 757, 'cash\\tquero': 758, 'castigo': 759, 'cat\\teste': 760, 'cat\\teu': 761, 'cat\\tÃ©': 762, 'catch': 763, 'cats\\tadoro': 764, 'cats\\tamo': 765, 'cats\\teu': 766, 'caught\\tfui': 767, 'cavando': 768, 'cavei': 769, 'caviar': 770, 'caviar\\teu': 771, 'cavou': 772, 'caÃ\\xad': 773, 'cd': 774, 'cd\\tÃ©': 775, 'cds': 776, 'cds\\tcompramos': 777, 'cedeu': 778, 'cedo': 779, 'cega': 780, 'cego': 781, 'certa': 782, 'certain\\testou': 783, 'certain\\ttenho': 784, 'certeza': 785, 'certo': 786, 'certos': 787, 'cerveja': 788, 'chamar': 789, 'chamarÃ£o': 790, 'chamou': 791, 'chance': 792, 'change\\tmudarei': 793, 'changed\\teu': 794, 'changed\\ttom': 795, 'chapado': 796, 'chapÃ©u': 797, 'chat\\tvamos': 798, 'chateada': 799, 'chateado': 800, 'chateados': 801, 'chatice': 802, 'chato': 803, 'chave': 804, 'cheat\\teles': 805, 'cheat\\tnÃ£o': 806, 'cheated': 807, 'cheated\\teu': 808, 'cheated\\to': 809, 'cheated\\tquem': 810, 'cheated\\tvocÃª': 811, 'cheated\\tvocÃªs': 812, 'cheats\\ttom': 813, 'check': 814, 'check\\tnÃ³s': 815, 'check\\tverificarei': 816, 'cheer': 817, 'cheered\\telas': 818, 'cheered\\teles': 819, 'cheered\\tquem': 820, 'cheered\\ttom': 821, 'cheers\\tsaÃºde': 822, 'cheese\\tsorria': 823, 'chegando': 824, 'chegou': 825, 'cheguei': 826, 'cheia': 827, 'cheio': 828, 'cheirada': 829, 'chess\\teu': 830, 'chess\\tgosto': 831, 'china': 832, 'china\\teu': 833, 'chinese\\teu': 834, 'chinese\\tsou': 835, 'chinÃªs': 836, 'chocado': 837, 'choose': 838, 'choque': 839, 'choram': 840, 'choramingar': 841, 'choramos': 842, 'chorando': 843, 'chorar': 844, 'chorava': 845, 'chore': 846, 'chorei': 847, 'chorem': 848, 'choro': 849, 'chorou': 850, 'chova': 851, 'chovendo': 852, 'chover': 853, 'chubby\\tsou': 854, 'chuckled\\ttom': 855, 'chutou': 856, 'chuvoso': 857, 'chÃ¡': 858, 'ci': 859, 'cidade': 860, 'cima': 861, 'cinco': 862, 'circle\\tdesenhe': 863, 'ciumenta': 864, 'ciumento': 865, 'clapped\\ttom': 866, 'claro': 867, 'clean': 868, 'clean\\testÃ¡': 869, 'clear\\testÃ¡': 870, 'clear\\tÃ©': 871, 'clever\\tque': 872, 'click': 873, 'client\\teu': 874, 'cliente': 875, 'clock\\tconserte': 876, 'close': 877, 'close\\testÃ¡': 878, 'close\\tfique': 879, 'closely\\tolhe': 880, 'closer\\tchega': 881, 'closer\\tchegue': 882, 'closer\\tcheguem': 883, 'closer\\tolhe': 884, 'closer\\tolhem': 885, 'cloudy\\testÃ¡': 886, 'clumsy\\tsou': 887, 'cnn': 888, 'cnn\\tligue': 889, 'coat\\tpegue': 890, 'cochilo': 891, 'cochilou': 892, 'coisa': 893, 'cola': 894, 'cold\\tera': 895, 'cold\\testamos': 896, 'cold\\testava': 897, 'cold\\testou': 898, 'cold\\testÃ¡vamos': 899, 'cold\\teu': 900, 'cold\\tfiquei': 901, 'cold\\tnÃ³s': 902, 'cold\\tsinto': 903, 'cold\\ttenho': 904, 'cole': 905, 'colem': 906, 'colidiram': 907, 'coloquei': 908, 'com': 909, 'comando': 910, 'come': 911, 'come\\taqui': 912, 'come\\tela': 913, 'come\\tele': 914, 'come\\testou': 915, 'come\\teu': 916, 'come\\tnÃ£o': 917, 'come\\to': 918, 'come\\tpode': 919, 'come\\tposso': 920, 'come\\ttom': 921, 'come\\tvem': 922, 'come\\tvenha': 923, 'come\\tvenham': 924, 'come\\tvocÃª': 925, 'come\\tvocÃªs': 926, 'comendo': 927, 'comentÃ¡rios': 928, 'comer': 929, 'comes\\tlÃ¡': 930, 'comeu': 931, 'comeÃ§a': 932, 'comeÃ§ar': 933, 'comeÃ§aremos': 934, 'comi': 935, 'comida': 936, 'comigo': 937, 'coming\\tele': 938, 'coming\\testou': 939, 'coming\\teu': 940, 'coming\\tnÃ³s': 941, 'coming\\tquem': 942, 'coming\\ttom': 943, 'command\\tassuma': 944, 'comment\\tsem': 945, 'como': 946, 'compartilhar': 947, 'compartilharemos': 948, 'comprando': 949, 'compraria': 950, 'compreende': 951, 'compreendo': 952, 'comprei': 953, 'concorda': 954, 'concordam': 955, 'concordamos': 956, 'concordo': 957, 'concordou': 958, 'confessed\\teu': 959, 'confessei': 960, 'confiante': 961, 'confiantes': 962, 'confident\\tsede': 963, 'confident\\tseja': 964, 'confident\\tsejam': 965, 'confident\\tsÃª': 966, 'confie': 967, 'confio': 968, 'confiÃ¡vel': 969, 'confused\\testou': 970, 'confuso': 971, 'congelando': 972, 'conhece': 973, 'conhecem': 974, 'conhecemos': 975, 'conhecer': 976, 'conheceu': 977, 'conheci': 978, 'conheÃ§o': 979, 'conheÃ§oo': 980, 'conosco': 981, 'consegue': 982, 'conseguimos': 983, 'conseguindo': 984, 'conseguiu': 985, 'consertei': 986, 'consertÃ¡lo': 987, 'consigo': 988, 'constrangedor': 989, 'construiu': 990, 'construÃ\\xad': 991, 'contact': 992, 'contar': 993, 'content\\tfique': 994, 'contente': 995, 'contigo': 996, 'contou': 997, 'contratado': 998, 'contratei': 999, 'contributed\\teu': 1000, 'contribuÃ\\xad': 1001, 'controle': 1002, 'contrÃ¡rio': 1003, 'conversamos': 1004, 'conversar': 1005, 'convicto': 1006, 'convidado': 1007, 'cook\\tcozinharei': 1008, 'cook\\teu': 1009, 'cook\\tnÃ³s': 1010, 'cook\\to': 1011, 'cook\\tquem': 1012, 'cook\\tsou': 1013, 'cook\\ttom': 1014, 'cookie\\tcome': 1015, 'cooking\\testou': 1016, 'cool': 1017, 'cool\\telas': 1018, 'cool\\teles': 1019, 'cool\\teu': 1020, 'cool\\tfique': 1021, 'cool\\tfoi': 1022, 'cool\\tisso': 1023, 'cool\\tisto': 1024, 'cool\\tmantenha': 1025, 'cool\\tseja': 1026, 'cool\\tsou': 1027, 'cool\\tte': 1028, 'cool\\tÃ©': 1029, 'cops\\tvocÃªs': 1030, 'coragem': 1031, 'corajoso': 1032, 'corda': 1033, 'corou': 1034, 'corpo': 1035, 'corra': 1036, 'corram': 1037, 'corras': 1038, 'corre': 1039, 'correct\\teu': 1040, 'corredor': 1041, 'correm': 1042, 'correndo': 1043, 'correr': 1044, 'correu': 1045, 'corri': 1046, 'corria': 1047, 'cortei': 1048, 'cough': 1049, 'coughed\\tele': 1050, 'coughed\\teu': 1051, 'coughed\\ttom': 1052, 'could': 1053, 'count': 1054, 'courage\\ttenha': 1055, 'courage\\ttenham': 1056, 'course\\tclaro': 1057, 'course\\tpois': 1058, 'course\\tÃ©': 1059, 'cover\\tprotejamse': 1060, 'cozinhando': 1061, 'cozinhar': 1062, 'cozinharemos': 1063, 'cozinharÃ¡': 1064, 'cozinheira': 1065, 'cozinheiro': 1066, 'cozinho': 1067, 'coÃ§am': 1068, 'cranky\\tele': 1069, 'crashed\\telas': 1070, 'crashed\\teles': 1071, 'crashed\\ttom': 1072, 'crazy\\ta': 1073, 'crazy\\testou': 1074, 'crazy\\testÃ¡s': 1075, 'crazy\\tisso': 1076, 'crazy\\to': 1077, 'crazy\\tque': 1078, 'crazy\\tsomos': 1079, 'crazy\\tvocÃª': 1080, 'creative\\teu': 1081, 'creative\\tsede': 1082, 'creative\\tseja': 1083, 'creative\\tsejam': 1084, 'creative\\tsou': 1085, 'creative\\tsÃª': 1086, 'crescem': 1087, 'cresceram': 1088, 'crescerÃ£o': 1089, 'cresceu': 1090, 'crianÃ§as': 1091, 'criativa': 1092, 'criativas': 1093, 'criativo': 1094, 'criativos': 1095, 'cried': 1096, 'cried\\tela': 1097, 'cried\\teu': 1098, 'cried\\tmamÃ£e': 1099, 'cried\\ttodos': 1100, 'cried\\ttom': 1101, 'criei': 1102, 'cringed\\ttom': 1103, 'crio': 1104, 'cruel': 1105, 'cruel\\tele': 1106, 'cruel\\tisso': 1107, 'cry': 1108, 'cry\\teu': 1109, 'cry\\tnÃ£o': 1110, 'cry\\to': 1111, 'cry\\ttom': 1112, 'crying\\tele': 1113, 'crying\\tpare': 1114, 'crying\\tparem': 1115, 'crying\\ttom': 1116, 'cuff': 1117, 'cuida': 1118, 'cuidado': 1119, 'culhÃ£o': 1120, 'culpada': 1121, 'culpado': 1122, 'cultiva': 1123, 'curado': 1124, 'cured\\to': 1125, 'curiosa': 1126, 'curioso': 1127, 'curious\\teu': 1128, 'curious\\tque': 1129, 'curious\\tquero': 1130, 'curse\\tisso': 1131, 'curse\\tÃ©': 1132, 'curta': 1133, 'cut': 1134, 'cute\\ta': 1135, 'cute\\tas': 1136, 'cute\\tele': 1137, 'cute\\to': 1138, 'cute\\tos': 1139, 'cute\\tque': 1140, 'cute\\ttu': 1141, 'cute\\tvocÃª': 1142, 'cute\\tvocÃªs': 1143, 'cute\\tÃ©': 1144, 'cÃ¡': 1145, 'cÃ¢ncer': 1146, 'cÃ£es': 1147, 'cÃ£o': 1148, 'cÃ\\xadrculo': 1149, 'cÃ³cegas': 1150, 'da': 1151, 'damas': 1152, 'dance': 1153, 'dance\\tdancemos': 1154, 'dance\\tdanÃ§aremos': 1155, 'dance\\teu': 1156, 'dance\\tnÃ³s': 1157, 'dance\\tquer': 1158, 'danced\\tdanÃ§aram': 1159, 'danced\\telas': 1160, 'danced\\teles': 1161, 'danced\\teu': 1162, 'danced\\ttom': 1163, 'dancei': 1164, 'dances\\ttom': 1165, 'dancing\\tcontinue': 1166, 'dancing\\tcontinuem': 1167, 'dancing\\teu': 1168, 'danÃ§a': 1169, 'danÃ§ando': 1170, 'danÃ§ar': 1171, 'danÃ§aram': 1172, 'danÃ§aremos': 1173, 'danÃ§ou': 1174, 'daqui': 1175, 'daquilo': 1176, 'dar': 1177, 'dark\\testava': 1178, 'dark\\testÃ¡': 1179, 'dark\\tÃ©': 1180, 'date\\tÃ©': 1181, 'dating\\testamos': 1182, 'day\\tque': 1183, 'daÃ\\xad': 1184, 'de': 1185, 'dead\\tela': 1186, 'dead\\telas': 1187, 'dead\\teles': 1188, 'dead\\teu': 1189, 'dead\\tnÃ£o': 1190, 'deaf\\teu': 1191, 'deaf\\tnÃ£o': 1192, 'deaf\\to': 1193, 'deaf\\tsou': 1194, 'deaf\\ttom': 1195, 'deaf\\tvocÃª': 1196, 'decide\\tdecidirei': 1197, 'decide\\tdecidiremos': 1198, 'decide\\tnos': 1199, 'decide\\tnÃ³s': 1200, 'decided\\tdecidi': 1201, 'decided\\teu': 1202, 'decided\\ttom': 1203, 'decidi': 1204, 'decidir': 1205, 'decidiremos': 1206, 'decidiu': 1207, 'decisive\\tsou': 1208, 'decisivo': 1209, 'deep\\tqual': 1210, 'deep\\tquÃ£o': 1211, 'dei': 1212, 'deixa': 1213, 'deixe': 1214, 'deixei': 1215, 'dela': 1216, 'delas': 1217, 'dele': 1218, 'deles': 1219, 'delicioso': 1220, 'delicious\\tque': 1221, 'delÃ\\xadcia': 1222, 'demais': 1223, 'demiti': 1224, 'demitido': 1225, 'demito': 1226, 'demore': 1227, 'dentro': 1228, 'depois': 1229, 'depressa': 1230, 'desabrigado': 1231, 'desagradÃ¡vel': 1232, 'desajeitado': 1233, 'desapareceu': 1234, 'desastrado': 1235, 'descansando': 1236, 'descansar': 1237, 'desconfiado': 1238, 'describe': 1239, 'desculpamos': 1240, 'desculpe': 1241, 'desejo': 1242, 'desenhou': 1243, 'desista': 1244, 'desistimos': 1245, 'desistiram': 1246, 'desistiu': 1247, 'desisto': 1248, 'desliga': 1249, 'desligou': 1250, 'desmaiaram': 1251, 'desmaiei': 1252, 'desmaiou': 1253, 'desorganizada': 1254, 'desorganizado': 1255, 'despedida': 1256, 'despedido': 1257, 'despedidos': 1258, 'despediu': 1259, 'desperdÃ\\xadcio': 1260, 'despise': 1261, 'desprezo': 1262, 'desses': 1263, 'detesta': 1264, 'detesto': 1265, 'deu': 1266, 'deus': 1267, 'devagar': 1268, 'deve': 1269, 'deveria': 1270, 'deverÃ\\xadamos': 1271, 'deves': 1272, 'devia': 1273, 'dez': 1274, 'dezesseis': 1275, 'dia': 1276, 'diabetic\\teu': 1277, 'diabetic\\tsou': 1278, 'diabÃ©tico': 1279, 'did': 1280, 'didnt': 1281, 'die\\tdeixeme': 1282, 'die\\tdeixemme': 1283, 'die\\tele': 1284, 'die\\teu': 1285, 'die\\tlute': 1286, 'die\\tnÃ£o': 1287, 'die\\to': 1288, 'die\\ttodos': 1289, 'die\\ttom': 1290, 'die\\tvocÃª': 1291, 'died\\tela': 1292, 'died\\tninguÃ©m': 1293, 'died\\to': 1294, 'died\\tquantas': 1295, 'died\\tquantos': 1296, 'died\\tquem': 1297, 'died\\ttom': 1298, 'dies\\ttodo': 1299, 'dieta': 1300, 'dieting\\testou': 1301, 'difÃ\\xadcil': 1302, 'digging\\tcontinue': 1303, 'digging\\tcontinuem': 1304, 'dignidade': 1305, 'digo': 1306, 'dinheiro': 1307, 'direita': 1308, 'dirige': 1309, 'dirigi': 1310, 'dirigindo': 1311, 'dirigir': 1312, 'dirigiu': 1313, 'dirijo': 1314, 'dirty\\testÃ¡': 1315, 'dirty\\tisso': 1316, 'disagree\\tdiscordo': 1317, 'disagree\\teu': 1318, 'disagreed\\teu': 1319, 'discordei': 1320, 'discordo': 1321, 'discr': 1322, 'discreet\\tsede': 1323, 'discreet\\tseja': 1324, 'discreet\\tsejam': 1325, 'discreet\\tsÃª': 1326, 'discreta': 1327, 'discretas': 1328, 'discreto': 1329, 'discretos': 1330, 'discuta': 1331, 'discutam': 1332, 'discutas': 1333, 'discutir': 1334, 'dislÃ©xico': 1335, 'disparou': 1336, 'disperdÃ\\xadcio': 1337, 'disse': 1338, 'disseram': 1339, 'disso': 1340, 'distante': 1341, 'disto': 1342, 'distÃ¢ncia': 1343, 'diverte': 1344, 'diverti': 1345, 'divertida': 1346, 'divertido': 1347, 'divertimos': 1348, 'divertiu': 1349, 'dividir': 1350, 'divorced\\tsou': 1351, 'divorciado': 1352, 'diz': 1353, 'dizer': 1354, 'dizzy\\testamos': 1355, 'dizzy\\testou': 1356, 'dizzy\\teu': 1357, 'dizzy\\tnÃ³s': 1358, 'dj': 1359, 'dj\\tele': 1360, 'do': 1361, 'do\\tcomo': 1362, 'do\\tisso': 1363, 'doce': 1364, 'doctor\\teu': 1365, 'doctor\\tsou': 1366, 'doe': 1367, 'doem': 1368, 'doen': 1369, 'doend': 1370, 'doendo': 1371, 'doente': 1372, 'doentes': 1373, 'does': 1374, 'dog\\tcomo': 1375, 'dog\\tcuidado': 1376, 'dog\\teu': 1377, 'dog\\tvi': 1378, 'dogs': 1379, 'dogs\\teu': 1380, 'dogs\\tgosto': 1381, 'dogs\\todeio': 1382, 'doido': 1383, 'doing': 1384, 'dois': 1385, 'doll\\tÃ©': 1386, 'done': 1387, 'done\\tainda': 1388, 'done\\tbem': 1389, 'done\\tbom': 1390, 'done\\tbravo': 1391, 'done\\testÃ¡': 1392, 'done\\teu': 1393, 'done\\tjÃ¡': 1394, 'done\\tparabÃ©ns': 1395, 'done\\tpronto': 1396, 'done\\tterminamos': 1397, 'done\\tterminou': 1398, 'dont': 1399, 'donut\\teu': 1400, 'door\\tsegure': 1401, 'dor': 1402, 'dormem': 1403, 'dormi': 1404, 'dormindo': 1405, 'dormir': 1406, 'dormiu': 1407, 'dos': 1408, 'dou': 1409, 'doubt': 1410, 'doubts\\teu': 1411, 'down\\tabaixese': 1412, 'down\\tacalmate': 1413, 'down\\tacalmemse': 1414, 'down\\tacalmese': 1415, 'down\\tagora': 1416, 'down\\taquieta': 1417, 'down\\tassentese': 1418, 'down\\tdeitemse': 1419, 'down\\tdesliga': 1420, 'down\\tdesÃ§a': 1421, 'down\\tdevagar': 1422, 'down\\tescreva': 1423, 'down\\teu': 1424, 'down\\tfique': 1425, 'down\\tfiquem': 1426, 'down\\tnos': 1427, 'down\\tnÃ³s': 1428, 'down\\tse': 1429, 'down\\tsentese': 1430, 'down\\tte': 1431, 'down\\tvenha': 1432, 'down\\tvÃ¡': 1433, 'dozed': 1434, 'dozed\\ttom': 1435, 'drag\\tque': 1436, 'drank': 1437, 'draw': 1438, 'dream': 1439, 'dream\\teu': 1440, 'dreaming\\testou': 1441, 'dreams\\tbons': 1442, 'dressed\\tvistamse': 1443, 'dressed\\tvistase': 1444, 'drew': 1445, 'drink': 1446, 'drink\\tcoma': 1447, 'drink\\tcome': 1448, 'drink\\teu': 1449, 'drink\\tvocÃª': 1450, 'drink\\tvocÃªs': 1451, 'drinks\\ttom': 1452, 'drive': 1453, 'drive\\tdirigirei': 1454, 'drive\\tnÃ³s': 1455, 'drive\\tquem': 1456, 'drive\\ttom': 1457, 'drive\\tvamos': 1458, 'drive\\tvocÃª': 1459, 'drive\\tvocÃªs': 1460, 'drives\\ttom': 1461, 'driving\\tcontinue': 1462, 'drop': 1463, 'drove\\teu': 1464, 'drove\\ttom': 1465, 'drowned\\ttom': 1466, 'drowning\\testou': 1467, 'drunk\\tele': 1468, 'drunk\\testou': 1469, 'drunk\\teu': 1470, 'drunk\\to': 1471, 'drunk\\ttom': 1472, 'duas': 1473, 'dug': 1474, 'dumb\\teu': 1475, 'dura': 1476, 'duro': 1477, 'dusty\\testÃ¡': 1478, 'duvido': 1479, 'dying\\testamos': 1480, 'dying\\testou': 1481, 'dying\\teu': 1482, 'dying\\tquem': 1483, 'dying\\ttom': 1484, 'dying\\tvocÃª': 1485, 'dyslexic\\teu': 1486, 'dyslexic\\tsou': 1487, 'dÃ¡': 1488, 'dÃ³i': 1489, 'dÃ³lares': 1490, 'dÃºvida': 1491, 'dÃºvidas': 1492, 'e': 1493, 'early\\tcheguei': 1494, 'early\\testamos': 1495, 'early\\testou': 1496, 'early\\teu': 1497, 'early\\to': 1498, 'early\\ttom': 1499, 'early\\tvocÃª': 1500, 'ears\\tsou': 1501, 'easily\\teu': 1502, 'easter\\tfeliz': 1503, 'easy\\tacalmese': 1504, 'easy\\tisso': 1505, 'easy\\tisto': 1506, 'easy\\tpega': 1507, 'easy\\tpegue': 1508, 'easy\\ttenha': 1509, 'easy\\tÃ©': 1510, 'eat': 1511, 'eat\\teu': 1512, 'eat\\to': 1513, 'eat\\tposso': 1514, 'eat\\tvamos': 1515, 'eat\\tvocÃª': 1516, 'eaten\\teu': 1517, 'eating\\tele': 1518, 'eating\\testou': 1519, 'eating\\testÃ¡': 1520, 'eating\\teu': 1521, 'eating\\to': 1522, 'eating\\ttom': 1523, 'edit\\tclique': 1524, 'editar': 1525, 'educado': 1526, 'egg\\tcozinhe': 1527, 'egoÃ\\xadsta': 1528, 'eight\\tela': 1529, 'eight\\tele': 1530, 'ela': 1531, 'elas': 1532, 'ele': 1533, 'eles': 1534, 'elk\\tÃ©': 1535, 'else\\tmais': 1536, 'else\\to': 1537, 'em': 1538, 'embo': 1539, 'embora': 1540, 'emboscada': 1541, 'embriagado': 1542, 'emocionante': 1543, 'emperrada': 1544, 'emperrado': 1545, 'empoeirado': 1546, 'empregada': 1547, 'emprego': 1548, 'empty\\testava': 1549, 'empty\\testÃ¡': 1550, 'empurra': 1551, 'empurre': 1552, 'empurrei': 1553, 'empurrem': 1554, 'empurres': 1555, 'encara': 1556, 'encarar': 1557, 'encolheu': 1558, 'encontramos': 1559, 'encontrar': 1560, 'encontrei': 1561, 'encontro': 1562, 'ended\\to': 1563, 'enfermeiro': 1564, 'enfermo': 1565, 'enganei': 1566, 'enganou': 1567, 'english\\tele': 1568, 'engorde': 1569, 'engordou': 1570, 'engraÃ§ado': 1571, 'enjoy': 1572, 'enlisted\\ttom': 1573, 'enorme': 1574, 'enraiveceu': 1575, 'ensina': 1576, 'ensopado': 1577, 'entediada': 1578, 'entediado': 1579, 'entendi': 1580, 'entendo': 1581, 'entrar': 1582, 'entraram': 1583, 'entre': 1584, 'entrei': 1585, 'entrem': 1586, 'entrou': 1587, 'entÃ£o': 1588, 'envolvido': 1589, 'envy': 1590, 'equivocada': 1591, 'era': 1592, 'errada': 1593, 'errado': 1594, 'errei': 1595, 'escapando': 1596, 'escaparam': 1597, 'escaped\\teles': 1598, 'escaped\\tescaparam': 1599, 'escaped\\teu': 1600, 'escaped\\tquem': 1601, 'escaped\\ttom': 1602, 'escapei': 1603, 'escaping\\teu': 1604, 'escapou': 1605, 'escola': 1606, 'escondase': 1607, 'esconden': 1608, 'escondendo': 1609, 'escondete': 1610, 'escorregou': 1611, 'escorreguei': 1612, 'escrevendo': 1613, 'escreveu': 1614, 'escritor': 1615, 'escuro': 1616, 'escuta': 1617, 'escutei': 1618, 'escutou': 1619, 'esforÃ§a': 1620, 'espanhol': 1621, 'espaÃ§o': 1622, 'especial': 1623, 'especiali': 1624, 'especÃ\\xadfico': 1625, 'esperamos': 1626, 'esperando': 1627, 'esperanÃ§a': 1628, 'esperar': 1629, 'esperaram': 1630, 'esperaremos': 1631, 'espere': 1632, 'esperei': 1633, 'espero': 1634, 'esperou': 1635, 'esperta': 1636, 'esperto': 1637, 'espertos': 1638, 'espirituoso': 1639, 'espirrei': 1640, 'espirrou': 1641, 'espiÃ£o': 1642, 'esquecemos': 1643, 'esqueceu': 1644, 'esqueci': 1645, 'esquerda': 1646, 'esqueÃ§a': 1647, 'esquiando': 1648, 'esquiar': 1649, 'esquio': 1650, 'esquisito': 1651, 'essa': 1652, 'esse': 1653, 'esses': 1654, 'estamos': 1655, 'estaremos': 1656, 'estava': 1657, 'estou': 1658, 'estranho': 1659, 'estrela': 1660, 'estudando': 1661, 'estudar': 1662, 'estudei': 1663, 'estudo': 1664, 'estÃ¡': 1665, 'estÃ¡s': 1666, 'estÃ¡vamos': 1667, 'estÃ£o': 1668, 'estÃºpido': 1669, 'eu': 1670, 'even\\testamos': 1671, 'evening\\tboa': 1672, 'everybody\\tolÃ¡': 1673, 'everyone': 1674, 'evil\\ta': 1675, 'evil\\tcain': 1676, 'evil\\telas': 1677, 'evil\\teles': 1678, 'evil\\tnÃ£o': 1679, 'evita': 1680, 'evitame': 1681, 'ex': 1682, 'ex\\ttom': 1683, 'exalou': 1684, 'examine': 1685, 'exausto': 1686, 'excitado': 1687, 'excited\\testou': 1688, 'exciting\\tque': 1689, 'excuse': 1690, 'exercised\\teu': 1691, 'exercised\\tme': 1692, 'exercitei': 1693, 'exhaled\\ttom': 1694, 'exigente': 1695, 'exist\\tfantasmas': 1696, 'existe': 1697, 'existem': 1698, 'exists\\tdeus': 1699, 'experimentar': 1700, 'expert\\tpergunte': 1701, 'explain\\texplicarei': 1702, 'eyes': 1703, 'eyes\\teu': 1704, 'f': 1705, 'face\\tele': 1706, 'facho': 1707, 'facilmente': 1708, 'fail\\telas': 1709, 'fail\\teles': 1710, 'failed\\teu': 1711, 'failed\\tfalhamos': 1712, 'failed\\tfalhou': 1713, 'failed\\tnÃ³s': 1714, 'failed\\ttom': 1715, 'fainted\\tdesmaiei': 1716, 'fainted\\teu': 1717, 'fainted\\ttom': 1718, 'fainted\\tvocÃª': 1719, 'fainted\\tvocÃªs': 1720, 'fair\\taquilo': 1721, 'fair\\teu': 1722, 'fair\\tnÃ³s': 1723, 'fair\\tsede': 1724, 'fair\\tseja': 1725, 'fair\\tsejam': 1726, 'fair\\tsÃª': 1727, 'fair\\ttom': 1728, 'fair\\tvocÃª': 1729, 'fair\\tÃ©': 1730, 'faith\\ttenha': 1731, 'faith\\ttenham': 1732, 'fake\\tÃ©': 1733, 'fala': 1734, 'falando': 1735, 'falar': 1736, 'falarÃ¡': 1737, 'fale': 1738, 'falei': 1739, 'falem': 1740, 'fales': 1741, 'falhamos': 1742, 'falhou': 1743, 'falido': 1744, 'fall\\to': 1745, 'falo': 1746, 'falou': 1747, 'falta': 1748, 'faltando': 1749, 'faminto': 1750, 'famintos': 1751, 'famoso': 1752, 'famosos': 1753, 'famous\\testou': 1754, 'famous\\tsomos': 1755, 'famous\\ttom': 1756, 'fan\\ttom': 1757, 'fan\\ttu': 1758, 'fan\\tvocÃª': 1759, 'fantastic\\tfantÃ¡stico': 1760, 'far': 1761, 'far\\testÃ¡': 1762, 'far\\tisso': 1763, 'far\\tisto': 1764, 'far\\tÃ©': 1765, 'farei': 1766, 'faremos': 1767, 'faria': 1768, 'farmer\\teu': 1769, 'farmer\\tsou': 1770, 'farÃ¡': 1771, 'fast\\telas': 1772, 'fast\\tele': 1773, 'fast\\teles': 1774, 'fast\\teu': 1775, 'fast\\tfalo': 1776, 'fast\\tos': 1777, 'fast\\tsou': 1778, 'fast\\ttom': 1779, 'faster\\tdirija': 1780, 'faster\\teu': 1781, 'fasting\\testou': 1782, 'fasting\\teu': 1783, 'fat\\testou': 1784, 'fat\\teu': 1785, 'fat\\tnÃ£o': 1786, 'fat\\tsou': 1787, 'fat\\ttom': 1788, 'fat\\tvocÃª': 1789, 'father\\teu': 1790, 'father\\tsou': 1791, 'favor': 1792, 'favor\\tfaÃ§ame': 1793, 'faz': 1794, 'fazemos': 1795, 'fazendeiro': 1796, 'fazendo': 1797, 'fazer': 1798, 'fazÃªlo': 1799, 'faÃ§a': 1800, 'faÃ§o': 1801, 'fear': 1802, 'feast\\tque': 1803, 'fede': 1804, 'feed': 1805, 'feel': 1806, 'feet': 1807, 'feia': 1808, 'feias': 1809, 'feijÃ£o': 1810, 'feio': 1811, 'feios': 1812, 'feito': 1813, 'feliz': 1814, 'felizes': 1815, 'fell\\telas': 1816, 'fell\\teles': 1817, 'fell\\teu': 1818, 'fell\\tquem': 1819, 'fell\\tsentiram': 1820, 'fell\\ttom': 1821, 'felt': 1822, 'ferido': 1823, 'feriste': 1824, 'feriu': 1825, 'ferro': 1826, 'festejar': 1827, 'fez': 1828, 'ficar': 1829, 'ficarÃ¡': 1830, 'fico': 1831, 'ficou': 1832, 'fight': 1833, 'fight\\teu': 1834, 'fight\\tnÃ£o': 1835, 'fight\\tnÃ³s': 1836, 'fight\\tquem': 1837, 'fighting\\teu': 1838, 'filho': 1839, 'fill': 1840, 'find': 1841, 'fine': 1842, 'fine\\teles': 1843, 'fine\\testamos': 1844, 'fine\\testou': 1845, 'fine\\testÃ¡': 1846, 'fine\\teu': 1847, 'fine\\to': 1848, 'fine\\tvocÃª': 1849, 'fine\\tvou': 1850, 'finicky\\teu': 1851, 'finish': 1852, 'finished\\tjÃ¡': 1853, 'finished\\to': 1854, 'finished\\ttom': 1855, 'fique': 1856, 'fiquei': 1857, 'fire': 1858, 'fire\\tabrir': 1859, 'fire\\tfogo': 1860, 'fire\\tpreparar': 1861, 'fired': 1862, 'fired\\tdespediramme': 1863, 'fired\\testou': 1864, 'fired\\teu': 1865, 'fired\\tvocÃª': 1866, 'fired\\tvocÃªs': 1867, 'firme': 1868, 'first\\tas': 1869, 'first\\tele': 1870, 'first\\tpor': 1871, 'first\\tprimeiro': 1872, 'first\\tquem': 1873, 'first\\ttu': 1874, 'first\\tvocÃª': 1875, 'fish': 1876, 'fish\\teu': 1877, 'fish\\tisto': 1878, 'fish\\tvocÃª': 1879, 'fit\\testou': 1880, 'fit\\tvocÃª': 1881, 'fix': 1882, 'fixed': 1883, 'fiz': 1884, 'flies\\to': 1885, 'flinched\\ttom': 1886, 'floats\\ta': 1887, 'florescem': 1888, 'flowers': 1889, 'fly\\teu': 1890, 'fly\\tnÃ£o': 1891, 'fly\\tos': 1892, 'fly\\tpÃ¡ssaros': 1893, 'focado': 1894, 'foco': 1895, 'focused\\tmantenha': 1896, 'focused\\tmantenhase': 1897, 'focused\\tse': 1898, 'fofinha': 1899, 'fofinho': 1900, 'fofo': 1901, 'fogo': 1902, 'foi': 1903, 'foise': 1904, 'follow': 1905, 'follow\\tnÃ³s': 1906, 'fome': 1907, 'food\\teu': 1908, 'food\\tisso': 1909, 'food\\ttemos': 1910, 'food\\ttraga': 1911, 'food\\ttragam': 1912, 'food\\tÃ©': 1913, 'fool\\tele': 1914, 'fool\\teu': 1915, 'fool\\tnÃ£o': 1916, 'fooled': 1917, 'fools\\tvocÃªs': 1918, 'for': 1919, 'for\\tpara': 1920, 'fora': 1921, 'foram': 1922, 'forget': 1923, 'forget\\tnÃ£o': 1924, 'forgive': 1925, 'forgot': 1926, 'forgot\\tesquecemos': 1927, 'forgot\\tesqueci': 1928, 'forgot\\teu': 1929, 'forgot\\tme': 1930, 'forgot\\tnÃ³s': 1931, 'forgot\\ttom': 1932, 'forma': 1933, 'forte': 1934, 'fortes': 1935, 'forward\\tum': 1936, 'forward\\tvenha': 1937, 'forÃ§adamente': 1938, 'fought\\ttom': 1939, 'found': 1940, 'fracassarÃ£o': 1941, 'fracassei': 1942, 'fraco': 1943, 'fracos': 1944, 'francÃªs': 1945, 'franziu': 1946, 'free': 1947, 'free\\testamos': 1948, 'free\\testou': 1949, 'free\\testÃ¡': 1950, 'free\\teu': 1951, 'free\\tisso': 1952, 'free\\tliberte': 1953, 'free\\tlibertem': 1954, 'free\\tnÃ£o': 1955, 'free\\tsomos': 1956, 'free\\tvocÃª': 1957, 'free\\tÃ©': 1958, 'freezing\\testou': 1959, 'french\\tele': 1960, 'frente': 1961, 'frequentemente': 1962, 'frequÃªncia': 1963, 'friendly\\tsede': 1964, 'friendly\\tseja': 1965, 'friendly\\tsejam': 1966, 'friendly\\tsÃª': 1967, 'frio': 1968, 'frowned\\ttom': 1969, 'fruit\\teu': 1970, 'fruit\\tÃ©': 1971, 'fruta': 1972, 'frutas': 1973, 'fugindo': 1974, 'fugiu': 1975, 'fui': 1976, 'full\\testou': 1977, 'full\\teu': 1978, 'full\\tvocÃª': 1979, 'fuma': 1980, 'fumam': 1981, 'fumar': 1982, 'fume': 1983, 'fumem': 1984, 'fumo': 1985, 'fun\\ta': 1986, 'fun\\tdivertete': 1987, 'fun\\tdivirtamse': 1988, 'fun\\tdivirtase': 1989, 'fun\\teu': 1990, 'fun\\tfoi': 1991, 'fun\\tisso': 1992, 'fun\\tisto': 1993, 'fun\\tnÃ£o': 1994, 'fun\\tnÃ³s': 1995, 'fun\\tque': 1996, 'fun\\tserÃ¡': 1997, 'fun\\ttom': 1998, 'fun\\tÃ©': 1999, 'funciona': 2000, 'funcionando': 2001, 'funcionar': 2002, 'funcionou': 2003, 'fundo': 2004, 'funny\\tfoi': 2005, 'funny\\tisso': 2006, 'funny\\ttom': 2007, 'funny\\tvocÃª': 2008, 'furiosa': 2009, 'furioso': 2010, 'furious\\testou': 2011, 'fussy\\ttom': 2012, 'fÃ¡cil': 2013, 'fÃ£': 2014, 'fÃ©': 2015, 'gagueja': 2016, 'ganancioso': 2017, 'gananciosos': 2018, 'ganha': 2019, 'ganhando': 2020, 'ganhar': 2021, 'ganharam': 2022, 'ganharÃ¡': 2023, 'ganhe': 2024, 'ganhei': 2025, 'ganho': 2026, 'ganhou': 2027, 'garbage\\tÃ©': 2028, 'garota': 2029, 'garotas': 2030, 'garoto': 2031, 'gasped\\ttom': 2032, 'gata': 2033, 'gato': 2034, 'gatos': 2035, 'gave': 2036, 'gawking\\tnÃ£o': 2037, 'gelo': 2038, 'gemeu': 2039, 'generoso': 2040, 'genius\\teu': 2041, 'gente': 2042, 'gentil': 2043, 'gentileza': 2044, 'gentis': 2045, 'gently\\tfaz': 2046, 'get': 2047, 'ghosts': 2048, 'gift\\tÃ©': 2049, 'giggled\\tmaria': 2050, 'giggled\\tmary': 2051, 'giggled\\ttom': 2052, 'girl\\teu': 2053, 'girls\\teu': 2054, 'girls\\tgosto': 2055, 'girls\\toi': 2056, 'girls\\tolÃ¡': 2057, 'give': 2058, 'glad\\testou': 2059, 'glad\\ttom': 2060, 'gloated\\ttom': 2061, 'glue\\teu': 2062, 'go': 2063, 'go\\tcomo': 2064, 'go\\tdeixeme': 2065, 'go\\tdevo': 2066, 'go\\tei': 2067, 'go\\teu': 2068, 'go\\tnÃ£o': 2069, 'go\\tnÃ³s': 2070, 'go\\to': 2071, 'go\\tpara': 2072, 'go\\tposso': 2073, 'go\\tpra': 2074, 'go\\tquem': 2075, 'go\\tquero': 2076, 'go\\ttenho': 2077, 'go\\ttive': 2078, 'go\\ttom': 2079, 'go\\ttu': 2080, 'go\\tvai': 2081, 'go\\tvamos': 2082, 'go\\tvocÃª': 2083, 'go\\tvÃ¡': 2084, 'go\\tÃ©': 2085, 'god': 2086, 'god\\tconfia': 2087, 'god\\tconfie': 2088, 'going\\tcontinue': 2089, 'going\\testou': 2090, 'going\\teu': 2091, 'going\\tjÃ¡': 2092, 'going\\tnÃ³s': 2093, 'going\\ttom': 2094, 'going\\tvocÃª': 2095, 'going\\tvocÃªs': 2096, 'going\\tvoume': 2097, 'gold\\tisso': 2098, 'golf\\teu': 2099, 'golfe': 2100, 'gone\\telas': 2101, 'gone\\teles': 2102, 'gone\\to': 2103, 'gone\\ttom': 2104, 'good': 2105, 'good\\ta': 2106, 'good\\testava': 2107, 'good\\teu': 2108, 'good\\tisso': 2109, 'good\\tnÃ£o': 2110, 'good\\ttom': 2111, 'good\\tvocÃª': 2112, 'good\\tÃ©': 2113, 'goodbye\\tatÃ©': 2114, 'goodbye\\tdiga': 2115, 'goodbye\\ttchau': 2116, 'gorda': 2117, 'gordinho': 2118, 'gordo': 2119, 'gosta': 2120, 'gostamos': 2121, 'gostei': 2122, 'gosto': 2123, 'gostou': 2124, 'got': 2125, 'grab': 2126, 'grande': 2127, 'grandes': 2128, 'granizo': 2129, 'gratuito': 2130, 'grave': 2131, 'graÃ§a': 2132, 'great\\tque': 2133, 'greedy\\tnÃ³s': 2134, 'greedy\\ttom': 2135, 'grimaced\\ttom': 2136, 'grinned\\ttom': 2137, 'gritar': 2138, 'gritaram': 2139, 'gritaremos': 2140, 'grite': 2141, 'gritei': 2142, 'gritem': 2143, 'grites': 2144, 'gritou': 2145, 'groaned\\ttom': 2146, 'groggy\\ttom': 2147, 'grogue': 2148, 'gross\\tvocÃª': 2149, 'grosseiro': 2150, 'grounded\\testou': 2151, 'grow\\tas': 2152, 'grow\\telas': 2153, 'grow\\teles': 2154, 'grown\\tvocÃª': 2155, 'grown\\tvocÃªs': 2156, 'grows': 2157, 'grumbled\\ttom': 2158, 'grunhiu': 2159, 'grunted\\teu': 2160, 'grunted\\ttom': 2161, 'grÃ¡tis': 2162, 'grÃ¡vida': 2163, 'guerra': 2164, 'guess': 2165, 'guilty\\teu': 2166, 'guts\\tele': 2167, 'guys\\tolÃ¡': 2168, 'gÃªmeos': 2169, 'gÃªnio': 2170, 'had': 2171, 'hailing\\testÃ¡': 2172, 'hand\\tsegure': 2173, 'handled': 2174, 'hands': 2175, 'hang': 2176, 'happen\\tisso': 2177, 'happened\\taconteceu': 2178, 'happens\\tacontece': 2179, 'happens\\tisso': 2180, 'happy': 2181, 'happy\\tela': 2182, 'happy\\tele': 2183, 'happy\\testou': 2184, 'happy\\teu': 2185, 'happy\\tme': 2186, 'happy\\tnÃ³s': 2187, 'happy\\tsintome': 2188, 'happy\\tsomos': 2189, 'happy\\tsou': 2190, 'happy\\ttom': 2191, 'happy\\tvocÃª': 2192, 'hard\\ta': 2193, 'hard\\tele': 2194, 'hard\\tesforcese': 2195, 'hard\\testude': 2196, 'hard\\testudem': 2197, 'hard\\tisto': 2198, 'hard\\tmatemÃ¡tica': 2199, 'hard\\to': 2200, 'hard\\tse': 2201, 'has': 2202, 'hat\\tpreciso': 2203, 'hat\\ttraga': 2204, 'hate': 2205, 'hated\\tele': 2206, 'hates': 2207, 'have': 2208, 'have\\ttenho': 2209, 'he': 2210, 'he\\tonde': 2211, 'he\\tqual': 2212, 'he\\tquantos': 2213, 'he\\tquem': 2214, 'healthy\\testou': 2215, 'hear': 2216, 'hear\\tconsegue': 2217, 'heard': 2218, 'heavy\\ttom': 2219, 'hell\\ta': 2220, 'hello': 2221, 'hello\\talÃ´': 2222, 'hello\\tcumprimenta': 2223, 'hello\\tdiga': 2224, 'hello\\toi': 2225, 'hello\\tolÃ¡': 2226, 'help': 2227, 'help\\tajuda': 2228, 'help\\tajudaremos': 2229, 'help\\tdeixeme': 2230, 'help\\teu': 2231, 'help\\tisso': 2232, 'help\\tnÃ³s': 2233, 'help\\to': 2234, 'help\\tposso': 2235, 'help\\tprecisamos': 2236, 'help\\tpreciso': 2237, 'help\\tsocorro': 2238, 'help\\ttom': 2239, 'help\\ttraga': 2240, 'help\\ttragam': 2241, 'help\\tvocÃª': 2242, 'helped': 2243, 'helped\\teu': 2244, 'helped\\tisso': 2245, 'helped\\ttom': 2246, 'helps': 2247, 'helps\\tisso': 2248, 'her\\tamoa': 2249, 'her\\tele': 2250, 'her\\teu': 2251, 'her\\ttom': 2252, 'her\\tvocÃª': 2253, 'her\\tvocÃªs': 2254, 'her\\tÃ©': 2255, 'here': 2256, 'here\\tassina': 2257, 'here\\tassine': 2258, 'here\\tcomece': 2259, 'here\\telas': 2260, 'here\\teles': 2261, 'here\\tespere': 2262, 'here\\tesperem': 2263, 'here\\testamos': 2264, 'here\\testarei': 2265, 'here\\testava': 2266, 'here\\testou': 2267, 'here\\testÃ¡': 2268, 'here\\teu': 2269, 'here\\tfique': 2270, 'here\\tfiquem': 2271, 'here\\tmeu': 2272, 'here\\tmoro': 2273, 'here\\tnÃ³s': 2274, 'here\\tolhe': 2275, 'here\\tpare': 2276, 'here\\tquem': 2277, 'here\\tsenta': 2278, 'here\\tsentese': 2279, 'here\\ttem': 2280, 'here\\ttom': 2281, 'here\\ttraga': 2282, 'here\\tvem': 2283, 'here\\tvenha': 2284, 'here\\tvocÃª': 2285, 'here\\tvolte': 2286, 'here\\tÃ©': 2287, 'heres\\taqui': 2288, 'hero\\teu': 2289, 'hero\\tsou': 2290, 'heroic\\tele': 2291, 'heroico': 2292, 'heroÃ\\xadna': 2293, 'hers\\tisso': 2294, 'hers\\tÃ©': 2295, 'herÃ³i': 2296, 'hes': 2297, 'hey': 2298, 'hi': 2299, 'hi\\toi': 2300, 'hide\\tcorra': 2301, 'hide\\tcorre': 2302, 'hiding\\teu': 2303, 'hiding\\tnÃ³s': 2304, 'higher\\tmire': 2305, 'him': 2306, 'him\\tagarremno': 2307, 'him\\tagarreo': 2308, 'him\\tajudeo': 2309, 'him\\talgemeo': 2310, 'him\\tamoo': 2311, 'him\\tconfiamos': 2312, 'him\\tconheÃ§oo': 2313, 'him\\tela': 2314, 'him\\tesqueÃ§ao': 2315, 'him\\testou': 2316, 'him\\teu': 2317, 'him\\tignora': 2318, 'him\\tignore': 2319, 'him\\tignoreo': 2320, 'him\\tirei': 2321, 'him\\tliberteo': 2322, 'him\\tliguei': 2323, 'him\\tolhe': 2324, 'him\\tpegueo': 2325, 'him\\tsigamno': 2326, 'him\\tsigao': 2327, 'him\\ttom': 2328, 'him\\ttu': 2329, 'him\\tvocÃª': 2330, 'him\\tÃ©': 2331, 'hip': 2332, 'hipÃ³crita': 2333, 'hired': 2334, 'hired\\testou': 2335, 'hired\\teu': 2336, 'his': 2337, 'his\\tisto': 2338, 'his\\tÃ©': 2339, 'hit': 2340, 'hit\\tfui': 2341, 'hit\\ttom': 2342, 'hoj': 2343, 'hoje': 2344, 'hold': 2345, 'hole\\tele': 2346, 'hole\\teu': 2347, 'home': 2348, 'home\\tagora': 2349, 'home\\tbemvindo': 2350, 'home\\tbemvindos': 2351, 'home\\tcheguei': 2352, 'home\\tcorra': 2353, 'home\\tcorri': 2354, 'home\\telas': 2355, 'home\\teles': 2356, 'home\\testou': 2357, 'home\\testÃ¡': 2358, 'home\\testÃ¡s': 2359, 'home\\teu': 2360, 'home\\tligue': 2361, 'home\\tnÃ£o': 2362, 'home\\ttem': 2363, 'home\\ttom': 2364, 'home\\tvenha': 2365, 'home\\tvocÃª': 2366, 'home\\tvocÃªs': 2367, 'home\\tvÃ¡': 2368, 'home\\tvÃ£o': 2369, 'homeless\\tsou': 2370, 'homem': 2371, 'homens': 2372, 'homesick\\testou': 2373, 'honest\\teu': 2374, 'honesto': 2375, 'hope': 2376, 'hope\\teu': 2377, 'hope\\ttemos': 2378, 'hora': 2379, 'horas': 2380, 'horrible\\testou': 2381, 'horrible\\teu': 2382, 'horrible\\tsou': 2383, 'horror': 2384, 'horrÃ\\xadvel': 2385, 'hot\\tela': 2386, 'hot\\testamos': 2387, 'hot\\testou': 2388, 'hot\\testÃ¡': 2389, 'hot\\tnÃ³s': 2390, 'hot\\ttenho': 2391, 'how': 2392, 'hows': 2393, 'hug': 2394, 'hug\\teu': 2395, 'hug\\tpreciso': 2396, 'huge\\tÃ©': 2397, 'hugged': 2398, 'hugged\\telas': 2399, 'hugged\\teles': 2400, 'human\\teu': 2401, 'human\\tsou': 2402, 'humano': 2403, 'humble\\tsou': 2404, 'humilde': 2405, 'hung': 2406, 'hungry\\testamos': 2407, 'hungry\\testou': 2408, 'hungry\\teu': 2409, 'hungry\\tnÃ³s': 2410, 'hungry\\tquem': 2411, 'hungry\\tquero': 2412, 'hungry\\ttenho': 2413, 'hungry\\ttom': 2414, 'hurry': 2415, 'hurry\\tapressemonos': 2416, 'hurry\\tapressemse': 2417, 'hurry\\tapressese': 2418, 'hurry\\tdespachate': 2419, 'hurry\\tdespachemse': 2420, 'hurry\\tdevemos': 2421, 'hurry\\tdevo': 2422, 'hurry\\tpor': 2423, 'hurry\\tvamos': 2424, 'hurt': 2425, 'hurt\\talguÃ©m': 2426, 'hurt\\tas': 2427, 'hurt\\tdoemme': 2428, 'hurt\\testou': 2429, 'hurt\\tmeus': 2430, 'hurt\\tos': 2431, 'hurt\\ttom': 2432, 'hurt\\tvocÃª': 2433, 'hurt\\tvocÃªs': 2434, 'hurts\\tisso': 2435, 'hurts\\tminha': 2436, 'hurts\\to': 2437, 'hÃ¡': 2438, 'i': 2439, 'i\\tonde': 2440, 'i\\tquem': 2441, 'ice\\teu': 2442, 'ice\\tvou': 2443, 'icky\\tisso': 2444, 'id': 2445, 'idad': 2446, 'idade': 2447, 'idea\\tnÃ£o': 2448, 'idea\\tque': 2449, 'ideia': 2450, 'idiot\\tidiota': 2451, 'idiot\\tseu': 2452, 'idiota': 2453, 'idoso': 2454, 'ienes': 2455, 'ignore': 2456, 'ignored': 2457, 'ignorou': 2458, 'ill': 2459, 'ill\\tele': 2460, 'ill\\teu': 2461, 'ill\\to': 2462, 'im': 2463, 'imediatamente': 2464, 'imitaÃ§Ã£o': 2465, 'impiedoso': 2466, 'importa': 2467, 'importamos': 2468, 'importo': 2469, 'importou': 2470, 'impostos': 2471, 'improvised\\teu': 2472, 'improvisei': 2473, 'in': 2474, 'in\\tconte': 2475, 'in\\tdeixeme': 2476, 'in\\tdeixenos': 2477, 'in\\tdeixeo': 2478, 'in\\telas': 2479, 'in\\tele': 2480, 'in\\teles': 2481, 'in\\tentra': 2482, 'in\\tentre': 2483, 'in\\tentrem': 2484, 'in\\testÃ£o': 2485, 'in\\teu': 2486, 'in\\tfaÃ§ao': 2487, 'in\\tmandeo': 2488, 'in\\tmary': 2489, 'in\\tnÃ£o': 2490, 'in\\to': 2491, 'in\\tposso': 2492, 'in\\ttom': 2493, 'in\\ttragao': 2494, 'in\\tvocÃª': 2495, 'in\\tvou': 2496, 'inalou': 2497, 'included\\teu': 2498, 'incluÃ\\xaddo': 2499, 'incomum': 2500, 'incrÃ\\xadvel': 2501, 'indo': 2502, 'inferno': 2503, 'inglÃªs': 2504, 'ingÃªnua': 2505, 'ingÃªnuo': 2506, 'inhaled\\ttom': 2507, 'injured\\testou': 2508, 'injusto': 2509, 'innocent\\teu': 2510, 'inocente': 2511, 'insano': 2512, 'insensÃ\\xadvel': 2513, 'inside\\tdÃª': 2514, 'inside\\tentra': 2515, 'inside\\tentre': 2516, 'inside\\tentrem': 2517, 'inside\\testamos': 2518, 'inside\\testou': 2519, 'inside\\teu': 2520, 'inside\\tnÃ³s': 2521, 'inside\\tvai': 2522, 'inside\\tvÃ¡': 2523, 'insisted\\ttom': 2524, 'insistente': 2525, 'insistiu': 2526, 'inteligente': 2527, 'intrigado': 2528, 'intrometido': 2529, 'invejo': 2530, 'inventei': 2531, 'invited\\teu': 2532, 'involved\\testou': 2533, 'involved\\teu': 2534, 'inÃºtil': 2535, 'ir': 2536, 'irei': 2537, 'iremos': 2538, 'iron': 2539, 'irÃ¡': 2540, 'is': 2541, 'is\\taqui': 2542, 'is\\tlÃ¡': 2543, 'is\\tsÃ£o': 2544, 'isnt': 2545, 'isso': 2546, 'isto': 2547, 'it': 2548, 'it\\ta': 2549, 'it\\tachei': 2550, 'it\\tagora': 2551, 'it\\tcadÃª': 2552, 'it\\tcomo': 2553, 'it\\tde': 2554, 'it\\tdeixa': 2555, 'it\\tdeixe': 2556, 'it\\tdesista': 2557, 'it\\tela': 2558, 'it\\tele': 2559, 'it\\tencontrei': 2560, 'it\\tencontreia': 2561, 'it\\tentendeu': 2562, 'it\\tentendi': 2563, 'it\\tesquece': 2564, 'it\\tesqueci': 2565, 'it\\tesqueÃ§a': 2566, 'it\\tesqueÃ§am': 2567, 'it\\testou': 2568, 'it\\testÃ¡': 2569, 'it\\teu': 2570, 'it\\tfaremos': 2571, 'it\\tfica': 2572, 'it\\tfique': 2573, 'it\\tfiquem': 2574, 'it\\tgostamos': 2575, 'it\\tirei': 2576, 'it\\tlargue': 2577, 'it\\tlembre': 2578, 'it\\tme': 2579, 'it\\tmemorize': 2580, 'it\\tnÃ£o': 2581, 'it\\tnÃ³s': 2582, 'it\\to': 2583, 'it\\tonde': 2584, 'it\\tpara': 2585, 'it\\tpare': 2586, 'it\\tpegaa': 2587, 'it\\tpegao': 2588, 'it\\tpeguea': 2589, 'it\\tpeguemna': 2590, 'it\\tpeguemno': 2591, 'it\\tpegueo': 2592, 'it\\tperdi': 2593, 'it\\tperdio': 2594, 'it\\tpodemos': 2595, 'it\\tposso': 2596, 'it\\tpreciso': 2597, 'it\\tprovea': 2598, 'it\\tproveo': 2599, 'it\\tquem': 2600, 'it\\tqueremos': 2601, 'it\\tquero': 2602, 'it\\tquÃ£o': 2603, 'it\\tsaquei': 2604, 'it\\tsaudades': 2605, 'it\\tse': 2606, 'it\\tsegura': 2607, 'it\\tsegure': 2608, 'it\\tsolte': 2609, 'it\\tsolteo': 2610, 'it\\tsubstitua': 2611, 'it\\ttentao': 2612, 'it\\ttodos': 2613, 'it\\ttom': 2614, 'it\\tvamos': 2615, 'it\\tvaza': 2616, 'it\\tvocÃª': 2617, 'it\\tvocÃªs': 2618, 'it\\tvÃ¡': 2619, 'it\\tÃ©': 2620, 'itch\\tmeus': 2621, 'itd': 2622, 'itll': 2623, 'its': 2624, 'its\\tsÃ£o': 2625, 'ive': 2626, 'japanese\\teu': 2627, 'japanese\\tsou': 2628, 'japonesa': 2629, 'japonÃªs': 2630, 'jaw': 2631, 'jazz': 2632, 'jazz\\teu': 2633, 'jealous\\teu': 2634, 'jealous\\tsou': 2635, 'jeito': 2636, 'jejuando': 2637, 'jejum': 2638, 'jerk\\tque': 2639, 'jesus': 2640, 'job\\tarrume': 2641, 'job\\tbom': 2642, 'job\\teu': 2643, 'job\\tpreciso': 2644, 'job\\tÃ©': 2645, 'jogar': 2646, 'jogue': 2647, 'join': 2648, 'joke\\tÃ©': 2649, 'jokes\\teu': 2650, 'joking\\testamos': 2651, 'joking\\testou': 2652, 'joking\\teu': 2653, 'jovem': 2654, 'jovens': 2655, 'jump\\teu': 2656, 'jump\\tnÃ£o': 2657, 'jump\\tpule': 2658, 'jump\\tpulem': 2659, 'jumped\\teu': 2660, 'jumped\\ttom': 2661, 'junto': 2662, 'jurou': 2663, 'just': 2664, 'justa': 2665, 'justas': 2666, 'justo': 2667, 'justos': 2668, 'jÃ¡': 2669, 'keep': 2670, 'key\\ttraga': 2671, 'kicked': 2672, 'kid': 2673, 'kidding\\teu': 2674, 'kidding\\tsem': 2675, 'kidding\\tsÃ©rio': 2676, 'kids\\telas': 2677, 'kids\\teles': 2678, 'kids\\tsÃ£o': 2679, 'kill': 2680, 'kind\\tele': 2681, 'kind\\tsede': 2682, 'kind\\tseja': 2683, 'kind\\tsejam': 2684, 'kind\\tsÃª': 2685, 'kind\\ttom': 2686, 'kind\\tvocÃª': 2687, 'kiss': 2688, 'kissed': 2689, 'kissed\\teles': 2690, 'klutz\\teu': 2691, 'klutz\\tsou': 2692, 'kneeled\\ttom': 2693, 'knew': 2694, 'knew\\ttom': 2695, 'know': 2696, 'know\\telas': 2697, 'know\\tele': 2698, 'know\\teles': 2699, 'know\\teu': 2700, 'know\\tnÃ³s': 2701, 'know\\tsei': 2702, 'know\\ttodos': 2703, 'know\\ttom': 2704, 'knows': 2705, 'knows\\tninguÃ©m': 2706, 'knows\\tquem': 2707, 'knows\\ttom': 2708, 'ladies': 2709, 'ladrÃ£o': 2710, 'lar': 2711, 'last\\teu': 2712, 'lasts\\to': 2713, 'late\\teles': 2714, 'late\\testamos': 2715, 'late\\testou': 2716, 'late\\testÃ¡': 2717, 'late\\teu': 2718, 'late\\tnÃ£o': 2719, 'late\\to': 2720, 'late\\ttarde': 2721, 'late\\ttom': 2722, 'late\\tvocÃª': 2723, 'late\\tvou': 2724, 'late\\tÃ©': 2725, 'latem': 2726, 'later\\tligue': 2727, 'later\\tme': 2728, 'laugh\\tnÃ£o': 2729, 'laugh\\to': 2730, 'laughed\\tele': 2731, 'laughed\\teles': 2732, 'laughed\\teu': 2733, 'laughed\\tnÃ³s': 2734, 'laughed\\tri': 2735, 'laughed\\ttom': 2736, 'lavei': 2737, 'law\\teu': 2738, 'law\\tÃ©': 2739, 'lazy\\tele': 2740, 'lazy\\tnÃ£o': 2741, 'lazy\\tnÃ³s': 2742, 'lazy\\ttom': 2743, 'leal': 2744, 'learn\\taprenderei': 2745, 'learn\\teu': 2746, 'learn\\tirei': 2747, 'learn\\tvou': 2748, 'leave': 2749, 'leave\\tabandone': 2750, 'leave\\tdeixame': 2751, 'leave\\tdeixeme': 2752, 'leave\\testou': 2753, 'leave\\teu': 2754, 'leave\\tnÃ£o': 2755, 'leave\\tsaia': 2756, 'leave\\tvamos': 2757, 'leave\\tvou': 2758, 'leave\\tvÃ¡': 2759, 'leaving\\testou': 2760, 'leciono': 2761, 'left': 2762, 'left\\tele': 2763, 'left\\teles': 2764, 'left\\teu': 2765, 'left\\ttom': 2766, 'left\\tvire': 2767, 'leg': 2768, 'legais': 2769, 'legal': 2770, 'legs': 2771, 'lei': 2772, 'leio': 2773, 'leite': 2774, 'lembrados': 2775, 'lembramos': 2776, 'lembro': 2777, 'lendo': 2778, 'lento': 2779, 'ler': 2780, 'let': 2781, 'lets': 2782, 'levantar': 2783, 'levante': 2784, 'levantei': 2785, 'levantou': 2786, 'leve': 2787, 'lhe': 2788, 'liar\\tele': 2789, 'liar\\teu': 2790, 'liar\\tnÃ£o': 2791, 'licenÃ§a': 2792, 'lidei': 2793, 'lie\\taquilo': 2794, 'lie\\teu': 2795, 'lie\\tisso': 2796, 'lie\\tnÃ£o': 2797, 'lied': 2798, 'lied\\teles': 2799, 'lied\\tninguÃ©m': 2800, 'lied\\ttom': 2801, 'lies\\tele': 2802, 'lies\\ttom': 2803, 'life': 2804, 'life\\tÃ©': 2805, 'lifes': 2806, 'lift': 2807, 'liga': 2808, 'ligada': 2809, 'ligar': 2810, 'ligaram': 2811, 'lighten': 2812, 'ligo': 2813, 'ligou': 2814, 'ligue': 2815, 'liguei': 2816, 'like': 2817, 'liked': 2818, 'likes': 2819, 'limpo': 2820, 'lip\\teu': 2821, 'lips\\tleio': 2822, 'listen': 2823, 'listen\\tescuta': 2824, 'listen\\tescutai': 2825, 'listen\\tescute': 2826, 'listen\\tescutem': 2827, 'listen\\tescutemme': 2828, 'listen\\touÃ§a': 2829, 'listen\\touÃ§ame': 2830, 'listen\\tvamos': 2831, 'listened\\tquem': 2832, 'listened\\ttom': 2833, 'listens\\ttom': 2834, 'litter\\tnÃ£o': 2835, 'live': 2836, 'live\\tele': 2837, 'live\\teu': 2838, 'livre': 2839, 'livres': 2840, 'livro': 2841, 'livros': 2842, 'lixo': 2843, 'lobo': 2844, 'locked\\testÃ¡': 2845, 'logo': 2846, 'lonely\\teu': 2847, 'lonely\\to': 2848, 'long\\tnÃ£o': 2849, 'longe': 2850, 'look': 2851, 'look\\tcomo': 2852, 'look\\tdÃ¡': 2853, 'look\\tdÃª': 2854, 'look\\teu': 2855, 'look\\tolha': 2856, 'look\\tolhe': 2857, 'look\\tveja': 2858, 'look\\tvÃª': 2859, 'looked\\teu': 2860, 'looked\\ttom': 2861, 'looking\\tfique': 2862, 'looks': 2863, 'lose\\teu': 2864, 'lose\\ttom': 2865, 'lose\\tvocÃª': 2866, 'lose\\tvou': 2867, 'loser\\tque': 2868, 'losers\\tsomos': 2869, 'losing\\testou': 2870, 'lost': 2871, 'lost\\teles': 2872, 'lost\\tenganÃ¡monos': 2873, 'lost\\testamos': 2874, 'lost\\testou': 2875, 'lost\\teu': 2876, 'lost\\tme': 2877, 'lost\\tnÃ³s': 2878, 'lost\\to': 2879, 'lost\\tperdemos': 2880, 'lost\\tse': 2881, 'lost\\ttom': 2882, 'lost\\tvaite': 2883, 'lost\\tvocÃª': 2884, 'lost\\tvocÃªs': 2885, 'lost\\tvÃ¡se': 2886, 'lot\\teu': 2887, 'lot\\tleio': 2888, 'lot\\tmuito': 2889, 'lot\\ttrabalho': 2890, 'lot\\tÃ©': 2891, 'louco': 2892, 'loucura': 2893, 'love': 2894, 'love\\testou': 2895, 'love\\tisso': 2896, 'love\\to': 2897, 'love\\tque': 2898, 'loved': 2899, 'loved\\teu': 2900, 'loved\\tsou': 2901, 'lovely\\tque': 2902, 'loves': 2903, 'loyal\\teu': 2904, 'luck\\tdesejeme': 2905, 'lucky\\tela': 2906, 'lucky\\teu': 2907, 'lucky\\to': 2908, 'lucky\\ttive': 2909, 'lucky\\ttom': 2910, 'lucky\\tvocÃª': 2911, 'lugar': 2912, 'lunch\\talmoÃ§amos': 2913, 'lutando': 2914, 'lutar': 2915, 'lutaremos': 2916, 'lutarÃ¡': 2917, 'lute': 2918, 'lutem': 2919, 'lutes': 2920, 'lutou': 2921, 'lying\\tele': 2922, 'lying\\testou': 2923, 'lying\\to': 2924, 'lying\\tpare': 2925, 'lying\\ttom': 2926, 'lying\\tvocÃª': 2927, 'lÃ¡': 2928, 'lÃ¡bio': 2929, 'lÃ¡bios': 2930, 'lÃ¡stima': 2931, 'm': 2932, 'machuca': 2933, 'machucada': 2934, 'machucado': 2935, 'machucaram': 2936, 'machucou': 2937, 'mad\\testou': 2938, 'mad\\testÃ¡s': 2939, 'mad\\teu': 2940, 'mad\\tfiquei': 2941, 'mad\\tnÃ£o': 2942, 'mad\\ttom': 2943, 'mad\\tvocÃª': 2944, 'made': 2945, 'madeira': 2946, 'maduros': 2947, 'magic\\tfoi': 2948, 'magro': 2949, 'maid\\tele': 2950, 'mais': 2951, 'make': 2952, 'mal': 2953, 'malas': 2954, 'maldiÃ§Ã£o': 2955, 'malucas': 2956, 'maluco': 2957, 'malucos': 2958, 'malvado': 2959, 'mama': 2960, 'man\\teu': 2961, 'man\\tsou': 2962, 'manage\\tconseguirei': 2963, 'managing\\testou': 2964, 'manda': 2965, 'mandona': 2966, 'mandÃ£o': 2967, 'mandÃ\\xadbula': 2968, 'maneira': 2969, 'mantenha': 2970, 'many': 2971, 'map\\teu': 2972, 'map\\tpreciso': 2973, 'mapa': 2974, 'maravilha': 2975, 'maria': 2976, 'married\\teu': 2977, 'married\\tsou': 2978, 'marry': 2979, 'mary': 2980, 'mary\\teu': 2981, 'mary\\tquero': 2982, 'mary\\ttom': 2983, 'massa': 2984, 'matarei': 2985, 'matemÃ¡tica': 2986, 'math': 2987, 'math\\teu': 2988, 'matters\\timporta': 2989, 'matters\\tisso': 2990, 'mature\\testamos': 2991, 'mature\\teu': 2992, 'mature\\tnÃ³s': 2993, 'mature\\tsomos': 2994, 'maturo': 2995, 'mau': 2996, 'maus': 2997, 'may': 2998, 'maybe\\teu': 2999, 'maÃ§Ã£s': 3000, 'me': 3001, 'me\\tagora': 3002, 'me\\tajudame': 3003, 'me\\tajudeme': 3004, 'me\\tajudemme': 3005, 'me\\tassustoume': 3006, 'me\\tbeijeme': 3007, 'me\\tcase': 3008, 'me\\tcom': 3009, 'me\\tconfia': 3010, 'me\\tconfie': 3011, 'me\\tdance': 3012, 'me\\tdanÃ§a': 3013, 'me\\tdeixeme': 3014, 'me\\tdesculpa': 3015, 'me\\tdesculpe': 3016, 'me\\tdigame': 3017, 'me\\tdigamme': 3018, 'me\\tdizme': 3019, 'me\\tdÃªme': 3020, 'me\\tei': 3021, 'me\\tela': 3022, 'me\\telas': 3023, 'me\\tele': 3024, 'me\\teles': 3025, 'me\\tescrevame': 3026, 'me\\tesqueÃ§ame': 3027, 'me\\tesse': 3028, 'me\\teste': 3029, 'me\\tfale': 3030, 'me\\tfaz': 3031, 'me\\tfaÃ§a': 3032, 'me\\tfaÃ§am': 3033, 'me\\tficame': 3034, 'me\\tisso': 3035, 'me\\tme': 3036, 'me\\tmostreme': 3037, 'me\\tnÃ£o': 3038, 'me\\to': 3039, 'me\\tobserveme': 3040, 'me\\tperdoeme': 3041, 'me\\tperdÃ£o': 3042, 'me\\tpor': 3043, 'me\\tquem': 3044, 'me\\trespondame': 3045, 'me\\trespondamme': 3046, 'me\\treze': 3047, 'me\\trezem': 3048, 'me\\tsegurese': 3049, 'me\\tsentese': 3050, 'me\\tsigame': 3051, 'me\\tsigamme': 3052, 'me\\tsolteme': 3053, 'me\\tsou': 3054, 'me\\tsurpreendeume': 3055, 'me\\ttom': 3056, 'me\\tvem': 3057, 'me\\tvenha': 3058, 'me\\tvocÃª': 3059, 'me\\tvocÃªs': 3060, 'me\\tvote': 3061, 'me\\tvÃ¡': 3062, 'me\\tÃ©': 3063, 'mean': 3064, 'mean\\tnÃ£o': 3065, 'means': 3066, 'meat': 3067, 'meat\\teu': 3068, 'meat\\tnÃ³s': 3069, 'medo': 3070, 'melhor': 3071, 'memorize': 3072, 'men': 3073, 'men\\tnÃ³s': 3074, 'men\\tsomos': 3075, 'meninas': 3076, 'menino': 3077, 'meninos': 3078, 'mente': 3079, 'menti': 3080, 'mentindo': 3081, 'mentir': 3082, 'mentira': 3083, 'mentiram': 3084, 'mentiroso': 3085, 'mentiu': 3086, 'merciless\\tseja': 3087, 'mesmo': 3088, 'mess\\teu': 3089, 'mess\\tque': 3090, 'met': 3091, 'met\\tacabamos': 3092, 'meu': 3093, 'meus': 3094, 'mexa': 3095, 'mexendo': 3096, 'mexer': 3097, 'milk\\tbebi': 3098, 'milk\\teu': 3099, 'mim': 3100, 'mind': 3101, 'mind\\tesquece': 3102, 'mind\\tesqueÃ§a': 3103, 'mind\\teu': 3104, 'mind\\tnÃ£o': 3105, 'mind\\ttudo': 3106, 'mind\\tvocÃª': 3107, 'mine\\ta': 3108, 'mine\\teles': 3109, 'mine\\tera': 3110, 'mine\\tessa': 3111, 'mine\\tesse': 3112, 'mine\\teu': 3113, 'mine\\tisso': 3114, 'mine\\tisto': 3115, 'mine\\tmary': 3116, 'mine\\tpegue': 3117, 'mine\\tpreciso': 3118, 'mine\\tquero': 3119, 'mine\\tsÃ£o': 3120, 'mine\\tvocÃª': 3121, 'mine\\tÃ©': 3122, 'minha': 3123, 'minhas': 3124, 'minta': 3125, 'minto': 3126, 'minuto': 3127, 'miss': 3128, 'missed': 3129, 'missed\\tvocÃª': 3130, 'missed\\tvocÃªs': 3131, 'missing\\testÃ¡': 3132, 'mistaken\\testou': 3133, 'moaned\\ttom': 3134, 'mocked': 3135, 'modo': 3136, 'molhada': 3137, 'molhado': 3138, 'momento': 3139, 'monday\\tÃ©': 3140, 'money': 3141, 'money\\teu': 3142, 'money\\tnecessito': 3143, 'money\\tpreciso': 3144, 'monge': 3145, 'monk\\tsou': 3146, 'moramos': 3147, 'morder': 3148, 'mordeu': 3149, 'mordo': 3150, 'more\\tconteme': 3151, 'more\\tele': 3152, 'more\\tfale': 3153, 'more\\tprecisamos': 3154, 'more\\tpreciso': 3155, 'morning\\tbom': 3156, 'moro': 3157, 'morra': 3158, 'morram': 3159, 'morre': 3160, 'morremos': 3161, 'morrendo': 3162, 'morrer': 3163, 'morrera': 3164, 'morreram': 3165, 'morrerÃ¡': 3166, 'morreu': 3167, 'morta': 3168, 'mortas': 3169, 'morto': 3170, 'mortos': 3171, 'mosca': 3172, 'mostra': 3173, 'mostre': 3174, 'mova': 3175, 'move': 3176, 'move\\tnÃ£o': 3177, 'moved\\ttom': 3178, 'moving\\tcontinue': 3179, 'moving\\tmexase': 3180, 'moving\\tpara': 3181, 'moving\\tpare': 3182, 'moving\\tparem': 3183, 'mudei': 3184, 'mudou': 3185, 'muito': 3186, 'mulher': 3187, 'mulheres': 3188, 'mundo': 3189, 'music\\teu': 3190, 'muslim\\tsou': 3191, 'must': 3192, 'muÃ§ulmana': 3193, 'muÃ§ulmano': 3194, 'my': 3195, 'myself\\teu': 3196, 'mÃ¡': 3197, 'mÃ¡gico': 3198, 'mÃ¡s': 3199, 'mÃ£o': 3200, 'mÃ£os': 3201, 'mÃ£ozinha': 3202, 'mÃ©dico': 3203, 'mÃºsica': 3204, 'na': 3205, 'nada': 3206, 'nadam': 3207, 'nadar': 3208, 'nadaram': 3209, 'nadei': 3210, 'nadou': 3211, 'nailed': 3212, 'naive\\teu': 3213, 'naive\\ttom': 3214, 'naked\\testou': 3215, 'naked\\to': 3216, 'namorando': 3217, 'nap\\teu': 3218, 'nap\\tpreciso': 3219, 'nap\\ttire': 3220, 'nariz': 3221, 'nasty\\to': 3222, 'navio': 3223, 'neat\\ttom': 3224, 'necessidades': 3225, 'necessito': 3226, 'need': 3227, 'needed': 3228, 'needs': 3229, 'needs\\teu': 3230, 'needy\\tvocÃª': 3231, 'nela': 3232, 'nele': 3233, 'nem': 3234, 'nenhum': 3235, 'nervosa': 3236, 'nervoso': 3237, 'nervous\\testou': 3238, 'nervous\\teu': 3239, 'neutral\\teu': 3240, 'neutro': 3241, 'nevando': 3242, 'nevar': 3243, 'neve': 3244, 'never': 3245, 'new\\talgo': 3246, 'new\\talguma': 3247, 'new\\teles': 3248, 'new\\tisso': 3249, 'new\\to': 3250, 'new\\tquais': 3251, 'new\\tque': 3252, 'new\\tvocÃª': 3253, 'new\\tvocÃªs': 3254, 'news\\ttenho': 3255, 'next\\tquem': 3256, 'next\\tsou': 3257, 'next\\ttom': 3258, 'next\\tvocÃª': 3259, 'nice': 3260, 'nice\\tela': 3261, 'nice\\tele': 3262, 'nice\\tfoi': 3263, 'nice\\tisso': 3264, 'nice\\tisto': 3265, 'nice\\tque': 3266, 'nice\\tseja': 3267, 'nice\\tsejam': 3268, 'nice\\tÃ©': 3269, 'night\\tboa': 3270, 'night\\tera': 3271, 'night\\tque': 3272, 'ninguÃ©m': 3273, 'no': 3274, 'no\\tah': 3275, 'no\\teles': 3276, 'no\\teu': 3277, 'no\\tisso': 3278, 'no\\tnÃ£o': 3279, 'no\\ttom': 3280, 'nobody': 3281, 'nodded\\ttom': 3282, 'noite': 3283, 'nojento': 3284, 'normal': 3285, 'normal\\teu': 3286, 'normal\\tisso': 3287, 'norteamericano': 3288, 'nos': 3289, 'nose': 3290, 'nosso': 3291, 'nosy\\ttom': 3292, 'not': 3293, 'not\\tespero': 3294, 'not\\teu': 3295, 'not\\tpor': 3296, 'notei': 3297, 'nothing\\teu': 3298, 'nothing\\tisso': 3299, 'noticed\\teu': 3300, 'noticed\\tnotei': 3301, 'noticed\\ttom': 3302, 'notou': 3303, 'novamente': 3304, 'novas': 3305, 'novidade': 3306, 'novidades': 3307, 'novo': 3308, 'novos': 3309, 'now': 3310, 'now\\tagora': 3311, 'now\\tcomece': 3312, 'now\\tele': 3313, 'now\\testou': 3314, 'now\\teu': 3315, 'now\\tfique': 3316, 'now\\tsaia': 3317, 'now\\tvai': 3318, 'now\\tvÃ¡': 3319, 'ntt': 3320, 'ntt\\teu': 3321, 'nu': 3322, 'nublado': 3323, 'numb\\ttom': 3324, 'nunca': 3325, 'nurse\\teu': 3326, 'nuts\\testÃ¡': 3327, 'nuts\\testÃ¡s': 3328, 'nuts\\ttom': 3329, 'nuts\\tvocÃª': 3330, 'nÃ': 3331, 'nÃ£o': 3332, 'nÃ³s': 3333, 'o': 3334, 'obedecer': 3335, 'obedeceram': 3336, 'obedeceremos': 3337, 'obedeceu': 3338, 'obese\\testou': 3339, 'obeso': 3340, 'obey\\teu': 3341, 'obey\\tnÃ³s': 3342, 'obey\\tobedecerei': 3343, 'obeyed\\teles': 3344, 'obeyed\\ttom': 3345, 'objection\\tsem': 3346, 'objective\\tseja': 3347, 'objective\\tsejam': 3348, 'objetiva': 3349, 'objetivo': 3350, 'objetivos': 3351, 'objeto': 3352, 'objeÃ§Ãµes': 3353, 'obrigada': 3354, 'obrigado': 3355, 'ocd\\teu': 3356, 'ocd\\ttom': 3357, 'ocupada': 3358, 'ocupadas': 3359, 'ocupado': 3360, 'ocupados': 3361, 'odd\\tfoi': 3362, 'odd\\tisso': 3363, 'odd\\ttom': 3364, 'odd\\tÃ©': 3365, 'odeia': 3366, 'odeiam': 3367, 'odeio': 3368, 'odiado': 3369, 'odiamos': 3370, 'of': 3371, 'ofendido': 3372, 'off': 3373, 'off\\tacalmese': 3374, 'off\\tdesligao': 3375, 'off\\tdesligue': 3376, 'off\\tdesliguea': 3377, 'off\\tdesligueo': 3378, 'off\\tele': 3379, 'off\\to': 3380, 'off\\tpara': 3381, 'off\\trecua': 3382, 'off\\trecue': 3383, 'off\\trecuem': 3384, 'off\\ttira': 3385, 'off\\ttire': 3386, 'off\\ttirem': 3387, 'offended\\testou': 3388, 'often': 3389, 'oh': 3390, 'oito': 3391, 'ok': 3392, 'ok\\testamos': 3393, 'ok\\testou': 3394, 'ok\\testÃ¡': 3395, 'ok\\teu': 3396, 'ok\\tfoi': 3397, 'ok\\tnÃ³s': 3398, 'ok\\to': 3399, 'ok\\ttom': 3400, 'ok\\ttudo': 3401, 'ok\\ttÃ¡': 3402, 'ok\\tvai': 3403, 'ok\\tvamos': 3404, 'ok\\tvocÃª': 3405, 'ok\\tvocÃªs': 3406, 'okay\\testou': 3407, 'okay\\testÃ¡': 3408, 'okay\\teu': 3409, 'okay\\ttudo': 3410, 'okay\\tvocÃª': 3411, 'old': 3412, 'old\\ta': 3413, 'old\\tas': 3414, 'old\\tela': 3415, 'old\\tele': 3416, 'old\\teles': 3417, 'old\\testais': 3418, 'old\\teu': 3419, 'old\\to': 3420, 'old\\tos': 3421, 'old\\tsintome': 3422, 'old\\tsois': 3423, 'old\\ttom': 3424, 'old\\ttu': 3425, 'old\\tvocÃª': 3426, 'old\\tvocÃªs': 3427, 'old\\tvÃ³s': 3428, 'old\\tÃ©': 3429, 'old\\tÃ©s': 3430, 'olhada': 3431, 'olhadela': 3432, 'olhando': 3433, 'olhar': 3434, 'olhe': 3435, 'olhei': 3436, 'olhos': 3437, 'olhou': 3438, 'olÃ¡': 3439, 'ombros': 3440, 'omen\\tÃ©': 3441, 'on': 3442, 'on\\ta': 3443, 'on\\taguarde': 3444, 'on\\taguardem': 3445, 'on\\tah': 3446, 'on\\tavante': 3447, 'on\\tcoloqueo': 3448, 'on\\tdirija': 3449, 'on\\texperimentea': 3450, 'on\\texperimenteo': 3451, 'on\\tliguea': 3452, 'on\\tligueo': 3453, 'on\\tmexase': 3454, 'on\\tqual': 3455, 'on\\tvamos': 3456, 'on\\tvenha': 3457, 'on\\tvÃ¡': 3458, 'once': 3459, 'once\\tvenha': 3460, 'one': 3461, 'one\\tele': 3462, 'one\\tescolha': 3463, 'one\\teu': 3464, 'one\\tnÃ£o': 3465, 'one\\tnÃ³s': 3466, 'one\\tquero': 3467, 'online': 3468, 'online\\testou': 3469, 'online\\teu': 3470, 'open': 3471, 'open\\testÃ¡': 3472, 'opera\\teu': 3473, 'or': 3474, 'orei': 3475, 'os': 3476, 'otÃ¡rios': 3477, 'ou': 3478, 'our': 3479, 'ouro': 3480, 'ours\\tisso': 3481, 'ours\\tÃ©': 3482, 'out': 3483, 'out\\tafastese': 3484, 'out\\tagora': 3485, 'out\\tajuda': 3486, 'out\\tajude': 3487, 'out\\tajudei': 3488, 'out\\tajudeme': 3489, 'out\\tatenÃ§Ã£o': 3490, 'out\\tchame': 3491, 'out\\tcuidado': 3492, 'out\\tdeixame': 3493, 'out\\tdeixeme': 3494, 'out\\tele': 3495, 'out\\testou': 3496, 'out\\teu': 3497, 'out\\tfora': 3498, 'out\\tindique': 3499, 'out\\tme': 3500, 'out\\tnÃ£o': 3501, 'out\\tnÃ³s': 3502, 'out\\to': 3503, 'out\\tpara': 3504, 'out\\tpare': 3505, 'out\\tparem': 3506, 'out\\tpreencha': 3507, 'out\\tpreste': 3508, 'out\\tsai': 3509, 'out\\tsaia': 3510, 'out\\tsaiam': 3511, 'out\\ttom': 3512, 'out\\tvamos': 3513, 'outra': 3514, 'outro': 3515, 'outside\\tespera': 3516, 'outside\\tespere': 3517, 'outside\\tesperem': 3518, 'outside\\tvenha': 3519, 'ouvidos': 3520, 'ouvindo': 3521, 'ouvir': 3522, 'ouÃ§o': 3523, 'over': 3524, 'over\\tacabou': 3525, 'over\\tchega': 3526, 'over\\tchegue': 3527, 'over\\trecomece': 3528, 'over\\tvem': 3529, 'over\\tvenha': 3530, 'over\\tvenham': 3531, 'over\\tvire': 3532, 'overslept\\tdormi': 3533, 'overslept\\tdormimos': 3534, 'overslept\\teu': 3535, 'ovo': 3536, 'owe': 3537, 'own\\tpegue': 3538, 'p': 3539, 'pa': 3540, 'paciÃªncia': 3541, 'packing\\testou': 3542, 'padeiro': 3543, 'pagando': 3544, 'pagar': 3545, 'pagarei': 3546, 'pagarÃ¡': 3547, 'pago': 3548, 'pagou': 3549, 'paguei': 3550, 'pai': 3551, 'paid': 3552, 'paid\\teu': 3553, 'paid\\to': 3554, 'pain\\testou': 3555, 'pain\\tque': 3556, 'paint\\teu': 3557, 'pale\\tele': 3558, 'pale\\ttom': 3559, 'panic\\tnÃ£o': 3560, 'panicked\\teu': 3561, 'panicked\\tquem': 3562, 'panicked\\ttom': 3563, 'papo': 3564, 'para': 3565, 'parada': 3566, 'parado': 3567, 'parafuso': 3568, 'parar': 3569, 'pararam': 3570, 'pardon': 3571, 'pare': 3572, 'parece': 3573, 'parecida': 3574, 'parecido': 3575, 'parei': 3576, 'pareÃ§o': 3577, 'paris': 3578, 'paris\\testou': 3579, 'paris\\teu': 3580, 'parou': 3581, 'part\\teu': 3582, 'parte': 3583, 'partida': 3584, 'partir': 3585, 'party\\tvamos': 3586, 'pasmado': 3587, 'pass\\tnÃ³s': 3588, 'passaram': 3589, 'passaremos': 3590, 'passed\\tanos': 3591, 'passed\\tpassaram': 3592, 'passo': 3593, 'passou': 3594, 'pathetic\\tque': 3595, 'patient\\ttenha': 3596, 'patinar': 3597, 'patrÃ£o': 3598, 'patÃ©tico': 3599, 'pay': 3600, 'pay\\ta': 3601, 'pay\\teu': 3602, 'pay\\tnÃ³s': 3603, 'pay\\tquem': 3604, 'pay\\ttom': 3605, 'paying\\tnÃ³s': 3606, 'paying\\tquem': 3607, 'paz': 3608, 'pbs': 3609, 'pbs\\tassistimos': 3610, 'pedi': 3611, 'pegar': 3612, 'pegarei': 3613, 'pego': 3614, 'pegou': 3615, 'peguei': 3616, 'peixe': 3617, 'peixes': 3618, 'pelada': 3619, 'pelado': 3620, 'pelo': 3621, 'pen\\teu': 3622, 'pen\\ttenho': 3623, 'pena': 3624, 'pensando': 3625, 'pequena': 3626, 'pequeno': 3627, 'perco': 3628, 'perdedor': 3629, 'perdedora': 3630, 'perdedores': 3631, 'perdemos': 3632, 'perdendo': 3633, 'perder': 3634, 'perderam': 3635, 'perdeu': 3636, 'perdi': 3637, 'perdida': 3638, 'perdidas': 3639, 'perdido': 3640, 'perdidos': 3641, 'perdoe': 3642, 'perdÃ£o': 3643, 'perfect\\testÃ¡': 3644, 'perfect\\tisso': 3645, 'perfect\\tisto': 3646, 'perfect\\tperfeito': 3647, 'perfect\\tÃ©': 3648, 'perfeito': 3649, 'pergunta': 3650, 'perguntar': 3651, 'perguntarei': 3652, 'pergunte': 3653, 'perguntei': 3654, 'perguntou': 3655, 'perguntÃ¡lo': 3656, 'perna': 3657, 'pernas': 3658, 'perth': 3659, 'perth\\testou': 3660, 'perto': 3661, 'pesado': 3662, 'pesca': 3663, 'pessoal': 3664, 'pessoas': 3665, 'phoned': 3666, 'phoned\\teu': 3667, 'phoned\\tquem': 3668, 'phoned\\ttom': 3669, 'phony\\tque': 3670, 'piada': 3671, 'piadas': 3672, 'piano': 3673, 'piano\\teu': 3674, 'pick': 3675, 'picky\\tsou': 3676, 'picky\\ttom': 3677, 'piedosa': 3678, 'piedoso': 3679, 'pig\\ttom': 3680, 'pilot\\tsou': 3681, 'piloto': 3682, 'pior': 3683, 'piscou': 3684, 'pisquei': 3685, 'pity\\tpena': 3686, 'pity\\tque': 3687, 'pity\\tÃ©': 3688, 'pizza': 3689, 'pizza\\teu': 3690, 'planos': 3691, 'plans\\teu': 3692, 'plant\\tÃ©': 3693, 'planta': 3694, 'plantas': 3695, 'plants': 3696, 'play': 3697, 'play\\tjoguemos': 3698, 'play\\tvamos': 3699, 'please': 3700, 'please\\tcarne': 3701, 'please\\tchÃ¡': 3702, 'please\\tdiga': 3703, 'please\\tfalem': 3704, 'please\\tpeixe': 3705, 'po': 3706, 'pobre': 3707, 'pobres': 3708, 'pode': 3709, 'podeis': 3710, 'podem': 3711, 'podemos': 3712, 'poderia': 3713, 'podes': 3714, 'poet\\tele': 3715, 'poet\\tsou': 3716, 'poeta': 3717, 'point': 3718, 'poison\\tÃ©': 3719, 'policiais': 3720, 'polite\\to': 3721, 'polite\\ttom': 3722, 'pontuais': 3723, 'pontual': 3724, 'poor\\tnÃ£o': 3725, 'poor\\tsomos': 3726, 'poor\\tsou': 3727, 'popular': 3728, 'popular\\teu': 3729, 'por': 3730, 'porco': 3731, 'porqu': 3732, 'porquÃª': 3733, 'porta': 3734, 'posso': 3735, 'possui': 3736, 'possÃ\\xadvel': 3737, 'pouco': 3738, 'pra': 3739, 'pray': 3740, 'pray\\to': 3741, 'prayed\\teu': 3742, 'prayed\\ttom': 3743, 'precipitado': 3744, 'precisa': 3745, 'precisam': 3746, 'precisamos': 3747, 'precisava': 3748, 'preciso': 3749, 'pregnant\\teu': 3750, 'preguiÃ§osas': 3751, 'preguiÃ§oso': 3752, 'preguiÃ§osos': 3753, 'preocupada': 3754, 'preocupe': 3755, 'prepaid\\ttom': 3756, 'preparada': 3757, 'preparadas': 3758, 'preparado': 3759, 'preparados': 3760, 'preparamos': 3761, 'prepared\\testeja': 3762, 'prepared\\testejam': 3763, 'prepared\\testou': 3764, 'preparou': 3765, 'presa': 3766, 'presente': 3767, 'preso': 3768, 'pressÃ¡gio': 3769, 'prestÃ\\xadgio': 3770, 'pretty\\tÃ©': 3771, 'primavera': 3772, 'primeira': 3773, 'primeiro': 3774, 'pro\\tsou': 3775, 'pro\\ttom': 3776, 'problem\\tnÃ£o': 3777, 'problem\\tproblema': 3778, 'problem\\tsem': 3779, 'problema': 3780, 'process': 3781, 'processar': 3782, 'processou': 3783, 'profissional': 3784, 'profundidade': 3785, 'prometemos': 3786, 'prometeram': 3787, 'prometeu': 3788, 'prometo': 3789, 'promise\\teu': 3790, 'promised\\tnÃ³s': 3791, 'promised\\tprometemos': 3792, 'promised\\tprometi': 3793, 'promised\\ttom': 3794, 'promised\\tvocÃª': 3795, 'promised\\tvocÃªs': 3796, 'pront': 3797, 'pronta': 3798, 'pronto': 3799, 'prontos': 3800, 'proof\\teu': 3801, 'prova': 3802, 'provar': 3803, 'prudent\\teu': 3804, 'prudent\\tsou': 3805, 'prudente': 3806, 'prÃ³prio': 3807, 'prÃ³xima': 3808, 'prÃ³ximo': 3809, 'pude': 3810, 'pular': 3811, 'pule': 3812, 'pulei': 3813, 'pulou': 3814, 'punctual\\tseja': 3815, 'punctual\\tsejam': 3816, 'punctual\\tsou': 3817, 'purist\\tsou': 3818, 'purista': 3819, 'push': 3820, 'push\\tnÃ£o': 3821, 'pushy\\ttom': 3822, 'put': 3823, 'puzzled\\testou': 3824, 'puzzled\\teu': 3825, 'pÃ¡lido': 3826, 'pÃ¡scoa': 3827, 'pÃ¡ssaro': 3828, 'pÃ¡ssaros': 3829, 'pÃ¢nico': 3830, 'pÃ£o': 3831, 'pÃ©': 3832, 'pÃ©s': 3833, 'quadrado': 3834, 'quadril': 3835, 'qualquer': 3836, 'quando': 3837, 'quarenta': 3838, 'quase': 3839, 'que': 3840, 'quebrado': 3841, 'quebrou': 3842, 'quebrouo': 3843, 'queima': 3844, 'queimar': 3845, 'quem': 3846, 'quente': 3847, 'quer': 3848, 'querem': 3849, 'queremos': 3850, 'quero': 3851, 'questions\\talguma': 3852, 'quick\\tvem': 3853, 'quick\\tvenha': 3854, 'quickly\\teu': 3855, 'quickly\\tvem': 3856, 'quickly\\tvenha': 3857, 'quickly\\tvenham': 3858, 'quiet': 3859, 'quiet\\tfica': 3860, 'quiet\\tfique': 3861, 'quiet\\ttudo': 3862, 'quieta': 3863, 'quietly\\tsaia': 3864, 'quieto': 3865, 'quiser': 3866, 'quit\\teu': 3867, 'quit\\tfomos': 3868, 'quit\\tnÃ³s': 3869, 'quit\\tquem': 3870, 'quit\\ttom': 3871, 'quit\\tvamos': 3872, 'quites': 3873, 'quÃª': 3874, 'racional': 3875, 'rain\\tpode': 3876, 'rain\\ttalvez': 3877, 'rained\\testava': 3878, 'rainy\\to': 3879, 'raised': 3880, 'raiva': 3881, 'ran': 3882, 'ran\\tele': 3883, 'ran\\teu': 3884, 'ran\\to': 3885, 'ran\\tquem': 3886, 'ran\\ttom': 3887, 'ranzinza': 3888, 'raramente': 3889, 'rarely': 3890, 'rash\\tnÃ£o': 3891, 'rational\\teu': 3892, 'ratos': 3893, 'rats\\teu': 3894, 'razoÃ¡vel': 3895, 'razÃ£o': 3896, 'read': 3897, 'read\\tele': 3898, 'read\\teu': 3899, 'read\\tleio': 3900, 'reading\\tcontinue': 3901, 'reading\\tele': 3902, 'reading\\tpare': 3903, 'ready\\testou': 3904, 'ready\\testÃ¡': 3905, 'ready\\tnos': 3906, 'ready\\tnÃ³s': 3907, 'ready\\ttom': 3908, 'real': 3909, 'real\\tacorda': 3910, 'real\\tcai': 3911, 'real\\tisso': 3912, 'real\\tÃ©': 3913, 'realista': 3914, 'realistic\\tseja': 3915, 'really\\tmesmo': 3916, 'really\\tsÃ©rio': 3917, 'really\\tÃ©': 3918, 'realmente': 3919, 'reasonable\\tseja': 3920, 'recess': 3921, 'recinto': 3922, 'recordo': 3923, 'recovered\\teu': 3924, 'recreio': 3925, 'recuou': 3926, 'recuperei': 3927, 'recusaram': 3928, 'recusou': 3929, 'red\\tisso': 3930, 'red\\to': 3931, 'red\\tÃ©': 3932, 'redor': 3933, 'reforÃ§o': 3934, 'reforÃ§os': 3935, 'refused\\teles': 3936, 'refused\\ttom': 3937, 'regras': 3938, 'regret': 3939, 'rei': 3940, 'relax\\tapenas': 3941, 'relax\\tei': 3942, 'relax\\tpor': 3943, 'relax\\trelaxa': 3944, 'relax\\trelaxe': 3945, 'relax\\ttente': 3946, 'relaxa': 3947, 'relaxada': 3948, 'relaxado': 3949, 'relaxar': 3950, 'relaxe': 3951, 'relaxed\\testou': 3952, 'relaxed\\ttom': 3953, 'relaxem': 3954, 'relaxou': 3955, 'release': 3956, 'reliable\\teu': 3957, 'relÃ³gio': 3958, 'remember': 3959, 'remember\\teu': 3960, 'remember\\tlembramos': 3961, 'remember\\tlembro': 3962, 'remember\\tme': 3963, 'remember\\tnÃ³s': 3964, 'rendo': 3965, 'renunciei': 3966, 'renunciou': 3967, 'replace': 3968, 'reply\\to': 3969, 'repugnante': 3970, 'reservado': 3971, 'reserved\\teu': 3972, 'resign\\teu': 3973, 'resigned\\tele': 3974, 'resigned\\teu': 3975, 'resigned\\tquem': 3976, 'resigned\\ttom': 3977, 'resmungou': 3978, 'resmunguei': 3979, 'respeito': 3980, 'respond\\tnÃ£o': 3981, 'responda': 3982, 'respondam': 3983, 'respondas': 3984, 'responde': 3985, 'respondeu': 3986, 'rest\\tdeixe': 3987, 'rest\\tdescansa': 3988, 'rest\\tdescanse': 3989, 'rest\\tdescansem': 3990, 'rest\\ttente': 3991, 'resting\\testou': 3992, 'retired\\testou': 3993, 'retired\\teu': 3994, 'retired\\tsou': 3995, 'retired\\ttom': 3996, 'rever': 3997, 'review\\tvamos': 3998, 'revisar': 3999, 'rezei': 4000, 'rezou': 4001, 'ri': 4002, 'ria': 4003, 'riam': 4004, 'rias': 4005, 'rica': 4006, 'rice\\tele': 4007, 'rice\\teu': 4008, 'rice\\tgosto': 4009, 'rich\\tele': 4010, 'rich\\teu': 4011, 'rich\\tnÃ£o': 4012, 'rich\\tnÃ³s': 4013, 'rich\\ttom': 4014, 'rich\\tvocÃª': 4015, 'rico': 4016, 'ricos': 4017, 'ridÃ\\xadculo': 4018, 'right\\tdobre': 4019, 'right\\tele': 4020, 'right\\testamos': 4021, 'right\\teu': 4022, 'right\\texatamente': 4023, 'right\\tnÃ³s': 4024, 'right\\tquem': 4025, 'right\\ttenho': 4026, 'right\\ttom': 4027, 'right\\tvire': 4028, 'right\\tvocÃª': 4029, 'rimos': 4030, 'rindo': 4031, 'riram': 4032, 'risada': 4033, 'risadinha': 4034, 'risadinhas': 4035, 'risk': 4036, 'riu': 4037, 'riuse': 4038, 'robbed\\teu': 4039, 'rock': 4040, 'rock\\teu': 4041, 'rocks\\ttom': 4042, 'roma': 4043, 'romantic\\tque': 4044, 'rome\\testou': 4045, 'rome\\teu': 4046, 'romÃ¢ntico': 4047, 'ronca': 4048, 'ronco': 4049, 'room\\ttemos': 4050, 'rope\\tsegure': 4051, 'rope\\tsegurem': 4052, 'rosa': 4053, 'rose\\testou': 4054, 'rose\\tvejo': 4055, 'rosquinha': 4056, 'rosto': 4057, 'roubado': 4058, 'roubo': 4059, 'roubou': 4060, 'rude': 4061, 'rude\\tnÃ£o': 4062, 'rude\\ttom': 4063, 'ruim': 4064, 'ruins': 4065, 'rules\\teu': 4066, 'rumor\\tÃ©': 4067, 'run': 4068, 'run\\tcorra': 4069, 'run\\tcorram': 4070, 'run\\tcorre': 4071, 'run\\teu': 4072, 'run\\tnÃ£o': 4073, 'run\\tvocÃª': 4074, 'runner\\teu': 4075, 'running\\tcontinue': 4076, 'runs': 4077, 'runs\\tela': 4078, 'runs\\tele': 4079, 'rush': 4080, 'ruthless\\tseja': 4081, 'ruÃ\\xaddo': 4082, 'rÃ¡pi': 4083, 'rÃ¡pida': 4084, 'rÃ¡pidas': 4085, 'rÃ¡pido': 4086, 'rÃ¡pidos': 4087, 'sabe': 4088, 'sabem': 4089, 'sabemos': 4090, 'saber': 4091, 'saberÃ£o': 4092, 'sabia': 4093, 'sabÃ\\xadamos': 4094, 'sad\\tagora': 4095, 'sad\\testou': 4096, 'sad\\teu': 4097, 'sad\\tfoi': 4098, 'sad\\tnÃ£o': 4099, 'sad\\tnÃ³s': 4100, 'sad\\to': 4101, 'sad\\ttom': 4102, 'sad\\tvocÃª': 4103, 'sad\\tvocÃªs': 4104, 'sad\\tÃ©': 4105, 'safe': 4106, 'safe\\teles': 4107, 'safe\\testamos': 4108, 'safe\\teu': 4109, 'safe\\tnÃ³s': 4110, 'safe\\to': 4111, 'safe\\ttom': 4112, 'safe\\ttu': 4113, 'safe\\tvocÃª': 4114, 'safe\\tÃ©': 4115, 'safely\\tdirija': 4116, 'sai': 4117, 'saia': 4118, 'said': 4119, 'sair': 4120, 'saiu': 4121, 'salvei': 4122, 'salvos': 4123, 'salvou': 4124, 'sand\\teu': 4125, 'sangrando': 4126, 'sangrava': 4127, 'sangue': 4128, 'sapatos': 4129, 'sat': 4130, 'saudade': 4131, 'saudades': 4132, 'saudÃ¡vel': 4133, 'save': 4134, 'saved': 4135, 'saved\\testamos': 4136, 'saved\\tnÃ³s': 4137, 'saw': 4138, 'say': 4139, 'say\\tfaÃ§a': 4140, 'say\\tfaÃ§am': 4141, 'say\\tnÃ£o': 4142, 'says\\tfaz': 4143, 'says\\tfaÃ§a': 4144, 'says\\tfaÃ§am': 4145, 'saÃ\\xad': 4146, 'saÃ\\xadmos': 4147, 'saÃ\\xadram': 4148, 'saÃºde': 4149, 'scare': 4150, 'scared': 4151, 'scared\\testou': 4152, 'scared\\teu': 4153, 'scared\\tfiquei': 4154, 'scares': 4155, 'scary\\ttom': 4156, 'scary\\ttu': 4157, 'scary\\tvocÃª': 4158, 'school\\tcomo': 4159, 'school\\tvai': 4160, 'school\\tvÃ¡': 4161, 'scream\\tnÃ£o': 4162, 'scream\\tnÃ³s': 4163, 'screamed\\teu': 4164, 'screamed\\tgritei': 4165, 'screamed\\ttom': 4166, 'se': 4167, 'seat\\tsentate': 4168, 'seat\\tsentemse': 4169, 'seat\\tsentese': 4170, 'sec\\tsÃ³': 4171, 'secret\\tÃ©': 4172, 'security\\tchame': 4173, 'security\\tchamem': 4174, 'sede': 4175, 'sedento': 4176, 'see': 4177, 'see\\tdeixame': 4178, 'see\\tdeixeme': 4179, 'see\\tdeixemme': 4180, 'see\\tdÃ¡': 4181, 'see\\testou': 4182, 'see\\teu': 4183, 'see\\tnÃ£o': 4184, 'see\\tnÃ³s': 4185, 'see\\tvejamos': 4186, 'see\\tvenha': 4187, 'see\\tveremos': 4188, 'see\\tvocÃª': 4189, 'seems': 4190, 'seen': 4191, 'seen\\tela': 4192, 'segredo': 4193, 'seguiremos': 4194, 'segunda': 4195, 'segundafeira': 4196, 'segundinho': 4197, 'seguranÃ§a': 4198, 'seguro': 4199, 'seguros': 4200, 'sei': 4201, 'seize': 4202, 'seja': 4203, 'sejas': 4204, 'selfish\\teu': 4205, 'sell': 4206, 'sells': 4207, 'sem': 4208, 'sempre': 4209, 'senhor': 4210, 'senhora': 4211, 'senhoras': 4212, 'senhores': 4213, 'sensible\\ttem': 4214, 'sensible\\ttende': 4215, 'sensible\\ttenha': 4216, 'sensible\\ttenham': 4217, 'senso': 4218, 'senta': 4219, 'sentamos': 4220, 'sentar': 4221, 'sente': 4222, 'sentei': 4223, 'sentemse': 4224, 'sentese': 4225, 'senti': 4226, 'sentila': 4227, 'sentilo': 4228, 'sentimos': 4229, 'sentindo': 4230, 'sentiram': 4231, 'sentiu': 4232, 'ser': 4233, 'seria': 4234, 'serious\\testou': 4235, 'serious\\teu': 4236, 'serious\\tfale': 4237, 'serious\\tfica': 4238, 'serious\\tfique': 4239, 'serious\\tfiquem': 4240, 'serious\\tisto': 4241, 'seriously\\tmesmo': 4242, 'seriously\\tsÃ©rio': 4243, 'seriously\\tÃ©': 4244, 'serve': 4245, 'servir': 4246, 'set': 4247, 'sete': 4248, 'settle': 4249, 'seu': 4250, 'sexist\\tele': 4251, 'sexista': 4252, 'sexy': 4253, 'sexy\\ttom': 4254, 'shall': 4255, 'shame\\tque': 4256, 'shame\\tÃ©': 4257, 'share\\ta': 4258, 'share\\tcompartilharemos': 4259, 'share\\tnÃ³s': 4260, 'shaved\\tele': 4261, 'shaved\\tse': 4262, 'shaved\\ttom': 4263, 'she': 4264, 'she\\tcadÃª': 4265, 'she\\tonde': 4266, 'she\\tquem': 4267, 'shes': 4268, 'ship\\tabandonar': 4269, 'ship\\tÃ©': 4270, 'shock\\tque': 4271, 'shocked\\teu': 4272, 'shoes\\teu': 4273, 'shoes\\tvendo': 4274, 'shoot\\teu': 4275, 'shoot\\tirei': 4276, 'shoot\\tnÃ£o': 4277, 'shoot\\tnÃ³s': 4278, 'shoot\\tvou': 4279, 'short\\ta': 4280, 'short\\to': 4281, 'shot': 4282, 'shot\\tbelo': 4283, 'should': 4284, 'shout\\tnÃ£o': 4285, 'shouted\\teu': 4286, 'shoved': 4287, 'show': 4288, 'shrugged\\ttom': 4289, 'shut': 4290, 'shy\\teu': 4291, 'shy\\tnÃ£o': 4292, 'shy\\tsomos': 4293, 'shy\\tsou': 4294, 'shy\\ttom': 4295, 'shy\\ttu': 4296, 'shy\\tvocÃª': 4297, 'shy\\tvocÃªs': 4298, 'si': 4299, 'sick\\tele': 4300, 'sick\\testou': 4301, 'sick\\teu': 4302, 'sick\\tnÃ³s': 4303, 'sick\\to': 4304, 'sick\\tsintome': 4305, 'sick\\tsou': 4306, 'sick\\ttom': 4307, 'sick\\tvocÃª': 4308, 'sick\\tvocÃªs': 4309, 'siga': 4310, 'sigam': 4311, 'sighed\\ttom': 4312, 'sign': 4313, 'sign\\tvocÃª': 4314, 'silly\\teu': 4315, 'silly\\tme': 4316, 'sim': 4317, 'similar\\tÃ©': 4318, 'simpatizo': 4319, 'simpÃ¡tica': 4320, 'simpÃ¡tico': 4321, 'sincere\\teu': 4322, 'sincero': 4323, 'sing': 4324, 'sing\\tcantarei': 4325, 'sing\\tcantaremos': 4326, 'sing\\tcante': 4327, 'sing\\tcantem': 4328, 'sing\\tdeixeme': 4329, 'sing\\teu': 4330, 'sing\\tnÃ£o': 4331, 'sing\\to': 4332, 'sing\\tos': 4333, 'sing\\tpor': 4334, 'sing\\tposso': 4335, 'sing\\tpÃ¡ssaros': 4336, 'sing\\ttom': 4337, 'sing\\tvocÃªs': 4338, 'singing\\tcontinue': 4339, 'singing\\tpare': 4340, 'single\\testou': 4341, 'single\\tsou': 4342, 'sinto': 4343, 'sit': 4344, 'sit\\tpoderÃ\\xadamos': 4345, 'sit\\tpor': 4346, 'skate\\tvocÃª': 4347, 'ski\\tesquio': 4348, 'ski\\teu': 4349, 'skiing\\ttom': 4350, 'skinny\\tele': 4351, 'sleep': 4352, 'sleep\\tdeixeme': 4353, 'sleep\\teu': 4354, 'sleep\\tnÃ£o': 4355, 'sleep\\tos': 4356, 'sleep\\ttente': 4357, 'sleep\\tvÃ¡': 4358, 'sleeping\\testou': 4359, 'sleepy\\testou': 4360, 'sleepy\\teu': 4361, 'sleepy\\tnÃ³s': 4362, 'slept': 4363, 'slept\\tninguÃ©m': 4364, 'slipped\\tela': 4365, 'slipped\\teu': 4366, 'slipped\\ttom': 4367, 'slow': 4368, 'slow\\ttom': 4369, 'slower\\tfale': 4370, 'slowly\\tande': 4371, 'slowly\\tcoma': 4372, 'slowly\\tdirija': 4373, 'slowly\\ttrabalhe': 4374, 'small\\teu': 4375, 'small\\ttom': 4376, 'small\\ttu': 4377, 'small\\tvocÃª': 4378, 'smart\\tele': 4379, 'smart\\teu': 4380, 'smart\\tnÃ³s': 4381, 'smart\\ttom': 4382, 'smart\\tvocÃª': 4383, 'smell': 4384, 'smile\\tnÃ£o': 4385, 'smile\\tpor': 4386, 'smile\\tsorria': 4387, 'smile\\tsorriam': 4388, 'smiled\\tdei': 4389, 'smiled\\tela': 4390, 'smiled\\tele': 4391, 'smiled\\teles': 4392, 'smiled\\teu': 4393, 'smiled\\ttom': 4394, 'smiling\\tcontinue': 4395, 'smoke\\teu': 4396, 'smoke\\tnÃ£o': 4397, 'smoke\\tposso': 4398, 'smoke\\tvocÃª': 4399, 'smoke\\tvocÃªs': 4400, 'smokes\\ttom': 4401, 'smoking\\tpara': 4402, 'smoking\\tpare': 4403, 'smoking\\tparem': 4404, 'sneaky\\tsou': 4405, 'sneezed\\teu': 4406, 'sneezed\\ttom': 4407, 'snore\\teu': 4408, 'snore\\tvocÃª': 4409, 'snores\\ttom': 4410, 'snow\\teu': 4411, 'snow\\tpode': 4412, 'snowed\\tnevou': 4413, 'snowing\\testÃ¡': 4414, 'so': 4415, 'so\\tacho': 4416, 'so\\tacredito': 4417, 'so\\tassim': 4418, 'so\\tespero': 4419, 'so\\teu': 4420, 'so\\tnÃ£o': 4421, 'so\\tnÃ³s': 4422, 'so\\trealmente': 4423, 'so\\tsuponho': 4424, 'soaked\\teu': 4425, 'sober\\testou': 4426, 'sobrevivemos': 4427, 'sobreviveram': 4428, 'sobreviveu': 4429, 'sobrevivi': 4430, 'socapa': 4431, 'sociable\\teu': 4432, 'sociÃ¡vel': 4433, 'sofreu': 4434, 'sois': 4435, 'solitÃ¡rio': 4436, 'solta': 4437, 'solteiro': 4438, 'solto': 4439, 'some': 4440, 'some\\texperimenta': 4441, 'some\\texperimente': 4442, 'some\\texperimentem': 4443, 'some\\tpega': 4444, 'some\\tpegue': 4445, 'some\\tpeguem': 4446, 'some\\ttemos': 4447, 'some\\tvenha': 4448, 'someone': 4449, 'somos': 4450, 'son\\teu': 4451, 'son\\ttenho': 4452, 'sonhando': 4453, 'sonho': 4454, 'sonhos': 4455, 'sono': 4456, 'soon\\tatÃ©': 4457, 'soon\\tvenha': 4458, 'sopa': 4459, 'sorrateiro': 4460, 'sorri': 4461, 'sorria': 4462, 'sorrias': 4463, 'sorrindo': 4464, 'sorriram': 4465, 'sorriso': 4466, 'sorriu': 4467, 'sorry\\tdesculpa': 4468, 'sorry\\tdesculpe': 4469, 'sorry\\teu': 4470, 'sorry\\tlamento': 4471, 'sorry\\tme': 4472, 'sorry\\tnos': 4473, 'sorry\\tsinto': 4474, 'sorte': 4475, 'sortuda': 4476, 'sortudo': 4477, 'sou': 4478, 'soup\\teu': 4479, 'soup\\tquero': 4480, 'sozinha': 4481, 'sozinho': 4482, 'sozinhos': 4483, 'space\\teu': 4484, 'space\\tpreciso': 4485, 'spanish\\tele': 4486, 'speak': 4487, 'speak\\tdeixeme': 4488, 'speak\\tnÃ£o': 4489, 'speak\\ttom': 4490, 'speaking\\teu': 4491, 'special\\teu': 4492, 'specific\\tseja': 4493, 'split\\tvamos': 4494, 'spoke\\tele': 4495, 'spoke\\tquem': 4496, 'spoke\\ttom': 4497, 'spring\\tÃ©': 4498, 'spy\\to': 4499, 'square\\teu': 4500, 'stand': 4501, 'stand\\tnÃ³s': 4502, 'standing\\teu': 4503, 'star\\testou': 4504, 'star\\tvejo': 4505, 'stare\\tnÃ£o': 4506, 'staring\\tpare': 4507, 'start': 4508, 'start\\tcomecemos': 4509, 'start\\tcomeÃ§arei': 4510, 'start\\tiremos': 4511, 'start\\tnÃ³s': 4512, 'start\\tpodemos': 4513, 'start\\tquem': 4514, 'start\\tvamos': 4515, 'start\\tvocÃª': 4516, 'started\\tcomece': 4517, 'started\\tcomecem': 4518, 'started\\tcomeÃ§a': 4519, 'starve\\tnÃ³s': 4520, 'starved\\testou': 4521, 'starving\\testou': 4522, 'starving\\teu': 4523, 'stay': 4524, 'stay\\ta': 4525, 'stay\\tas': 4526, 'stay\\teu': 4527, 'stay\\tficarei': 4528, 'stay\\tfique': 4529, 'stay\\to': 4530, 'stay\\tos': 4531, 'stay\\tpode': 4532, 'stay\\tpodeis': 4533, 'stay\\tpodem': 4534, 'stay\\tpodes': 4535, 'stay\\tpor': 4536, 'stay\\ttom': 4537, 'stay\\ttu': 4538, 'stay\\tvocÃª': 4539, 'stay\\tvocÃªs': 4540, 'stay\\tvÃ³s': 4541, 'stayed\\teu': 4542, 'stayed\\to': 4543, 'stayed\\tquem': 4544, 'stayed\\ttom': 4545, 'staying\\teu': 4546, 'steal\\tisso': 4547, 'steal\\tÃ©': 4548, 'step': 4549, 'still': 4550, 'still\\tacalmese': 4551, 'still\\tfique': 4552, 'still\\tnÃ£o': 4553, 'still\\ttudo': 4554, 'stink\\tvocÃª': 4555, 'stinks\\tisso': 4556, 'stinks\\tisto': 4557, 'stole': 4558, 'stoned\\ttom': 4559, 'stood': 4560, 'stood\\to': 4561, 'stood\\tquem': 4562, 'stop': 4563, 'stop\\teu': 4564, 'stop\\tnÃ£o': 4565, 'stop\\tpare': 4566, 'stop\\tparem': 4567, 'stop\\tpor': 4568, 'stopped\\teles': 4569, 'stopped\\teu': 4570, 'stopped\\to': 4571, 'stopped\\tquem': 4572, 'stopped\\ttom': 4573, 'strange\\tque': 4574, 'strange\\tÃ©': 4575, 'strong\\tele': 4576, 'strong\\teu': 4577, 'strong\\tnÃ³s': 4578, 'strong\\tsou': 4579, 'strong\\ttom': 4580, 'stuck\\temperrei': 4581, 'stuck\\testou': 4582, 'stuck\\testÃ¡': 4583, 'stuck\\teu': 4584, 'studied\\teu': 4585, 'study': 4586, 'study\\teu': 4587, 'studying\\tele': 4588, 'studying\\testou': 4589, 'studying\\teu': 4590, 'stuffed\\testou': 4591, 'stunned\\teu': 4592, 'stupid\\tela': 4593, 'stupid\\tele': 4594, 'stupid\\teu': 4595, 'stupid\\tisso': 4596, 'stutters\\ttom': 4597, 'sua': 4598, 'suaram': 4599, 'suava': 4600, 'subornei': 4601, 'succeeded\\tnÃ³s': 4602, 'sue': 4603, 'sue\\tnÃ³s': 4604, 'sued': 4605, 'sugar\\teu': 4606, 'sugar\\tpreciso': 4607, 'suits': 4608, 'sujo': 4609, 'suppose': 4610, 'surdo': 4611, 'sure\\tagora': 4612, 'sure\\teu': 4613, 'sure\\ttem': 4614, 'sure\\ttenho': 4615, 'sure\\tvocÃª': 4616, 'sure\\tvocÃªs': 4617, 'surf\\teu': 4618, 'surfar': 4619, 'surrender\\teu': 4620, 'surrender\\tme': 4621, 'surrender\\tnÃ³s': 4622, 'survived\\teu': 4623, 'survived\\tnÃ³s': 4624, 'survived\\tquem': 4625, 'survived\\tsobrevivemos': 4626, 'survived\\ttom': 4627, 'survived\\tvocÃª': 4628, 'survived\\tvocÃªs': 4629, 'sushi': 4630, 'sushi\\teu': 4631, 'suspirou': 4632, 'suÃ\\xadÃ§o': 4633, 'swam\\telas': 4634, 'swam\\teles': 4635, 'swam\\to': 4636, 'swam\\tquem': 4637, 'swam\\ttom': 4638, 'sweated\\teles': 4639, 'sweated\\ttom': 4640, 'sweet': 4641, 'sweet\\testÃ¡': 4642, 'sweet\\tisso': 4643, 'sweet\\tÃ©': 4644, 'swim\\ta': 4645, 'swim\\tcachorros': 4646, 'swim\\tdeixeme': 4647, 'swim\\tela': 4648, 'swim\\tele': 4649, 'swim\\teu': 4650, 'swim\\tnademos': 4651, 'swim\\tnÃ³s': 4652, 'swim\\to': 4653, 'swim\\tpode': 4654, 'swim\\tpodem': 4655, 'swim\\tpodes': 4656, 'swim\\tsabe': 4657, 'swim\\tsabemos': 4658, 'swim\\tsabes': 4659, 'swim\\ttom': 4660, 'swim\\tvamos': 4661, 'swim\\tvocÃª': 4662, 'swim\\tvocÃªs': 4663, 'swims\\ttom': 4664, 'swiss\\tele': 4665, 'swore\\ttom': 4666, 'sympathize\\teu': 4667, 'sympathize\\tme': 4668, 'sÃ¡bio': 4669, 'sÃ£o': 4670, 'sÃ©ria': 4671, 'sÃ©rias': 4672, 'sÃ©rio': 4673, 'sÃ©rios': 4674, 'sÃ³': 4675, 'sÃ³brio': 4676, 't': 4677, 'tailandÃªs': 4678, 'take': 4679, 'talk': 4680, 'talk\\tnÃ£o': 4681, 'talk\\tpodemos': 4682, 'talk\\ttom': 4683, 'talked\\tconversamos': 4684, 'talked\\teu': 4685, 'talked\\tnÃ³s': 4686, 'talked\\tquem': 4687, 'talked\\ttom': 4688, 'talking\\tcontinue': 4689, 'talking\\tpara': 4690, 'talking\\tpare': 4691, 'talks': 4692, 'talks\\to': 4693, 'tall\\telas': 4694, 'tall\\tele': 4695, 'tall\\teles': 4696, 'tall\\teu': 4697, 'tall\\tmary': 4698, 'tall\\tnÃ£o': 4699, 'tall\\to': 4700, 'tall\\tsou': 4701, 'tall\\tvocÃª': 4702, 'taller\\teu': 4703, 'talvez': 4704, 'tambÃ©m': 4705, 'tanque': 4706, 'tarde': 4707, 'taste': 4708, 'taxes\\teu': 4709, 'taxes\\tpago': 4710, 'te': 4711, 'tea': 4712, 'tea\\tbeba': 4713, 'tea\\tele': 4714, 'tea\\teu': 4715, 'teach': 4716, 'teaches\\ttom': 4717, 'telefonei': 4718, 'telefonou': 4719, 'televisÃ£o': 4720, 'tell': 4721, 'tell\\teu': 4722, 'tem': 4723, 'temo': 4724, 'tempo': 4725, 'ten\\tconte': 4726, 'tenho': 4727, 'tenta': 4728, 'tentamos': 4729, 'tentando': 4730, 'tentar': 4731, 'tentaram': 4732, 'tentarei': 4733, 'tentaremos': 4734, 'tentava': 4735, 'tente': 4736, 'tentei': 4737, 'tento': 4738, 'tentou': 4739, 'terminamos': 4740, 'terminei': 4741, 'terminou': 4742, 'terrific\\texcelente': 4743, 'terrific\\tfantÃ¡stico': 4744, 'terrific\\tgenial': 4745, 'terrific\\tmagnÃ\\xadfico': 4746, 'terrÃ\\xadvel': 4747, 'testa': 4748, 'teu': 4749, 'thai\\teu': 4750, 'thai\\tsou': 4751, 'thank': 4752, 'thanks': 4753, 'thanks\\tmuito': 4754, 'thanks\\tobrigada': 4755, 'thanks\\tobrigado': 4756, 'that': 4757, 'that\\tapenas': 4758, 'that\\tcurto': 4759, 'that\\tele': 4760, 'that\\tescuto': 4761, 'that\\testou': 4762, 'that\\teu': 4763, 'that\\tfarei': 4764, 'that\\tfique': 4765, 'that\\tgosto': 4766, 'that\\tignore': 4767, 'that\\tignoreo': 4768, 'that\\tnÃ£o': 4769, 'that\\tnÃ³s': 4770, 'that\\to': 4771, 'that\\todeio': 4772, 'that\\tolhe': 4773, 'that\\tolhem': 4774, 'that\\tpare': 4775, 'that\\tparem': 4776, 'that\\tquem': 4777, 'that\\tquero': 4778, 'that\\tsabemos': 4779, 'that\\tsabÃ\\xadamos': 4780, 'that\\ttom': 4781, 'that\\tverifica': 4782, 'that\\tverifique': 4783, 'that\\tverifiquem': 4784, 'thatll': 4785, 'thats': 4786, 'the': 4787, 'them': 4788, 'them\\ta': 4789, 'them\\tdetenhamnos': 4790, 'them\\tdetenhanos': 4791, 'them\\tele': 4792, 'them\\teu': 4793, 'them\\texamineos': 4794, 'them\\tfique': 4795, 'them\\tignoreas': 4796, 'them\\tignoremnas': 4797, 'them\\tignoremnos': 4798, 'them\\tignoreos': 4799, 'them\\tnÃ³s': 4800, 'them\\tpergunte': 4801, 'them\\ttom': 4802, 'then': 4803, 'there': 4804, 'there\\tcoloqueo': 4805, 'there\\testÃ¡': 4806, 'there\\teu': 4807, 'there\\tfica': 4808, 'there\\tfique': 4809, 'there\\tnÃ³s': 4810, 'there\\to': 4811, 'there\\tolha': 4812, 'there\\tpare': 4813, 'there\\tquem': 4814, 'there\\tsenta': 4815, 'there\\tsentese': 4816, 'there\\ttom': 4817, 'there\\ttu': 4818, 'there\\tvocÃª': 4819, 'there\\tvocÃªs': 4820, 'theres': 4821, 'these\\teu': 4822, 'these\\tgosto': 4823, 'these\\tleve': 4824, 'these\\tquero': 4825, 'they': 4826, 'they\\tquem': 4827, 'theyll': 4828, 'theyre': 4829, 'theyve': 4830, 'thief\\tele': 4831, 'thin\\teu': 4832, 'thin\\tsou': 4833, 'thin\\ttom': 4834, 'think': 4835, 'thinking\\testou': 4836, 'thirsty\\testou': 4837, 'thirsty\\ttenho': 4838, 'thirty\\teu': 4839, 'thirty\\to': 4840, 'thirty\\ttenho': 4841, 'this': 4842, 'this\\tassine': 4843, 'this\\tassinem': 4844, 'this\\tcarregue': 4845, 'this\\tcheire': 4846, 'this\\tconserta': 4847, 'this\\tconsigo': 4848, 'this\\teu': 4849, 'this\\texperimenta': 4850, 'this\\texperimente': 4851, 'this\\texperimentem': 4852, 'this\\tfique': 4853, 'this\\tleia': 4854, 'this\\tleiam': 4855, 'this\\tnÃ£o': 4856, 'this\\tnÃ³s': 4857, 'this\\tobservem': 4858, 'this\\tolhe': 4859, 'this\\tpegue': 4860, 'this\\tposso': 4861, 'this\\tprove': 4862, 'this\\tprovem': 4863, 'this\\tquero': 4864, 'this\\tsabÃ\\xadamos': 4865, 'this\\tsegure': 4866, 'this\\tsinta': 4867, 'this\\ttermine': 4868, 'this\\ttom': 4869, 'this\\tuse': 4870, 'this\\tveja': 4871, 'this\\tverifica': 4872, 'this\\tverifique': 4873, 'this\\tverifiquem': 4874, 'this\\tvocÃª': 4875, 'this\\tvou': 4876, 'thisll': 4877, 'threw': 4878, 'thrilling\\tque': 4879, 'through\\tacabei': 4880, 'through\\tterminei': 4881, 'ti': 4882, 'ticklish\\teu': 4883, 'tidy\\testou': 4884, 'tidy\\teu': 4885, 'tight\\taguenta': 4886, 'tight\\tdurma': 4887, 'tight\\tperaÃ\\xad': 4888, 'tight\\tsegurem': 4889, 'tight\\tsegurese': 4890, 'time': 4891, 'time\\tdÃªlhe': 4892, 'time\\testÃ¡': 4893, 'time\\teu': 4894, 'time\\tnÃ³s': 4895, 'time\\tprecisamos': 4896, 'time\\tquero': 4897, 'time\\ttemos': 4898, 'time\\ttom': 4899, 'timid\\teu': 4900, 'timid\\tnÃ³s': 4901, 'timid\\ttom': 4902, 'timid\\ttu': 4903, 'timid\\tvocÃª': 4904, 'timing\\tna': 4905, 'tinha': 4906, 'tinta': 4907, 'tipo': 4908, 'tired\\tele': 4909, 'tired\\testamos': 4910, 'tired\\testou': 4911, 'tired\\teu': 4912, 'tired\\tme': 4913, 'tired\\tnÃ³s': 4914, 'tired\\to': 4915, 'tired\\tsentiase': 4916, 'tired\\tsintome': 4917, 'tired\\ttom': 4918, 'tired\\ttu': 4919, 'tired\\tvocÃª': 4920, 'tired\\tvocÃªs': 4921, 'tiro': 4922, 'tive': 4923, 'to': 4924, 'toc': 4925, 'tocante': 4926, 'tocar': 4927, 'toco': 4928, 'today\\teu': 4929, 'today\\tnÃ³s': 4930, 'todo': 4931, 'todos': 4932, 'tola': 4933, 'told': 4934, 'tolerant\\tsede': 4935, 'tolerant\\tseja': 4936, 'tolerant\\tsejam': 4937, 'tolerant\\tsÃª': 4938, 'tolerante': 4939, 'tolerantes': 4940, 'tolo': 4941, 'tom': 4942, 'tom\\tabrace': 4943, 'tom\\tabracem': 4944, 'tom\\tacerta': 4945, 'tom\\tacorda': 4946, 'tom\\tacorde': 4947, 'tom\\tacredito': 4948, 'tom\\tadorÃ¡vamos': 4949, 'tom\\tajuda': 4950, 'tom\\tajude': 4951, 'tom\\tajudem': 4952, 'tom\\tajudeme': 4953, 'tom\\tajudenos': 4954, 'tom\\talerte': 4955, 'tom\\tali': 4956, 'tom\\tamÃ¡vamos': 4957, 'tom\\tapressese': 4958, 'tom\\taquele': 4959, 'tom\\tavise': 4960, 'tom\\tavisem': 4961, 'tom\\tbeije': 4962, 'tom\\tcomo': 4963, 'tom\\tconfie': 4964, 'tom\\tconheÃ§o': 4965, 'tom\\tconta': 4966, 'tom\\tcontacte': 4967, 'tom\\tcontactem': 4968, 'tom\\tconte': 4969, 'tom\\tcontem': 4970, 'tom\\tcontratei': 4971, 'tom\\tconversa': 4972, 'tom\\tdeixe': 4973, 'tom\\tdescreva': 4974, 'tom\\tdescrevam': 4975, 'tom\\tele': 4976, 'tom\\tencontre': 4977, 'tom\\tencontrem': 4978, 'tom\\tera': 4979, 'tom\\tescreva': 4980, 'tom\\tesqueÃ§a': 4981, 'tom\\tesqueÃ§am': 4982, 'tom\\tesse': 4983, 'tom\\teste': 4984, 'tom\\testou': 4985, 'tom\\teu': 4986, 'tom\\tfale': 4987, 'tom\\tfaÃ§a': 4988, 'tom\\tfaÃ§am': 4989, 'tom\\tfaÃ§o': 4990, 'tom\\tfoi': 4991, 'tom\\tgostamos': 4992, 'tom\\tgostei': 4993, 'tom\\tgosto': 4994, 'tom\\tignora': 4995, 'tom\\tignore': 4996, 'tom\\timpeÃ§a': 4997, 'tom\\timpeÃ§am': 4998, 'tom\\tinterrompa': 4999, 'tom\\tinterrompam': 5000, 'tom\\tirei': 5001, 'tom\\tleva': 5002, 'tom\\tleve': 5003, 'tom\\tligue': 5004, 'tom\\tnÃ£o': 5005, 'tom\\tnÃ³s': 5006, 'tom\\todeio': 5007, 'tom\\toi': 5008, 'tom\\tolha': 5009, 'tom\\tolÃ¡': 5010, 'tom\\tpare': 5011, 'tom\\tparem': 5012, 'tom\\tpegue': 5013, 'tom\\tpeguem': 5014, 'tom\\tperdoem': 5015, 'tom\\tpergunta': 5016, 'tom\\tperguntarei': 5017, 'tom\\tpergunte': 5018, 'tom\\tperguntem': 5019, 'tom\\tpeÃ§a': 5020, 'tom\\tprecisamos': 5021, 'tom\\tprossiga': 5022, 'tom\\tquem': 5023, 'tom\\tqueremos': 5024, 'tom\\tquero': 5025, 'tom\\tresponda': 5026, 'tom\\tsalve': 5027, 'tom\\tsentimos': 5028, 'tom\\tsiga': 5029, 'tom\\tsigam': 5030, 'tom\\tsinto': 5031, 'tom\\tsolte': 5032, 'tom\\tsou': 5033, 'tom\\tum': 5034, 'tom\\tvai': 5035, 'tom\\tvi': 5036, 'tom\\tvocÃª': 5037, 'tom\\tvote': 5038, 'tom\\tvou': 5039, 'tom\\tvÃ¡': 5040, 'tom\\tÃ©': 5041, 'tomei': 5042, 'tomll': 5043, 'tomo': 5044, 'tomorrow\\tvenha': 5045, 'tomorrow\\tvenham': 5046, 'toms': 5047, 'toms\\tera': 5048, 'toms\\tisso': 5049, 'toms\\tÃ©': 5050, 'tomÃ¡s': 5051, 'tonto': 5052, 'tontos': 5053, 'too': 5054, 'too\\teu': 5055, 'too\\thomens': 5056, 'too\\tposso': 5057, 'took': 5058, 'tossi': 5059, 'tossiu': 5060, 'touching\\tque': 5061, 'town\\tdeixe': 5062, 'town\\tdeixem': 5063, 'toys\\tcomprem': 5064, 'toys\\tele': 5065, 'trabalh': 5066, 'trabalha': 5067, 'trabalhando': 5068, 'trabalhar': 5069, 'trabalho': 5070, 'trabalhou': 5071, 'tragic\\tque': 5072, 'trancado': 5073, 'tranquilos': 5074, 'trap\\tcoloquei': 5075, 'trap\\teu': 5076, 'trap\\tÃ©': 5077, 'trapacearam': 5078, 'trapaceei': 5079, 'trapaceia': 5080, 'trapaceou': 5081, 'trapped\\testou': 5082, 'trapped\\teu': 5083, 'trash\\tisso': 5084, 'trash\\tÃ©': 5085, 'trick\\tÃ©': 5086, 'tried\\tela': 5087, 'tried\\teles': 5088, 'tried\\teu': 5089, 'tried\\tnÃ³s': 5090, 'tried\\to': 5091, 'tried\\ttentei': 5092, 'tried\\tvocÃª': 5093, 'tried\\tvocÃªs': 5094, 'tries': 5095, 'tries\\tele': 5096, 'tries\\to': 5097, 'trinta': 5098, 'tripped\\teu': 5099, 'tripped\\ttom': 5100, 'tripped\\ttropecei': 5101, 'trips\\tamo': 5102, 'triste': 5103, 'tristes': 5104, 'tropecei': 5105, 'true\\tisso': 5106, 'true\\tisto': 5107, 'true\\tÃ©': 5108, 'truque': 5109, 'trust': 5110, 'try': 5111, 'try\\ta': 5112, 'try\\tagora': 5113, 'try\\tdeixeme': 5114, 'try\\tdevemos': 5115, 'try\\teu': 5116, 'try\\tnÃ³s': 5117, 'try\\ttentamos': 5118, 'try\\ttentarei': 5119, 'try\\ttentaremos': 5120, 'try\\ttento': 5121, 'try\\ttom': 5122, 'try\\tvou': 5123, 'trying\\tcontinue': 5124, 'trying\\testou': 5125, 'trying\\teu': 5126, 'trying\\tpare': 5127, 'trying\\tparem': 5128, 'trÃ¡gico': 5129, 'trÃ¡s': 5130, 'tu': 5131, 'tua': 5132, 'tuas': 5133, 'tudo': 5134, 'turn': 5135, 'turned\\tele': 5136, 'tusso': 5137, 'tv': 5138, 'tv\\tisso': 5139, 'tv\\tisto': 5140, 'tv\\to': 5141, 'tv\\ttom': 5142, 'tv\\tÃ©': 5143, 'tvs': 5144, 'twice\\teu': 5145, 'twins\\teu': 5146, 'twins\\tnÃ³s': 5147, 'twins\\tsomos': 5148, 'two\\teu': 5149, 'type\\tele': 5150, 'tÃ¡xi': 5151, 'tÃ£o': 5152, 'tÃªm': 5153, 'tÃ\\xadmida': 5154, 'tÃ\\xadmidas': 5155, 'tÃ\\xadmido': 5156, 'tÃ\\xadmidos': 5157, 'ufo\\teu': 5158, 'ugly\\teles': 5159, 'ugly\\testou': 5160, 'ugly\\teu': 5161, 'ugly\\tnÃ£o': 5162, 'ugly\\tsou': 5163, 'ugly\\ttom': 5164, 'ugly\\ttu': 5165, 'ugly\\tvocÃª': 5166, 'ugly\\tvocÃªs': 5167, 'um': 5168, 'uma': 5169, 'unbelievable\\tinacreditÃ¡vel': 5170, 'unbelievable\\tincrÃ\\xadvel': 5171, 'understand\\tcompreendo': 5172, 'understand\\tentendo': 5173, 'understand\\teu': 5174, 'understood\\tentendi': 5175, 'understood\\teu': 5176, 'unfair\\tisso': 5177, 'unsure\\to': 5178, 'untidy\\tsou': 5179, 'unusual\\tÃ©': 5180, 'up': 5181, 'up\\tabra': 5182, 'up\\tacorda': 5183, 'up\\tacorde': 5184, 'up\\tacordem': 5185, 'up\\tacordeos': 5186, 'up\\tapanheo': 5187, 'up\\tapressate': 5188, 'up\\tapressese': 5189, 'up\\tcala': 5190, 'up\\tcalate': 5191, 'up\\tcale': 5192, 'up\\tcalem': 5193, 'up\\tcalemse': 5194, 'up\\tcalese': 5195, 'up\\tcontinua': 5196, 'up\\tcontinue': 5197, 'up\\tde': 5198, 'up\\tdepressa': 5199, 'up\\te': 5200, 'up\\tela': 5201, 'up\\telas': 5202, 'up\\tele': 5203, 'up\\teles': 5204, 'up\\tencha': 5205, 'up\\tescute': 5206, 'up\\tescutem': 5207, 'up\\tespere': 5208, 'up\\tesperem': 5209, 'up\\testou': 5210, 'up\\teu': 5211, 'up\\tfala': 5212, 'up\\tfale': 5213, 'up\\tfalem': 5214, 'up\\tlavate': 5215, 'up\\tlave': 5216, 'up\\tlavemse': 5217, 'up\\tlavese': 5218, 'up\\tlevantate': 5219, 'up\\tlevantemse': 5220, 'up\\tlevanteo': 5221, 'up\\tlevantese': 5222, 'up\\tlimpe': 5223, 'up\\tnÃ£o': 5224, 'up\\tnÃ³s': 5225, 'up\\to': 5226, 'up\\tpegue': 5227, 'up\\tpegueo': 5228, 'up\\trelaxa': 5229, 'up\\trelaxe': 5230, 'up\\tse': 5231, 'up\\ttom': 5232, 'up\\ttudo': 5233, 'up\\tvou': 5234, 'up\\tÃ¢nimo': 5235, 'upset\\testamos': 5236, 'upset\\testou': 5237, 'upset\\teu': 5238, 'upset\\tnÃ³s': 5239, 'upset\\to': 5240, 'upset\\ttom': 5241, 'upset\\ttu': 5242, 'upset\\tvocÃª': 5243, 'upstairs\\tsobe': 5244, 'upstairs\\tsuba': 5245, 'upstairs\\tsubam': 5246, 'urgent\\tÃ©': 5247, 'urgente': 5248, 'us': 5249, 'us\\tajude': 5250, 'us\\tajudemnos': 5251, 'us\\tajudenos': 5252, 'us\\tcante': 5253, 'us\\tconverse': 5254, 'us\\tdeixenos': 5255, 'us\\tela': 5256, 'us\\telas': 5257, 'us\\tele': 5258, 'us\\teles': 5259, 'us\\tfale': 5260, 'us\\tfique': 5261, 'us\\tfiquem': 5262, 'us\\tjuntese': 5263, 'us\\tligue': 5264, 'us\\tnos': 5265, 'us\\tnÃ£o': 5266, 'us\\tolhe': 5267, 'us\\tperdoenos': 5268, 'us\\tsigamnos': 5269, 'us\\tsiganos': 5270, 'us\\ttom': 5271, 'us\\tvenha': 5272, 'us\\tvenham': 5273, 'usa': 5274, 'usar': 5275, 'use': 5276, 'used': 5277, 'useless\\teu': 5278, 'uso': 5279, 'usou': 5280, 'vai': 5281, 'valente': 5282, 'vamos': 5283, 'vangloriouse': 5284, 'vanished\\tquem': 5285, 'vanished\\ttom': 5286, 'vazio': 5287, 've': 5288, 'vegan\\teu': 5289, 'vegana': 5290, 'vegano': 5291, 'veio': 5292, 'velha': 5293, 'velhas': 5294, 'velho': 5295, 'velhos': 5296, 'vem': 5297, 'vemos': 5298, 'vencemos': 5299, 'vencendo': 5300, 'vencer': 5301, 'venceram': 5302, 'venceremos': 5303, 'venceu': 5304, 'venci': 5305, 'vende': 5306, 'vendo': 5307, 'veneno': 5308, 'venha': 5309, 'venham': 5310, 'venhas': 5311, 'venho': 5312, 'ventando': 5313, 'venÃ§a': 5314, 'ver': 5315, 'verda': 5316, 'verdade': 5317, 'verei': 5318, 'veremos': 5319, 'vergonha': 5320, 'verificaremos': 5321, 'vermelha': 5322, 'vermelho': 5323, 'very': 5324, 'verÃ¡': 5325, 'verÃ\\xaddico': 5326, 'vet\\ttom': 5327, 'veterano': 5328, 'vez': 5329, 'vezes': 5330, 'vi': 5331, 'viagem': 5332, 'viagens': 5333, 'vice\\tÃ©': 5334, 'viciado': 5335, 'vida': 5336, 'vim': 5337, 'vindo': 5338, 'vinho': 5339, 'vir': 5340, 'viram': 5341, 'virei': 5342, 'virÃ¡': 5343, 'vista': 5344, 'viu': 5345, 'viva': 5346, 'viver': 5347, 'vivo': 5348, 'vo': 5349, 'voa': 5350, 'voador': 5351, 'voam': 5352, 'voar': 5353, 'vocÃ': 5354, 'vocÃª': 5355, 'vocÃªs': 5356, 'volta': 5357, 'voltar': 5358, 'voltaram': 5359, 'voltarei': 5360, 'volte': 5361, 'volto': 5362, 'vomited\\to': 5363, 'vomitei': 5364, 'vomitou': 5365, 'vos': 5366, 'votaram': 5367, 'vote': 5368, 'vote\\teu': 5369, 'vote\\ttom': 5370, 'vote\\tvocÃª': 5371, 'voted\\telas': 5372, 'voted\\teles': 5373, 'voted\\ttom': 5374, 'voted\\tvotaram': 5375, 'votei': 5376, 'votou': 5377, 'vou': 5378, 'vÃ¡': 5379, 'vÃ£o': 5380, 'vÃª': 5381, 'vÃ\\xadcio': 5382, 'vÃ³s': 5383, 'wait': 5384, 'wait\\tesperarei': 5385, 'wait\\tesperaremos': 5386, 'wait\\tespere': 5387, 'wait\\tesperem': 5388, 'wait\\teu': 5389, 'wait\\tirei': 5390, 'wait\\tnÃ£o': 5391, 'wait\\tnÃ³s': 5392, 'wait\\tpor': 5393, 'wait\\ttom': 5394, 'wait\\tvou': 5395, 'waited\\teles': 5396, 'waited\\tesperamos': 5397, 'waited\\tesperaram': 5398, 'waited\\teu': 5399, 'waited\\tnÃ³s': 5400, 'waited\\ttom': 5401, 'waiting\\testou': 5402, 'waiting\\teu': 5403, 'wake': 5404, 'walk': 5405, 'walk\\tandarei': 5406, 'walk\\tele': 5407, 'walk\\teu': 5408, 'walk\\to': 5409, 'walk\\tpoderÃ\\xadamos': 5410, 'walk\\ttom': 5411, 'walked\\tcaminharam': 5412, 'walked\\telas': 5413, 'walked\\teles': 5414, 'walked\\ttom': 5415, 'walking\\tcontinue': 5416, 'walking\\tcontinuem': 5417, 'walks': 5418, 'walks\\tela': 5419, 'walks\\ttom': 5420, 'want': 5421, 'wants': 5422, 'war': 5423, 'warm\\tmantenha': 5424, 'warn': 5425, 'wary\\ttom': 5426, 'was': 5427, 'wash': 5428, 'washed': 5429, 'wasnt': 5430, 'waste\\tque': 5431, 'waste\\tÃ©': 5432, 'watch': 5433, 'water\\teu': 5434, 'waved\\teu': 5435, 'waved\\ttom': 5436, 'way\\tde': 5437, 'way\\timpossÃ\\xadvel': 5438, 'way\\tsem': 5439, 'way\\tvem': 5440, 'we': 5441, 'we\\tonde': 5442, 'we\\tquem': 5443, 'weak\\teles': 5444, 'weak\\teu': 5445, 'weak\\tnÃ³s': 5446, 'weak\\ttom': 5447, 'weak\\ttu': 5448, 'weak\\tvocÃª': 5449, 'wealthy\\teu': 5450, 'weird\\tfoi': 5451, 'weird\\tisso': 5452, 'weird\\to': 5453, 'welcome': 5454, 'welcome\\tbemvinda': 5455, 'welcome\\tbemvindo': 5456, 'welcome\\tseja': 5457, 'well': 5458, 'well\\tele': 5459, 'well\\testou': 5460, 'well\\testÃ¡': 5461, 'well\\teu': 5462, 'well\\to': 5463, 'well\\ttudo': 5464, 'well\\tvocÃª': 5465, 'well\\tvocÃªs': 5466, 'went': 5467, 'went\\teu': 5468, 'wept\\tjesus': 5469, 'were': 5470, 'wet\\tele': 5471, 'wet\\testava': 5472, 'wet\\testou': 5473, 'wet\\testÃ¡': 5474, 'wet\\teu': 5475, 'wet\\ttom': 5476, 'weve': 5477, 'what': 5478, 'what\\te': 5479, 'whats': 5480, 'where': 5481, 'whiff\\tdÃª': 5482, 'while\\tespere': 5483, 'while\\tesperem': 5484, 'while\\tfique': 5485, 'whining\\tpare': 5486, 'whistled\\teu': 5487, 'whistled\\ttom': 5488, 'white\\tera': 5489, 'white\\tÃ©': 5490, 'who': 5491, 'who\\tquem': 5492, 'wholl': 5493, 'whos': 5494, 'whose': 5495, 'why': 5496, 'why\\tdiga': 5497, 'why\\teu': 5498, 'why\\tposso': 5499, 'why\\tsabemos': 5500, 'will': 5501, 'win\\teu': 5502, 'win\\tganhamos': 5503, 'win\\tnÃ³s': 5504, 'win\\to': 5505, 'win\\tquem': 5506, 'win\\ttom': 5507, 'win\\tvencemos': 5508, 'win\\tvenceremos': 5509, 'win\\tvou': 5510, 'windy\\testÃ¡': 5511, 'wine\\tele': 5512, 'wine\\teu': 5513, 'wine\\ttemos': 5514, 'wine\\ttraga': 5515, 'wine\\ttragam': 5516, 'winked\\teu': 5517, 'winked\\to': 5518, 'winning\\testou': 5519, 'wins\\ttodo': 5520, 'wise\\teu': 5521, 'wise\\ttom': 5522, 'wise\\ttu': 5523, 'wise\\tvocÃª': 5524, 'wish': 5525, 'wish\\tfaÃ§a': 5526, 'with': 5527, 'without': 5528, 'witty\\ttom': 5529, 'woke': 5530, 'wolf\\tisso': 5531, 'wolf\\tÃ©': 5532, 'woman\\teu': 5533, 'woman\\tque': 5534, 'woman\\tsou': 5535, 'women\\teu': 5536, 'won\\tadivinha': 5537, 'won\\telas': 5538, 'won\\teles': 5539, 'won\\teu': 5540, 'won\\tganhei': 5541, 'won\\tnÃ³s': 5542, 'won\\tquem': 5543, 'won\\ttom': 5544, 'won\\tvencemos': 5545, 'won\\tvocÃª': 5546, 'won\\tvocÃªs': 5547, 'wonderful\\tmagnÃ\\xadfico': 5548, 'wonderful\\tmaravilhoso': 5549, 'wonderful\\tque': 5550, 'wont': 5551, 'wood': 5552, 'work': 5553, 'work\\testou': 5554, 'work\\teu': 5555, 'work\\tfuncionou': 5556, 'work\\tisso': 5557, 'work\\tpoderia': 5558, 'work\\tvai': 5559, 'work\\tvou': 5560, 'work\\tÃ©': 5561, 'worked\\taquilo': 5562, 'worked\\tfuncionou': 5563, 'worked\\ttom': 5564, 'working\\tcontinue': 5565, 'working\\tcontinuem': 5566, 'working\\testou': 5567, 'working\\testÃ¡': 5568, 'working\\teu': 5569, 'works\\tfunciona': 5570, 'works\\tisto': 5571, 'works\\ttom': 5572, 'worn': 5573, 'worried\\testou': 5574, 'worry\\tnÃ£o': 5575, 'worse\\tfoi': 5576, 'wow\\tnossa': 5577, 'wow\\tuau': 5578, 'wow\\twow': 5579, 'write': 5580, 'writer\\teu': 5581, 'writing\\tcontinue': 5582, 'writing\\tcontinuem': 5583, 'wrong\\tela': 5584, 'wrong\\testou': 5585, 'wrong\\testÃ¡': 5586, 'wrong\\teu': 5587, 'wrong\\to': 5588, 'wrong\\tquem': 5589, 'wrong\\tvocÃª': 5590, 'wrote': 5591, 'xadrez': 5592, 'yawned\\teu': 5593, 'yawned\\ttom': 5594, 'years': 5595, 'yelled\\teles': 5596, 'yelled\\tquem': 5597, 'yelled\\ttom': 5598, 'yelling\\tpara': 5599, 'yelling\\tpare': 5600, 'yen\\tcusta': 5601, 'yes\\tdisse': 5602, 'yes\\teu': 5603, 'yes\\to': 5604, 'yes\\ttom': 5605, 'you': 5606, 'you\\ta': 5607, 'you\\tacredito': 5608, 'you\\tamavaa': 5609, 'you\\tamavao': 5610, 'you\\tamavate': 5611, 'you\\tameia': 5612, 'you\\tameio': 5613, 'you\\tameite': 5614, 'you\\tamo': 5615, 'you\\tamoa': 5616, 'you\\tamoo': 5617, 'you\\tamote': 5618, 'you\\tamovos': 5619, 'you\\tatÃ©': 5620, 'you\\tcomo': 5621, 'you\\tconfio': 5622, 'you\\tdeus': 5623, 'you\\te': 5624, 'you\\tele': 5625, 'you\\teles': 5626, 'you\\tencontrei': 5627, 'you\\testamos': 5628, 'you\\testou': 5629, 'you\\teu': 5630, 'you\\tgosto': 5631, 'you\\tirei': 5632, 'you\\tisso': 5633, 'you\\tnos': 5634, 'you\\tnÃ£o': 5635, 'you\\tnÃ³s': 5636, 'you\\tobrigada': 5637, 'you\\tobrigado': 5638, 'you\\touvimos': 5639, 'you\\tposso': 5640, 'you\\tpreciso': 5641, 'you\\tque': 5642, 'you\\tquem': 5643, 'you\\tquerote': 5644, 'you\\tsaudades': 5645, 'you\\tsaÃºde': 5646, 'you\\tsenti': 5647, 'you\\tsentimos': 5648, 'you\\tsinto': 5649, 'you\\tte': 5650, 'you\\ttenho': 5651, 'you\\ttom': 5652, 'you\\tvocÃª': 5653, 'you\\tvocÃªs': 5654, 'you\\tvou': 5655, 'you\\tÃ©': 5656, 'youll': 5657, 'young\\tela': 5658, 'young\\tele': 5659, 'young\\teu': 5660, 'young\\tnÃ³s': 5661, 'young\\to': 5662, 'young\\tsomos': 5663, 'young\\tsou': 5664, 'young\\tvocÃª': 5665, 'young\\tvocÃªs': 5666, 'your': 5667, 'youre': 5668, 'yours\\teu': 5669, 'yours\\tisso': 5670, 'yours\\tsou': 5671, 'yours\\tÃ©': 5672, 'yourself\\tseja': 5673, 'yourself\\tservete': 5674, 'yourself\\tsirvamse': 5675, 'yourself\\tsirvase': 5676, 'youve': 5677, 'zombou': 5678, 'Ã': 5679, 'Ã¡gua': 5680, 'Ã¡rabes': 5681, 'Ã©': 5682, 'Ã©s': 5683, 'Ã³pera': 5684, 'Ã³vni': 5685, 'Ã´nibus': 5686, 'Ãºltimo': 5687}\n"
     ]
    }
   ],
   "source": [
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes[0]).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing text data for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2         # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n",
    "\n",
    "# Print 10 pairs\n",
    "print_examples(sentences, next_chars, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd,0 )\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models\n",
    "## Sequentiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "LSTM (LSTM)                  (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the class\n",
    "model = Sequential(name=\"sequential_model\")\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"modelclass_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM\n",
    "# Define the input layer\n",
    "main_input =Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sample texts: (7, 4)\n",
      "Now the texts have fixed length: 60. Let's see the first one: \n",
      "[ 23 106 488   5   7 113 273   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Import relevant classes/functions\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sheldon_quotes[0])\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(sheldon_quotes[0])\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60, padding='post')\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "# Method '.evaluate()' shows the loss and accuracy\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploding gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse= model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LSTM layer\n",
    "from keras.layers.recurrent import LSTM\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Load pre-trained weights\n",
    "model.load_weights('lstm_stack_model_weights.h5')\n",
    "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove pre-trained vectors\n",
    "glove_matrix = load_glove('glove_200d.zip')\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
    "                    embeddings_initializer= Constant(glove_matrix), \n",
    "                    input_length=sentence_len, trainable=False))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification revisited\n",
    "## Better sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add( Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "# Print the obtained loss and accuracy\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "# One-hot encode the indexes\n",
    "Y = to_categorical(numerical_ids)\n",
    "# Check the new shape of the variable\n",
    "print(Y.shape)\n",
    "# Print the first 5 rows\n",
    "print(Y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit tokenizer\n",
    "tokenizer =Tokenizer()\n",
    "tokenizer.fit_on_texts(news_dataset)\n",
    "# Prepare the data\n",
    "prep_data = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "prep_data = pad_sequences(prep_data, maxlen=200)\n",
    "# Prepare the labels\n",
    "prep_labels = to_categorical(news_dataset.target)\n",
    "# Print the shapes\n",
    "print(prep_data.shape)\n",
    "print(prep_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change text for numerical ids and pad\n",
    "X_novel = tokenizer.texts_to_sequences(news_novel.data)\n",
    "X_novel = pad_sequences(X_novel, maxlen=400)\n",
    "# One-hot encode the labels\n",
    "Y_novel = to_categorical(news_novel.target)\n",
    "# Load the model pre-trained weights\n",
    "model.load_weights('classify_news_weights.h5')\n",
    "# Evaluate the model on the new dataset\n",
    "loss, acc = model.evaluate(X_novel, Y_novel, batch_size=64)\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss:\\t{0}\\nAccuracy:\\t{1}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-Recall trade-off\n",
    "\n",
    "When working with classification tasks, the term Precision-Recall trade-off often appears. Where does it comes from?\n",
    "\n",
    "Usually, the class with higher probability (obtained by the .predict_proba() method) is chosen to assign the document to. But, what if the maximum probability is equal to 0.1? Should you consider that document to belong to this class with only 10% probability?\n",
    "\n",
    "The answer varies according to problem at hand. It is possible to add a minimum threshold to accept the classification, and by changing the threshold the values of precision and recall move in opposite directions.\n",
    "\n",
    "The variables y_true and the model model are loaded. Also, if the probability is lower than the threshold, the document will be assigned to DEFAULT_CLASS (chosen to be class 2).\n",
    "\n",
    "# Precision or Recall, that is the question\n",
    "\n",
    "You learned about a few performance metrics and maybe you are asking, when should I use precision and when should I use recall? Those two metrics are calculated for each class, and sometimes it is difficult to understand when to focus on one and when to focus on the other.\n",
    "\n",
    "Precision is a metric that measures how well the model is predicting some class, while recall measures how well a class is being classified. If precision is high for one class, you can trust your model when it predicts that class. When recall is high for a class, you can rest assured that that class is well understood by the model.\n",
    "\n",
    "Follow the instruction to see this comparison between precision and recall with an example. The functions precision_score() and recall_score() are loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the precision of the sentiment model\n",
    "prec_sentiment = precision_score(sentiment_y_true, sentiment_y_pred, average=None)\n",
    "print(prec_sentiment)\n",
    "# Compute the recall of the sentiment model\n",
    "rec_sentiment = recall_score(sentiment_y_true, sentiment_y_pred, average=None)\n",
    "print(rec_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict on new data\n",
    "predicted = model.predict(X_test)\n",
    "# Choose the class with higher probability \n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "# Compute and print the confusion matrix\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "# Create the performance report\n",
    "print(classification_report(y_true, y_pred, target_names=news_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models\n",
    "## Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectors\n",
    "sentences = []\n",
    "next_chars = []\n",
    "# Loop for every sentence\n",
    "for sentence in sheldon.split('\\n'):\n",
    "    # Get 20 previous chars and next char; then shift by step\n",
    "    for i in range(0, len(sentence) - chars_window, step):\n",
    "        sentences.append(sentence[i:i + chars_window])\n",
    "        next_chars.append(sentence[i + chars_window])\n",
    "# Define a Data Frame with the vectors\n",
    "df = pd.DataFrame({'sentence': sentences, 'next_char': next_chars})\n",
    "# Print the initial rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the variables with zeros\n",
    "numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)\n",
    "numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)\n",
    "\n",
    "# Loop for every sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "  # Loop for every character in sentence\n",
    "  for t, char in enumerate(sentence):\n",
    "    # Set position of the character to 1\n",
    "    numerical_sentences[i, t, char_to_index[char]] = 1\n",
    "    # Set next character to 1\n",
    "    numerical_next_chars[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Print the first position of each\n",
    "print(numerical_sentences[0], numerical_next_chars[0], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Sequential(name=\"LSTM model\")\n",
    "# Add two LSTM layers\n",
    "model.add(LSTM(64, input_shape=input_shape, dropout=0.15, recurrent_dropout=0.15, return_sequences=True, name=\"Input_layer\"))\n",
    "model.add(LSTM(64, dropout=0.15, recurrent_dropout=0.15, return_sequences=False, name=\"LSTM_hidden\"))\n",
    "# Add the output layer\n",
    "model.add(Dense(n_vocab, activation='softmax', name=\"Output_layer\"))\n",
    "# Compile and load weights\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.load_weights('model_weights.h5')\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum length of the sentences\n",
    "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
    "\n",
    "# Transform text to sequence of numerical indexes\n",
    "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(X, maxlen=pt_length, padding='post')\n",
    "\n",
    "# Print first sentence\n",
    "print(pt_sentences[0])\n",
    "\n",
    "# Print transformed sentence\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variable\n",
    "Y = transform_text_to_sequences(en_sentences,output_tokenizer)\n",
    "\n",
    "# Temporary list\n",
    "ylist = list()\n",
    "for sequence in Y:\n",
    "  \t# One-hot encode sentence and append to list\n",
    "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
    "\n",
    "# Update the variable\n",
    "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
    "\n",
    "# Print the raw sentence and its transformed version\n",
    "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabl : \n",
    "https://shravan-kuchkula.github.io/mutli-class-multi-label-pipeline/#testing-the-improved-pipeline-on-a-holdout-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asiig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
