{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.2.0\n"
     ]
    }
   ],
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Summary\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "# tensor-Keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "import scipy.io\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit, watch Rafa and Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I'm a GSP fan, i just hate Nick ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             tweets\n",
       "0  positive   Gas by my house hit $3.39!!!! I'm going to Ch...\n",
       "1  negative   Theo Walcott is still shit, watch Rafa and Jo...\n",
       "2  negative   its not that I'm a GSP fan, i just hate Nick ..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data \n",
    "tweets_train= pd.read_table('C:/Users/rzouga/Downloads/Github/NLP/train_tweets.txt', '\\t',header=None)\n",
    "tweets_train = tweets_train[0].str.split(\",\", n = 1, expand = True) \n",
    "tweets_train.columns = [\"sentiment\", \"tweets\"]\n",
    "tweets_test =pd.read_table('C:/Users/rzouga/Downloads/Github/NLP/test_tweets.txt', '\\t',header=None)\n",
    "tweets_test = tweets_test[0].str.split(\",\", n = 1, expand = True) \n",
    "tweets_test.columns = [\"sentiment\", \"tweets\"]\n",
    "# Print the head of df\n",
    "tweets_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus, tomorrow is a ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y'all hear what Tony Romo dressed up as f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>Lunch from my new Lil spot ...THE COTTON BOWL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             tweets\n",
       "0   neutral   Won the match #getin . Plus, tomorrow is a ve...\n",
       "1   neutral   Did y'all hear what Tony Romo dressed up as f...\n",
       "2  positive   Lunch from my new Lil spot ...THE COTTON BOWL..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.sentiment.unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive, neutral and negative reviews:  neutral     3236\n",
      "positive    2413\n",
      "negative     939\n",
      "Name: sentiment, dtype: int64\n",
      "Proportion of positive,neutral and negative reviews:  neutral     0.491196\n",
      "positive    0.366272\n",
      "negative    0.142532\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find the number of positive and negative reviews\n",
    "print('Number of positive, neutral and negative reviews: ',tweets_train.sentiment.value_counts())\n",
    "\n",
    "# Find the proportion of positive and negative reviews\n",
    "print('Proportion of positive,neutral and negative reviews: ', tweets_train.sentiment.value_counts()/ len(tweets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus, tomorrow is a ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Did y'all hear what Tony Romo dressed up as f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>Lunch from my new Lil spot ...THE COTTON BOWL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>SNC Halloween Pr. Pumped. Let's work it for S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Manchester United will try to return to winni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#WEB YouTube improves upload process with opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gonna change my Tumblr theme. I hope I can fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I'm so jealous of everyone at the Justin Bieb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Jim Harbaugh, Alex Smith Drive Giants World S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Trending: Tim Tebow is now dating cave woman...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                             tweets\n",
       "0      neutral   Won the match #getin . Plus, tomorrow is a ve...\n",
       "1      neutral   Did y'all hear what Tony Romo dressed up as f...\n",
       "2     positive   Lunch from my new Lil spot ...THE COTTON BOWL...\n",
       "3     positive   SNC Halloween Pr. Pumped. Let's work it for S...\n",
       "4      neutral   Manchester United will try to return to winni...\n",
       "...        ...                                                ...\n",
       "1039   neutral   #WEB YouTube improves upload process with opt...\n",
       "1040  positive   Gonna change my Tumblr theme. I hope I can fi...\n",
       "1041   neutral   I'm so jealous of everyone at the Justin Bieb...\n",
       "1042   neutral   Jim Harbaugh, Alex Smith Drive Giants World S...\n",
       "1043   neutral   #Trending: Tim Tebow is now dating cave woman...\n",
       "\n",
       "[1042 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.dropna()\n",
    "tweets_train.drop_duplicates()\n",
    "tweets_test.dropna()\n",
    "tweets_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19ed9d88>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='sentiment',data=tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "# Spacy preporcessing \n",
    "#Stop words\n",
    "#importing stop words from English language.\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re \n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer_cleaner(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, '', sentence)\n",
    "    # Normalize text\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentence.lower())\n",
    "    # Remove urls\n",
    "    sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', sentence, flags=re.MULTILINE)\n",
    "    # Remove user @ references \n",
    "    sentence = re.sub(r'\\@\\w+','', sentence)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    sentence= re.sub(r'^RT[\\s]+', '', sentence)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    sentence= re.sub(r'#', '',sentence)\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    \n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return ' '.join(mytokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean our tweetes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train['tweets_cleaned'] = tweets_train.tweets.apply(spacy_tokenizer_cleaner)\n",
    "tweets_test['tweets_cleaned'] = tweets_test.tweets.apply(spacy_tokenizer_cleaner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweets</th>\n",
       "      <th>tweets_cleaned</th>\n",
       "      <th>n_char</th>\n",
       "      <th>n_words</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Ch...</td>\n",
       "      <td>gas house hit 3 39 m going chapel hill sat</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit, watch Rafa and Jo...</td>\n",
       "      <td>theo walcott shit watch rafa johnny deal saturday</td>\n",
       "      <td>77</td>\n",
       "      <td>17</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I'm a GSP fan, i just hate Nick ...</td>\n",
       "      <td>m gsp fan hate nick diaz t wait february</td>\n",
       "      <td>76</td>\n",
       "      <td>22</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             tweets  \\\n",
       "0  positive   Gas by my house hit $3.39!!!! I'm going to Ch...   \n",
       "1  negative   Theo Walcott is still shit, watch Rafa and Jo...   \n",
       "2  negative   its not that I'm a GSP fan, i just hate Nick ...   \n",
       "\n",
       "                                      tweets_cleaned  n_char  n_words  punct%  \n",
       "0         gas house hit 3 39 m going chapel hill sat      66       22    19.2  \n",
       "1  theo walcott shit watch rafa johnny deal saturday      77       17     3.2  \n",
       "2           m gsp fan hate nick diaz t wait february      76       22     8.3  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using spacy \n",
    "# Word tokenization\n",
    "from spacy.lang.en import English\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "tweets_train['n_char']=tweets_train.tweets.apply(lambda x:len(x))\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "# Tokenize each item in the review column\n",
    "# Create a new feature for the lengh of each review\n",
    "tweets_train['n_words'] =tweets_train.tweets.apply(lambda x: len(nlp(x)))\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "tweets_train['punct%'] = tweets_train['tweets'].apply(lambda x: count_punct(x))\n",
    "\n",
    "tweets_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning-Based Approaches\n",
    "# train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (6588,)\n",
      "y_test (1044,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X_train= tweets_train['tweets_cleaned']# the features we want to analyze\n",
    "X_test=tweets_test['tweets_cleaned']\n",
    "y_train=tweets_train['sentiment']\n",
    "y_test =tweets_test['sentiment']\n",
    "\n",
    "#### Create the encoder.\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(y_train)   # Assume for simplicity all features are categorical.\n",
    "\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"y_test\",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier\n",
    "# Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6588, 6364)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.39       193\n",
      "           1       0.61      0.66      0.63       466\n",
      "           2       0.59      0.63      0.61       385\n",
      "\n",
      "    accuracy                           0.59      1044\n",
      "   macro avg       0.56      0.54      0.54      1044\n",
      "weighted avg       0.58      0.59      0.58      1044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,ENGLISH_STOP_WORDS\n",
    "#len_train = [len(x) for x in X_train]\n",
    "#len_test = [len(x) for x in X_test]\n",
    "#dig_train = [sum(char.isnumeric() for char in x) for x in X_train]\n",
    "#dig_test = [sum(char.isnumeric() for char in x) for x in X_test]\n",
    "    \n",
    "# Not alpha numeric:\n",
    "#nan_train = X_train.str.count('\\W')\n",
    "#nan_test = X_test.str.count('\\W')\n",
    "# tokenizer = spacy_tokenizer\n",
    "vectorizer = CountVectorizer(min_df=2, max_df=1500).fit(X_train)\n",
    "X_train_cv =vectorizer.transform(X_train)\n",
    "X_test_cv =vectorizer.transform(X_test)\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "#X_train_cv = add_feature(X_train_cv, [len_train, dig_train, nan_train])\n",
    "#X_test_cv = add_feature(X_test_cv, [len_test, dig_test, nan_test])\n",
    "\n",
    "NB=MultinomialNB(alpha=0.1)\n",
    "NB.fit(X_train_cv, y_train)\n",
    "predicted1=NB.predict(X_test_cv)\n",
    "report = classification_report(y_test, predicted1)\n",
    "print(X_train_cv.shape)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy1: 0.5862068965517241\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy\n",
    "print(\" Accuracy1:\",metrics.accuracy_score(y_test, predicted1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.14      0.23       193\n",
      "           1       0.59      0.82      0.69       466\n",
      "           2       0.62      0.57      0.59       385\n",
      "\n",
      "    accuracy                           0.60      1044\n",
      "   macro avg       0.62      0.51      0.50      1044\n",
      "weighted avg       0.61      0.60      0.57      1044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "#len_train = [len(x) for x in X_train]\n",
    "#len_test = [len(x) for x in X_test]\n",
    "#dig_train = [sum(char.isnumeric() for char in x) for x in X_train]\n",
    "#dig_test = [sum(char.isnumeric() for char in x) for x in X_test]\n",
    "    \n",
    "# Not alpha numeric:\n",
    "#nan_train = X_train.str.count('\\W')\n",
    "#nan_test = X_test.str.count('\\W')\n",
    "tfidf = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=500).fit(X_train)\n",
    "X_train_tf = tfidf.transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "#X_train_tf = add_feature(X_train_cv, [len_train, dig_train, nan_train])\n",
    "#X_test_tf = add_feature(X_test_cv, [len_test, dig_test, nan_test])\n",
    "NB=MultinomialNB(alpha=0.1)\n",
    "NB.fit(X_train_tf, y_train)\n",
    "predicted2=NB.predict(X_test_tf)\n",
    "report = classification_report(y_test, predicted2)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy2: 0.6024904214559387\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy\n",
    "print(\" Accuracy2:\",metrics.accuracy_score(y_test, predicted2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.15      0.23       193\n",
      "           1       0.58      0.88      0.70       466\n",
      "           2       0.70      0.53      0.60       385\n",
      "\n",
      "    accuracy                           0.62      1044\n",
      "   macro avg       0.63      0.52      0.51      1044\n",
      "weighted avg       0.63      0.62      0.58      1044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.svm import SVC\n",
    "#len_train = [len(x) for x in X_train]\n",
    "#len_test = [len(x) for x in X_test]\n",
    "#dig_train = [sum(char.isnumeric() for char in x) for x in X_train]\n",
    "#dig_test = [sum(char.isnumeric() for char in x) for x in X_test]\n",
    "    \n",
    "# Not alpha numeric:\n",
    "#nan_train = X_train.str.count('\\W')\n",
    "#nan_test = X_test.str.count('\\W')\n",
    "tfidf = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=500).fit(X_train)\n",
    "X_train_tf = tfidf.transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "#X_train_tf = add_feature(X_train_cv, [len_train, dig_train, nan_train])\n",
    "#X_test_tf = add_feature(X_test_cv, [len_test, dig_test, nan_test])\n",
    "SVC=SVC()\n",
    "SVC.fit(X_train_tf, y_train)\n",
    "predicted3=SVC.predict(X_test_tf)\n",
    "report = classification_report(y_test, predicted3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy3: 0.6168582375478927\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy\n",
    "print(\" Accuracy3:\",metrics.accuracy_score(y_test, predicted3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV+Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "# Setup the pipeline\n",
    "tfidf_vector = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ).fit(X_train)\n",
    "classifier = SVC()\n",
    "steps = [('vectorizer', tfidf_vector),\n",
    "         ('SVM', classifier)]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "\n",
    "# Instantiate the GridSearchCV object: cv\n",
    "cv = GridSearchCV(pipeline,param_grid=parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning-Based Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "layers = tf.keras.layers\n",
    "# Import relevant classes/functions\n",
    "from tensorflow.keras.preprocessing.text import  Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(movies['review_cleaned']).split(' ')\n",
    "#all_words=[w for w in DOC ]\n",
    "# Get number of unique words\n",
    "vocab_size = len(set(all_words))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preporcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeded sentences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import  to_categorical\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "# Prepare the data\n",
    "embedded_prep_data = word_tokenizer.texts_to_sequences(X_train)\n",
    "#padded sentences \n",
    "from nltk.tokenize import word_tokenize\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(X_train, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "padded_prep_data= pad_sequences(embedded_prep_data, length_long_sentence, padding='post')\n",
    "# Prepare the labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "\n",
    "prep_labels = encoder.transform(y_train)\n",
    "\n",
    "\n",
    "#prep_labels = tf.keras.utils.to_categorical(movies.sentiment)\n",
    "print(padded_prep_data[0],prep_labels[0])\n",
    "# Print the shapes\n",
    "print(str(vocab_length))\n",
    "print(str(length_long_sentence))\n",
    "print(padded_prep_data.shape)\n",
    "print(prep_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custumise our embeded  matrix or import GLOVE that accelearate the training process \n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('C:/Users/rzouga/Desktop/cv-fr-en/CV+diplome/jobopr/glove.6B/glove.6B.50d.txt', encoding=\"utf8\")\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    " # embeddings_dictionary now contains words and corresponding GloVe embeddings for all the words.   \n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test the model \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_prep_data,prep_labels, test_size=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_length, 50))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "#embedding_matrix now contains pretrained word embeddings for the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid CNN-LSTM:\n",
    "#Convolutional neural networks have been found to work well with text data ,the CNN Model for feature extraction \n",
    "#LSTM Model for interpreting the features across time steps.\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Multiply, Add, LSTM, LeakyReLU\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "wordvec_dim=50\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_length, output_dim=wordvec_dim, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False))\n",
    "\n",
    "model.add(layers.Conv1D(100, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=0, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate( X_test, y_test, verbose=0)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "plot_history(history)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('loss: %f' % (loss*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
