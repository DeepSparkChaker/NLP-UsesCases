{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment analysis with generative modelling: application for stock price forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "1. Introduction\n",
    "    1. *Sentiment analysis for stock price forecasting*\n",
    "    2. *Differing approaches*\n",
    "    3. *A novel generative approach: SSESTM*\n",
    "2. Supervised Sentiment Extraction via Screening and Topic Modelling (SSESTM)\n",
    "    1. *Correlation screening for removing neutral words*\n",
    "    2. *Learning Positive/Negative Vocabulary Sets*\n",
    "    3. *Infering Sentiment from Unseen News Articles*\n",
    "3. Setup\n",
    "    1. *Data*\n",
    "    2. *Text Preprocessing*\n",
    "    3. *Model Implementation*\n",
    "    4. *Hyperparameter Selection and Learning Strategy*\n",
    "    5. *Performance Evaluation*\n",
    "4. Use-case: Forecasting Boeing Stock Prices\n",
    "    1. *Model Performance: All Data*\n",
    "    2. *Model Performance: Time Series Environment*\n",
    "5. Conclusion\n",
    "    1. *Summary*\n",
    "    2. *Limitations and potential improvements*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Sentiment analysis for stock price forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rio_tinto_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/goldman.png\">\n",
    "\n",
    "**Figure 1**: Originals from [The Guardian](https://www.theguardian.com/us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu, 2007\n",
    "Since the 2010s, there has been a rising interest within the financial services' industry for applications of natural language processing algorithms. One such application is **sentiment analysis for stock price forecasting**. The relationship between stock prices and news articles is not a novel subject, increasing compute power and the advent of Big Data has made it more easily accessible for financial institutions. Examples of newsworthy events with repercussions on stock market prices include:\n",
    "\n",
    "- Corporate scandals: e.g. Boeing 737 Max's crashes, Rio Tinto's accidental destruction of the Australian Aboriginal Juunkan Gorge, CD Projekt RED's botched launch of their flagship AAA video game on last-generation gaming consoles\n",
    "- Market downturn events: e.g. 2007-08 Financial/Real Estate Crisis, COVID-19 Pandemic\n",
    "\n",
    "In past economic litterature, stock market prices were assumed to incorporate all news/textual information available (Eugene Fama's market efficiency hypothesis from 1970). Therefore there were theoretically no gains from mining text information for exploiting arbitrage opportunies (e.g. after major news events, buying/selling a stock before the market reacts and adjusts the stock's market price).\n",
    "\n",
    "While it is a reasonable assumption to make, it is also worth considerating a relaxation of Fama's hypothesis: whereas in the long run, stocks fully incorporate all the available information in their prices, in the short term stock prices don't always adjust immediately to incoming information. There might even be news events that eventually foreshadow future stock price movements (e.g. solvency issues, disappointing sales). Ideally this should be revelatory of arbitrage opportunities that can be exploited for financial gain. \n",
    "\n",
    "As sentiment analysis is an application of text mining which aims to categorize textual data as either positive, negative or neutral (via a scoring system known as polarity), our goal here is to **classify textual data as either potentially indicative of future stock price increases or decreases** (eventually leading to investment/portfolio allocation decisions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Differing approaches for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For asset managers and data scientist looking to integrate sentiment analysis into the investment decision process, there are three main approaches: using pre-built vocabularies (with positive/negative sounding words) to output a polarity score on text, training supervised learning algorithms (from linear models to neural networks) or relying on commercial solutions offered by financial data vendors (e.g. Bloomberg, Reuters, RavenPack, IHS Markit). All three approaches have their inherent limitations: many pre-built vocabularies (such as Harvard-IV psychosocial dictionnary) weren't specifically designed with financial forecasting in mind and thus might not be relevant for our stock price prediction. The aforementioned commercial solutions are shrouded in opaqueness as data vendors are unlikely to reveal the entirety of the intricate details of their scoring system. **Supervised learning algorithms** represent a more viable solution as it solves the shortcomings of pre-built vocabularies and commercial solutions: first, sentiment vocabulary is learned directly from data and therefore suits the user's use-case. Second, there is no opaqueness as the user is in control of the scoring methodology (e.g. logistic regression, recurrent neural networks, supervised topic models).\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/ravenpack.png\">\n",
    "\n",
    "**Figure 2**: Screenshot illustration of RavenPack's Text Analytics Commercial Software obtained from a [blogpost on AWS](https://aws.amazon.com/solutions/case-studies/ravenpack-case-study/)\n",
    "\n",
    "*How do we select the most ideal scoring methodology for our prediction task? We will start by restrincting ourselves to Bag-of-word (BoW) representations (mainly that they ignore word context only whether a word is actually present in a document). There are two competing approaches: discriminative and generative modeling. In **discriminative models**, we are only interested in modeling (using our stock prediction use-case) stock returns with the respect to our text inputs $\\mathbb{P}(Y|X)$. Logistic regression, which makes no assumptions on the behaviour of our prediction variable, answers this description: it's straightforward, implemented in a wide range of software librairies and has the added bonus of being interpretable (we will know exactly which words contribute positively/negatively to stock price mouvements). Conversely, **generative models** look to model the joint probability distribution between stock returns and text inputs. Through Bayes formula, this allows us to model the full relationship of text inputs with respect to the prediction variable $\\mathbb{P}(X|Y)$, i.e. which words lead to positive/negative returns $\\mathbb{P}(X|Y=0)$ and $\\mathbb{P}(X|Y=1)$.*\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/discriminative_vs_generative.png\">\n",
    "\n",
    "**Figure 3**: Original from [Tu, 2007 on Semantic Scholar](https://www.semanticscholar.org/paper/Learning-Generative-Models-via-Discriminative-Tu/23b80dc704e25cf52b5a14935002fc083ce9c317).\n",
    "\n",
    "*Which approach outperforms the other is a matter of debate, although generative models tend to be less commonly used in stock price forecasting literature. This stems from the limited availability of generative algorithms, compared to discriminative models (which span from linear classifiers to neural networks). The most well known generative model in NLP literature when dealing with bag-of-word representations is topic models, dimensionality reduction algorithms that are mostly unsupervised and used for document clustering (thus unlabelled text data). There are a few extensions that allow for supervised topic models (e.g. Labelled Latent Dirichlet Allocation), which will be consider in our use-case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. A novel generative approach: SESTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While it is not often clear whether generative models - in spite of being more complex - outperform discriminative models,* Zheng Tracy Ke, Dacheng Xiu and Bryan Kelly - respectively professors at Harvard University, University of Chicago and Yale University (the latter also working as a Quantitative Researcher for AQR Capital Management) - devised a new supervised generative model for sentiment analysis: **Supervised Sentiment Extraction via Screening and Topic Modelling** (SSESTM), essentially a 3-stage methodology for:\n",
    "\n",
    "1. Initially screening words likely to portend stock price increases (and conversely decreases)\n",
    "2. Then rely on a supervised topic model to learn 2 seperate sets of sentiment vocabularies (or topics): one that augurs positive returns and another that foreshadows negative returns.\n",
    "3. This generative model is then used to score newly unseen documents. From those scored news articles, they are able to generate investment recommendations: buy stocks with positive news scores, sell stocks with negative scores.\n",
    "\n",
    "Stock returns are notoriously difficult to predict due to weak signal-to-noise properties, but the authors do hold that their algorithm provides sound investment recommendations, enough for generating decent financial performance.\n",
    "\n",
    "Following Ke et al. (2019), we will take an interest in stock price forecasting solely using textual data. Using news/forum data from our own internal NLP datalake (e.g. 3 million text inputs for Boeing), **we will attempt to forecast Boeing stock prices (from January 2015 to November 2020) with the authors' SESTM method**.\n",
    "\n",
    "We find that [Insert Conclusions]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. SSESTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sestm.png\" width=650 height=400>\n",
    "\n",
    "**Figure 4**: Original from [Ke et al., 2019](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3389884)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's assume we have a corpus of $n$ news articles and a lexicon of $m$ word tokens. Thus, we can model our corpus of $n$ documents as a bag of words representation: a document-word matrix $D$ of dimension $\\mathbb{R}^{n \\times m}$.\n",
    "\n",
    "From this corpus $D$, the goal of SSESTM is to learn custom sentiment (positive/negative) dictionnaries from one's own use-case dataset, without having to rely on pre-existing rule-based dictionnaries or purchase expensive solutions from third-party data vendors. This requires two components:\n",
    "\n",
    "- Select a set of words $\\hat S$ that are likely to foreshadow rises/decreases in the phenomena we are trying to forecast. SSESTM accomplishes this simply through word counts. E.g. stopwords (e.g. common words such as \"the\", \"a\", \"thus\",  which are unlikely to portend to any meaning)\n",
    "- From this filtered vocabulary list, weight each word by the sentiment it is the likeliest to foreshadow: e.g. \"stimulus\" for positive returns, \"coronavirus\" for negative returns. This is done through a supervised topic model (akin to Labelled LDA) that learns 2 distinct topics (or dictionnaries): one for words that presage positive returns ($O_{+}$) and one for words that presage negative returns ($O_{-}$).\n",
    "\n",
    "After learning $O_{+}$ and $O_{-}$, we can infer the sentiment (positive or negative) of unseen news articles $\\hat p$ through maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Correlation screening for excluding neutral words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.S1**. For each word $1 \\leq j \\leq m,$ let:\n",
    "\n",
    "$$f_{j}=\\frac{\\# \\text { articles including word } j \\text { AND having } \\operatorname{sgn}(y)=1}{\\# \\text { articles including word } j}$$\n",
    "\n",
    "This first step ranks words based on how often they appear in documents during periods of positive returns. Words with high $f_j$ are likely to augur positive returns, whereas low $f_j$ is likely to portend negative returns. Sandwitched in-between are neutral words, such as stopwords for example, unlikely to be indicative of stock rises/decreases.\n",
    "\n",
    "**2.1.S2**. For a proper threshold $\\alpha_{+}>0, \\alpha_{-}>0,$ and $\\kappa>0$ to be determined, construct:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\widehat{S}=\\left\\{j: f_{j} \\geq 1 / 2+\\alpha_{+}\\right\\} \\cup\\left\\{j: f_{j} \\leq 1 / 2-\\alpha_{-}\\right\\} \\cap\\left\\{j: k_{j} \\geq \\kappa\\right\\}\n",
    "$$\n",
    "where $k_{j}$ is the total count of articles in which word $j$ appears.\n",
    "\n",
    "Next step is excluding neutral words, i.e. vocabulary with middling $f_j$ values are excluded. The authors require the user to tune multiple hyperparameters:\n",
    "\n",
    "- $\\alpha_{+}$, upper bound for excluding neutral words with average $f_j$\n",
    "- $\\alpha_{-}$, lower bound for excluding neutral words with average $f_j$\n",
    "- $\\kappa$, number of count occurences required (excludes infrequent words)\n",
    "\n",
    "Two opposite pitfalls must be avoided: excluding to many words will drastically limit the size of the vocabulary, but the opposite will diminish the potency of SSESTM. If done optimally, this leads to a large dimensionality reduction, which explains how the authors claim that their approach can run on laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Learning Positive/Negative Vocabulary Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering out neutral words, the next step is learning positive/negative term vocabularies from our data. One approach would to run a LASSO regression classifier on our corpus $D_{[\\hat S]}$ (minus the neutral words) with words acting as features. We obtain positive/negative weights for a decent number of words and therefore our positive/negative vocabularies. The authors prefer to implement a generative model instead, where the joint distribution between words and returns is fully specified and learned from data.\n",
    "\n",
    "A popular generative model for modelling a distribution of words over documents is topic models (e.g. LDA, HDP, CTM) and for learning positive/negative dictionnaries, the authors construct what they describe to be a \"2-topic topic model\" that models positive auguring words as its first topic $\\widehat{O}_{+}$ and negative auguring terms as its second topic $\\widehat{O}_{-}$. Their \"2-topic topic model\" differs somewhat from classical topic models as the vast majority of topic models are unsupervised and thus don't require inputting labels. In contrast, the authors' model is a form of supervised topic model which are rarer in topic modeling litterature (e.g. Labelled LDA).\n",
    "\n",
    "In this supervised topic model with 2 topics, each document $\\hat{d}_{i,[S]}$ is modelled with a multinomial distribution:\n",
    "\n",
    "$$ {d}_{i,[S]} \\sim \\text{Multinomial}\\left(s_i, p_i O_{+} + (1 - p_i) O_{-}\\right) $$\n",
    "\n",
    "The expected value of $\\hat{d}_{i,[S]}$ can thus be written as:\n",
    "\n",
    "$$ \\mathbb{E}({d}_{i,[S]}) = p_i O_{+} + (1 - p_i) O_{-} $$\n",
    "\n",
    "We now need to learn $\\hat p_i$ and topics/vocabularies $\\widehat{O}_{+}$ and $\\widehat{O}_{-}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.S3**. Sort the returns $\\left\\{y_{i}\\right\\}_{i=1}^{n}$ in ascending order. For each $1 \\leq i \\leq n,$ let:\n",
    "$$\n",
    "\\widehat{p}_{i}=\\frac{\\text { rank of } y_{i} \\text { in all returns }}{n}\n",
    "$$\n",
    "\n",
    "For learning $\\hat p_i$, the authors rely on rank statistics, which is known to be robust to outliers.\n",
    "\n",
    "**2.2.S4**. For $1 \\leq i \\leq n,$ let $\\widehat{s}_{i}$ be the total counts of words from $\\widehat{S}$ in article $i,$ and let $\\hat{d}_{i}=\\widehat{s}_{i}^{-1} d_{i,[\\widehat{S}]}$ Write $\\widehat{D}=\\left[\\widehat{d}_{1}, \\widehat{d}_{2}, \\ldots, \\widehat{d}_{n}\\right] .$\n",
    "\n",
    "Recall the expected value per document:\n",
    "\n",
    "$$ \\mathbb{E}(\\tilde {d}_{i,[S]}) = p_i O_{+} + (1 - p_i) O_{-} $$\n",
    "\n",
    "Therefore, if we consider all corpus documents $D$, we obtain under matrix form:\n",
    "\n",
    "$$ \\mathbb{E} (\\widehat D^{T}) = O W $$\n",
    "\n",
    "According to the authors, $O$ can be approximated with an ordinary least squares (OLS) of $D$ on $W$\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\widehat{O}=\\widehat{D} \\widehat{W}^{\\prime}\\left(\\widehat{W} \\widehat{W}^{\\prime}\\right)^{-1}, \\quad \\text { where } \\quad \\widehat{W}=\\left[\\begin{array}{cccc}\n",
    "\\widehat{p}_{1} & \\widehat{p}_{2} & \\cdots & \\widehat{p}_{n} \\\\\n",
    "1-\\widehat{p}_{1} & 1-\\widehat{p}_{2} & \\cdots & 1-\\widehat{p}_{n}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Set negative entries of $\\widehat{O}$ to zero and re-normalize each column to have a unit $\\ell^{1}$ -norm. We use the same notation $\\widehat{O}$ for the resulting matrix. We also use $\\widehat{O}_{\\pm}$ to denote the two columns of $\\widehat{O}=\\left[\\widehat{O}_{+}, \\widehat{O}_{-}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Infering Sentiment from Unseen News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inferring the sentiment score for newer articles, the authors use maximum likelihood estimation to infer $\\hat p$.\n",
    "\n",
    "**2.3.S5**. Let $\\widehat{s}$ be the total count of words from $\\widehat{S}$ in the new article. Obtain $\\widehat{p}$ by\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\widehat{p}=\\arg \\max _{p \\in[0,1]}\\left\\{\\widetilde{s}^{-1} \\sum_{j=1}^{\\hat{s}} d_{j} \\log \\left(p \\widehat{O}_{+, j}+(1-p) \\widehat{O}_{-, j}\\right)+\\lambda \\log (p(1-p))\\right\\}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $d_{j}, \\widehat{O}_{+, j},$ and $\\widehat{O}_{-, j}$ are the $j$ th entries of the corresponding vectors, and $\\lambda>0$ is a tuning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our experimental approach differs from Ke et al. (2019) in three aspects:\n",
    "\n",
    "- Ke et al. (2019) trained their algorithms on a large Dow Jones news archive (*Dow Jones Newswires Machine Text Feed and Archive*), which spans from January 1989 to December 2012 (with data from February 2004 to July 2017 as their validation set). Our news dataset is more recent as it spans from **January 2015 to November 2020**.\n",
    "- The authors' news dataset is of considerably higher quality than the text data we will be using for our sentiment analysis use-case. This is primarly because their text articles from the *Dow Jones Newswires* archive are tagged to specific stocks. Our news dataset is **more raw and noisier**, which makes for a more challenging use-case.\n",
    "- Their dataset size is of size approximatly 13 million news articles (6.5 million for training and 6.7 for validation) and is multi-asset. Our use-case differs considerably as we will be focusing on a **single stock** (here Boeing Company)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Prediction Variable: Stock Price Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on predicting returns from the Boeing Company. Boeing is decent use-case for sentiment analysis with 2 major news events with considerable negative impact on Boeing Company's reputation: the 737 Max's flight woes and the COVID-19 Pandemic's darkening of the aviation industry.\n",
    "\n",
    "<img src=\"images/boeing_stock_price.png\">\n",
    "\n",
    "**Figure 5**: Boeing Stock Prices at Opening Time (9:00), available on Yahoo Finance\n",
    "\n",
    "Stock returns are a notoriously difficult to forecast due to low signal-to-noise ratio and Ke et al. (2019) spend a few sections on the lagged relationship between news and stock price mouvements. Thus, we have to be careful in defining our target variable, specifically the time horizon: are we just looking at 1-day daily returns? Or do we want to define a longer horizon, which will allow us to better capture market regimes? Huge stock price mouvements tend to undergo a correction, thus we can't use 1-day daily returns. After running a few tests, we chose 10 business days of cumulative returns as our predictive variable. (**Note: Need a better justification**)\n",
    "\n",
    "Thus we will compute cumulative returns in a 10 business day rolling window timeframe:\n",
    "\n",
    "```python\n",
    "horizon = 10\n",
    "target = 'Open'\n",
    "target_data['return'] = target_data[target] \\\n",
    "                                .pct_change() \\\n",
    "                                .rolling(horizon) \\\n",
    "                                .apply(lambda x: np.sum(x)) \\\n",
    "                                .shift(-horizon - 1) \\\n",
    "                                .dropna(how=\"all\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we obtain for Boeing:\n",
    "\n",
    "<img src=\"images/boeing_10d_cumulative_returns.png\">\n",
    "\n",
    "**Figure 6**: Rolling window of 10-day future cumulative returns on Boeing Open Stock Prices\n",
    "\n",
    "As sentiment analysis' goal is to predict the polarity (positive/negative) of a textual input, this can be addressed through a binary classification problem where we look to predict the likelihood that a text document will lead to future positive returns. Our training data will be composed of text documents preceeding positive returns (labelled as **1**) and text documents preceeding negative returns (labelled as **0**).\n",
    "\n",
    "```python\n",
    "target_data['return'] = (target_data['return'] > 0.0).astype(int)\n",
    "```\n",
    "\n",
    "Ideally, the distribution of our prediction variable should be balanced. But of course in practice it is never the case in a setting where market environments change and investor behavior shaken by downturn events. With the way we construct our sentiment analysis setup, we obtain unbalanced returns over time:\n",
    "\n",
    "<img src=\"images/boeing_return_distrib.png\">\n",
    "\n",
    "**Figure 7**: Average Distribution of Positive Labels (i.e. stock price increases) for Boeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the space of 3 months, the distribution of returns can change from mostly positive returns (over 80%) to unanimously negative returns (close de 0%). Realistically it is not hard to imagine that a bearish market environment (e.g. 2008 Financial Crisis, COVID-19 in March) would skewer our prediction variable distribution towards negative returns. Market corrections after downturns (e.g. COVID-19 post-April) would also skewer our predictive variable frequency towards positive returns. This will have to addressed when running our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Explanatory Variables: Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Boeing text inputs, in the past we collected an extremely large corpus of news/blog posts/forum posts from data vendors all stored on a datalake. Through ElasticSearch queries and disambiguation, we were able to obtain respectively 3 million text documents:\n",
    "\n",
    "<img src=\"images/boeing_article_compo.png\">\n",
    "\n",
    "**Figure 8**: Meta-information on 2.7 million text data available for Boeing (from Jan. 2015 to Nov. 2020)\n",
    "\n",
    "Let's check news count over time for Boeing:\n",
    "\n",
    "<img src=\"images/boeing_article_count.png\">\n",
    "\n",
    "**Figure 9**: Daily/bi-weekly news count available for Boeing (from Jan. 2015 to Nov. 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Regular expressions and stopwords\n",
    "\n",
    "Stopwords are words that express no intrinsic meaning and are most commonly used as grammatical expressions (e.g. the, who, where). We can also add a number of commonly used words (e.g. say, months). Our list combines NLTK stopwords with those from spaCy, for a total of 402 stopwords. Note that these words are highly likely to be seen as neutral during our correlation screening for keeping only positive/neutral words thus if we missed a couple of stopwords, SSESTM will take care of exclude them.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
    "\n",
    "nltk_stopwords = list(stopwords.words('english'))\n",
    "spacy_stopwords = list(spacy_stopwords)\n",
    "STOPWORDS = list(set(nltk_stopwords + spacy_stopwords))\n",
    "```\n",
    "\n",
    "Additionnally, string expressions representing emails or HTTP links are removed to prevent stopwords such as \"http\" or \"www\" from appearing in our learned vocabularies. Punctuation is also taken care of through regular expressions.\n",
    "\n",
    "```python\n",
    "punct = \"\"\"-!\"'#&$%\\()*+,.:;<=>?@[\\\\]^_`{|}~–’\"\"\"\n",
    "punc_pattern = '{}'.format('|'.join(['\\\\'+char for char in punct]))\n",
    "full_regex_pattern = r'(\\S*@\\S*\\s?|http\\S+|https\\S+|\\s+|{})'.format(punc_pattern)\n",
    "text_data['text'] = text_data['text'].str.replace(full_regex_pattern, ' ', regex=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SpaCy_logo.png\" width=\"250\" height=\"150\" class=\"center\">\n",
    "\n",
    "**Figure 10**: Logo for [spaCy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is running lemmatization on our text inputs. There are many redundant word declinations (e.g. plural versions) that can be reduced to a common lemma. Naturally when dealing with millions of text documents with text lengths ranging from a few words to over 100,000 words, this process can be time consuming. As for the library of choice for lemmatization, we opted for spaCy's lemmatization functions on our large corpus (inspired from [Cite Source]):\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable=['tagger', 'parser', 'ner'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "def lemmatize_pipe(doc):\n",
    "    \"\"\" AAA\n",
    "    \"\"\"\n",
    "    lemma_list = [str(tok.lemma_).lower() for tok in doc\n",
    "                  if ((tok.is_alpha) and\n",
    "                      (tok.lemma_ not in STOPWORDS)) and\n",
    "                      (len(tok.lemma_) >= 3)]\n",
    "    lemma = ' '.join(lemma_list)\n",
    "    return lemma\n",
    "\n",
    "def preprocess_pipe(texts):\n",
    "    \"\"\" AAA\n",
    "    \"\"\"\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=200):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "    return preproc_pipe\n",
    "\n",
    "text_data['text'] = preprocess_pipe(data_pandas['text'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Bag-of-word representations\n",
    "\n",
    "<img src=\"images/dask_logo.png\" width=\"250\" height=\"150\" class=\"center\">\n",
    "\n",
    "**Figure 11**: Logo for [Dask](https://dask.org/)\n",
    "\n",
    "As stated in previous sections, we are going to use **bag-of-word representations** to process our text data into a readable format for our supervised learning algorithms. It is undoubtely a fact that bag-of-word representations have limitations, primarily that word context is ignored (only the presence of words matters). It is also prone to computational difficulties as increasing the size of our training vocabularity increases the dimensionality of our training matrix (and the ensuing difficulties ten-fold). It also requires a decent amount of additional preprocessing (e.g. handling stopwords, synonyms, plurality, etc.).\n",
    "\n",
    "Embeddings - which are methods for representing highly-dimensional documents as lower-dimensional vectors - are often touted as superior alternatives to bag-of-word representations, as they capture more easily word context and can also be trained from training data. For NLP tasks more complex than sentiment analysis (e.g. language translation, text summarization, text generation such as GPT-3), it is indeed the case that embeddings have superseded bag-of-word representations owning to their much improved performance and ability to leverage deep learning algorithms.\n",
    "\n",
    "In spite of this, bag-of-word representations offer a crucial advantage over embeddings: interpretability. While there is a wide range of options for training embeddings (or relying on pretrained embeddings), bag-of-word representations are much easier to understand and most importantly control: if our algorithm fails to perform, we can simply check the inputs for irregularities (assuming we use mostly-interpretable supervised learning models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice to represent our text as bag-of-words, we rely on Scikit-Learn (or Dask ML) and its function `CountVectorizer`. It conveniantly outputs the bag-of-words matrix as a sparse matrix for ease in storing and computing.\n",
    "\n",
    "```python\n",
    "import dask.bag as db\n",
    "from dask_ml.feature_extraction.text import CountVectorizer as DaskMLCountVectorizer\n",
    "\n",
    "corpus = db.from_sequence(text_data, npartitions=10)\n",
    "vectorizer = DaskMLCountVectorizer(token_pattern = r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "                                   stop_words=STOPWORDS,\n",
    "                                   ngram_range=(1,1),\n",
    "                                   max_features=8577)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_bow = X.compute()\n",
    "X_bow\n",
    "```\n",
    "\n",
    "```\n",
    "<445798x8577 sparse matrix of type '<class 'numpy.int64'>'\n",
    "    with 66772209 stored elements in Compressed Sparse Row format>\n",
    "```\n",
    "\n",
    "For tokenization, we restrict ourselves to unigrams. The words the most likely to be highlighted by SSESTM are frequent non-neutral words and bigrams (by design) tend to appear in lower frequency than unigrams. Thus, for simplicity they are excluded from our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a description of how we implemented **SSESTM**:\n",
    "\n",
    "- ***Removal of neutral words***: there are two steps for removing neutral words. The first is to create a bag-of-word representation for every token and then filter out words that empirically appear equally during bullish/bearish market regimes. For bag-of-word representation, we utilize Scikit-Learn's CountVectorizer function (which can be substitued with a multithreaded variant provided by Dask ML). For filtering out neutral words, we compute a matrix dot product between the bag-of-word representation and the target vector (positive or negative returns). This allows to compute word frequencies only on documents on days preceding stock price increases/decreases, which will allow us to find and exclude neutral words. This reveals to be extremely memory-efficient and fast, especially when dealing with representations of over 2 million documents;\n",
    "- ***Learning sentiment topics/vocabularies***: for this step we need to implement rank statistics then an ordinary least squares (OLS), which can be simply through `numpy` functions;\n",
    "- ***Scoring out-of-sample documents***: for maximum likelihood estimation (MLE), we implement `scipy` optimization functions.\n",
    "\n",
    "As our baseline, we will consider 2 supervised discriminative models:\n",
    "\n",
    "- **Logistic Regression** (implemented in Scikit-Learn)\n",
    "- **LightGBM**\n",
    "\n",
    "And 2 supervised generative models:\n",
    "\n",
    "- **Naive Bayes** (implemented in Scikit-Learn)\n",
    "- **Labelled Latent Dirichlet Allocation** (implemented in `tomotopy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Hyperparameter Selection and Learning Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, training our algorithm on the entire dataset would prevent us form capturing changes in word polarity over time (e.g. how investors and news outlets soured on Boeing's 737 Max after multiple incidents).\n",
    "\n",
    "In the original paper from Ke et al. (2019), the authors considered Dow Jones' stocks spanning from 2004 to 2017 and updating their model every year (on a rolling window basis), thus a 14 training sets in total. Word polarity can change over time from highs (e.g. new announcements concerning Boeing's new 737 Max, Trump tax cuts) to lows (e.g. 737 Max crashes, COVID-19) so it is important to factor in evolving market regimes. We will retrain our model every 3 months (or 12 weeks) which will then be used to predict text polarity for unseen text the following month.\n",
    "\n",
    "<img src=\"images/cross_val_boeing.png\">\n",
    "\n",
    "**Figure 12**: Time Series Cross-Validation Strategy (only subset of training/testing dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal choice for hyperparameters is much trickier decision to be made. Let's start with the issue of excluding neutral words. The authors give users the option to set a lower/upper bound given a 50% threshold of word apperance in text documents published during positive market regimes. E.g. in our Boeing use-case, we could exclude words with apperance values between 48% and 52%. But this assumes a balanced distribution for our prediction variable (returns), which is unlikely to be verified in a real life environment as we addressed beforehand. The following chart illustrates changes in word frequency over time:\n",
    "\n",
    "<img src=\"images/vocab_dynamics.png\">\n",
    "\n",
    "**Figure 13**: Evolving word frequency over text and median word sentiment\n",
    "\n",
    "An easy temporary fix would be to dynamically set thresholds for excluding neutral words: during bullish markets, thresholds would be increased from 50% and conversely decreased during bearish markets. For each training set, we will take the median polarity as our threshold (e.g. if \"boeing\" has a polarity of 0.43 and represents the median polarity of our vocabularity then 43% is our threshold).\n",
    "\n",
    "Through a grid search routine, we will test a decent number of hyperparameter combinations for all 4 and choose the set that delivers the best testing set performance:\n",
    "\n",
    "- $(\\alpha_{+}, \\alpha_{-}) \\in \\{0.0, 0.01, 0.04\\}$\n",
    "- $\\kappa \\in \\{1500, 5000\\}$\n",
    "- $\\lambda \\in \\{0.0001, 0.5\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be training SESTM on the entirety of our text data, mostly as an exercise in showcasing the algorithm's scalability. We will be comparing our implementation of SESTM with an alternative version freely available on GitHub, measuring differences in execution time and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our use-case, sentiment analysis algorithms will output a probability (or a score) indicative of how likely a text document foreshadows future positive (label 1) or negative returns (label 0) for the next 10 days. For unseen text documents and if labels are evenly distributed, a threshold of 50% is set as the delimiter between text inputs that will lead to positive returns and those leading to negative returns.\n",
    "\n",
    "But in the case of unbalanced label distribution, the majority label will skewer predictions towards a median probability that is further away (in either direction) from the ideal 50% threshold. Therefore using the 50% threshold for predicting labels will lead to an over-representation of the majority label in its predictions.\n",
    "\n",
    "To address this constraint, we will be evaluating our model predictions with metrics (**ROC AUC Score**, **Cumulative Average Precision Score**) that evaluate more heuristically how well our sentiment analysis model is capable of distinguishing positive returns from negative returns (regardless of the threshold set, whether it is 50%, 25%, 75%, etc.).\n",
    "\n",
    "Once we get these polarity predictions for unseen Boeing text data, **how do we evaluate and exploit those predictions?** Since our text documents are labelled, we could compare the document labels with our predictions. We could also follow Ke et al. (2019) in computing Boeing's average polarity score every day. Thus instead of having to evaluate over predictions over 2 million text documents, we would only need to assess around 1,400 predictions. This would be closer to our ultimate goal of forecasting Boeing stock prices.\n",
    "\n",
    "For completeness, we will showcase both results on predicted polarities over text data (**text polarity**) and Boeing's average daily polarity score (**stock polarity**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Use-case: Forecasting Boeing Stock Prices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Results on a real life production environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of our five models:\n",
    "\n",
    "- SSESTM\n",
    "- Labelled Latent Dirichlet Allocation (LLDA)\n",
    "- Logistic Regression (LR)\n",
    "- Naive Bayes (NB)\n",
    "- XGBoost\n",
    "\n",
    "From January 2015 to November 2020, we run different sentiment analysis models on our set of Boeing stock returns and textual data, with a sliding window of 12 weeks of training data used then to predict 1 week of polarity scores (testing data)a. Those predictions are then evaluated through ROC AUC and Cumulative Average Precision (CAP) metrics. Thus, we obtain the following results:\n",
    "\n",
    "<br>\n",
    "\n",
    "|  Model | AUC Test | CAP Test |\n",
    "|:---|---:|---:|\n",
    "| **SSESTM**  |  51.5% | 57.6%  |\n",
    "| **Labelled Latent Dirichlet Allocation**  | a%  | a%  |\n",
    "| **Logistic Regression** | a%  | a%  |\n",
    "| **Naive Bayes Classifier** | 54.5%  | 60.7%  |\n",
    "| **XGBoost Classifier** | a%  | a%  |\n",
    "\n",
    "**Table 1**: Predictive performance for 2 million text polarity predictions (Apr. 2015 - Nov. 2020)\n",
    "\n",
    "<br>\n",
    "\n",
    "Next, we compute the average predicted polarity score for each day (e.g. 30 polarity scores for 30 unseen text inputs gives us an average polarity of 55%). From those average polarity scores foreshadows us whether Boeing's stock price is going to rise or fall in the next 10 days (starting tomorrow). We evaluate those predictions once again with ROC AUC and CAP metrics:\n",
    "\n",
    "<br>\n",
    "\n",
    "|  Model | AUC Test | CAP Test |\n",
    "|:---|---:|---:|\n",
    "| **SSESTM**  |  57.4% | 62.8%  |\n",
    "| **Labelled Latent Dirichlet Allocation**  | a% | a%  |\n",
    "| **Logistic Regression** | a%  | a%  |\n",
    "| **Naive Bayes Classifier** | 63.7%  | 70.0%  |\n",
    "| **XGBoost Classifier** | a%  | a%  |\n",
    "\n",
    "**Table 2**: Predictive performance for 1 410 stock sentiment predictions (Apr. 2015 - Nov. 2020)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot both ROC and Precision-Recall Curves over our five models (for both text polarity predictions and stock sentiment predictions):\n",
    "\n",
    "<img src=\"images/results_ROC_AUC.png\">\n",
    "\n",
    "**Figure 14**: ROC Curve for text polarity predictions (Apr. 2015 - Nov. 2020)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/results_PRC.png\">\n",
    "\n",
    "**Figure 15**: Precision-Recall Curve for text polarity predictions (Apr. 2015 - Nov. 2020)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at performance over time, [Insert Commentary]:\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/results_ts_metrics.png\">\n",
    "\n",
    "**Figure 16**: Performance over time every 3 months for stock sentiment predictions (Apr. 2015 - Nov. 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Add evolution of wordclouds for SSESTM]\n",
    "\n",
    "[Add evolution of average polarity for SSESTM and Naive Bayes]\n",
    "\n",
    "<img src=\"images/results_POLARITY.png\">\n",
    "\n",
    "**Figure 17**: Daily time series of predicted stock sentiment (Apr. 2015 - Nov. 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Grid Search for SESTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  $\\alpha_{+}$ | $\\alpha_{-}$ | $\\kappa$ | $\\lambda$ | Text AUC | Text CAP | Stock AUC | Stock CAP |\n",
    "|---:|---:|---:|---:|---:|---:| ---:|---:|\n",
    "| 0.01  |  0.01 | 1500  | 0.0001 | a% | a% | a% |a% |\n",
    "| 0.04  |  0.04 | 1500  | 0.0001 | a% | a% | a% |a% |\n",
    "| 0.01  |  0.01 | 1500  | 0.5 | a% | a%| a% |a% |\n",
    "| 0.04  |  0.04 | 1500  | 0.5 | a% | a% | a% |a% |\n",
    "| 0.01  |  0.01 | 5000  | 0.0001 | 51.6% | 57.1% | 56.8% | 64.1% |\n",
    "| 0.04  |  0.04 | 5000  | 0.0001 | 50.3% | 56.8% | 52.3% | 59.8% |\n",
    "| 0.01  |  0.01 | 5000  | 0.5 |  51.6% | 57.9% | 56.5% | 62.2% |\n",
    "| 0.04  |  0.04 | 5000  | 0.5 | 50.2% | 57.0% | 52.1% | 60.0% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Comparaison with alternative implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summary\n",
    "- Limitations of SESTM\n",
    "\n",
    "A number of potential improvements can be made on SSESTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeusEx] *",
   "language": "python",
   "name": "conda-env-DeusEx-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
