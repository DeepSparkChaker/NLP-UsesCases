{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment analysis\n",
    "Reading the mood from text with machine learning \n",
    "## Choosing a Data Set\n",
    "Go ahead and download the data set from the Sentiment Labelled Sentences Data Set from the UCI Machine Learning Repository.\n",
    "\n",
    "By the way, this repository is a wonderful source for machine learning data sets when you want to try out some algorithms. This data set includes labeled reviews from IMDb, Amazon, and Yelp. Each review is marked with a score of 0 for a negative sentiment or 1 for a positive sentiment.\n",
    "\n",
    "Extract the folder into a data folder and go ahead and load the data with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence    Wow... Loved this place.\n",
      "label                              1\n",
      "source                          yelp\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#https://realpython.com/python-keras-text-classification/#choosing-a-data-set\n",
    "filepath_dict = {'yelp':\n",
    "                 'C:/Users/rzouga/Downloads/Github/NLP/sentimentlabelledsentences/sentimentlabelledsentences/yelp_labelled.txt',\n",
    "                 'amazon':\n",
    "                 'C:/Users/rzouga/Downloads/Github/NLP/sentimentlabelledsentences/sentimentlabelledsentences/amazon_cells_labelled.txt',\n",
    "                 'imdb':  \n",
    "                 'C:/Users/rzouga/Downloads/Github/NLP/sentimentlabelledsentences/sentimentlabelledsentences/imdb_labelled.txt'}\n",
    "\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks about right. With this data set, you are able to train a model to predict the sentiment of a sentence. Take a quick moment to think about how you would go about predicting the data.\n",
    "\n",
    "One way you could do this is to count the frequency of each word in each sentence and tie this count back to the entire set of words in the data set. You would start by taking the data and creating a vocabulary from all the words in all sentences. The collection of texts is also called a corpus in NLP.\n",
    "\n",
    "The vocabulary in this case is a list of words that occurred in our text where each word has its own index. This enables you to create a vector for a sentence. You would then take the sentence you want to vectorize, and you count each occurrence in the vocabulary. The resulting vector will be with the length of the vocabulary and a count for each word in the vocabulary. \n",
    "The resulting vector is also called a feature vector. In a feature vector, each dimension can be a numeric or categorical feature, like for example the height of a building, the price of a stock, or, in our case, the count of a word in a vocabulary. These feature vectors are a crucial piece in data science and machine learning, as the model you want to train depends on them.\n",
    "\n",
    "Let’s quickly illustrate this. Imagine you have the following two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> sentences = ['John likes ice cream', 'John hates chocolate.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can use the CountVectorizer provided by the scikit-learn library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    ">>> vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    ">>> vectorizer.fit(sentences)\n",
    ">>> vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vocabulary serves also as an index of each word. Now, you can take each sentence and get the word occurrences of the words based on the previous vocabulary. The vocabulary consists of all five words in our sentences, each representing one word in the vocabulary. When you take the previous two sentences and transform them with the CountVectorizer you will get a vector representing the count of each word of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> vectorizer.transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see the resulting feature vectors for each sentence based on the previous vocabulary. For example, if you take a look at the first item, you can see that both vectors have a 1 there. This means that both sentences have one occurrence of John, which is in the first place in the vocabulary.\n",
    "\n",
    "This is considered a Bag-of-words (BOW) model, which is a common way in NLP to create vectors out of text. Each document is represented as a vector. You can use these vectors now as feature vectors for a machine learning model. This leads us to our next part, defining a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'likes': 2, 'movies': 2, 'john': 1, 'to': 1, 'watch': 1, 'mary': 1, 'too': 1}\n",
      "Bag of word sentence 1 :\n",
      "{'likes': 2, 'movies': 2, 'john': 1, 'to': 1, 'watch': 1, 'mary': 1, 'too': 1}\n",
      "We found 7 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# other way to solve it \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too.\"]\n",
    "\n",
    "def print_bow(sentence: str) -> None:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence)\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    word_index = tokenizer.word_index \n",
    "    bow = {}\n",
    "    for key in word_index:\n",
    "        bow[key] = sequences[0].count(word_index[key])\n",
    "    print(bow)\n",
    "    print(f\"Bag of word sentence 1 :\\n{bow}\")\n",
    "    print(f'We found {len(word_index)} unique tokens.')\n",
    "\n",
    "print_bow(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Baseline Model\n",
    "\n",
    "When you work with machine learning, one important step is to define a baseline model. This usually involves a simple model, which is then used as a comparison with the more advanced models that you want to test. In this case, you’ll use the baseline model to compare it to the more advanced methods involving (deep) neural networks, the meat and potatoes of this tutorial.\n",
    "\n",
    "First, you are going to split the data into a training and testing set which will allow you to evaluate the accuracy and see if your model generalizes well. This means whether the model is able to perform well on data it has not seen before. This is a way to see if the model is overfitting.\n",
    "\n",
    "Overfitting is when a model is trained too well on the training data. You want to avoid overfitting, as this would mean that the model mostly just memorized the training data. This would account for a large accuracy with the training data but a low accuracy in the testing data.\n",
    "\n",
    "We start by taking the Yelp data set which we extract from our concatenated data set. From there, we take the sentences and labels. The .values returns a NumPy array instead of a Pandas Series object which is in this context easier to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import train_test_split\n",
    "\n",
    ">>> df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    ">>> sentences = df_yelp['sentence'].values\n",
    ">>> y = df_yelp['label'].values\n",
    "\n",
    ">>> sentences_train, sentences_test, y_train, y_test = train_test_split( sentences, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use again on the previous BOW model to vectorize the sentences. You can use again the CountVectorizer for this task. Since you might not have the testing data available during training, you can create the vocabulary using only the training data. Using this vocabulary, you can create the feature vectors for each sentence of the training and testing s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<750x1714 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> vectorizer.fit(sentences_train)\n",
    "\n",
    ">>> X_train = vectorizer.transform(sentences_train)\n",
    ">>> X_test  = vectorizer.transform(sentences_test)\n",
    ">>> X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the resulting feature vectors have 750 samples which are the number of training samples we have after the train-test split. Each sample has 1714 dimensions which is the size of the vocabulary. Also, you can see that we get a sparse matrix. This is a data type that is optimized for matrices with only a few non-zero elements, which only keeps track of the non-zero elements reducing the memory load.\n",
    "\n",
    "CountVectorizer performs tokenization which separates the sentences into a set of tokens as you saw previously in the vocabulary. It additionally removes punctuation and special characters and can apply other preprocessing to each word. If you want, you can use a custom tokenizer from the NLTK library with the CountVectorizer or use any number of the customizations which you can explore to improve the performance of your model.\n",
    "\n",
    "Note: There are a lot of additional parameters to CountVectorizer() that we forgo using here, such as adding ngrams, beacuse the goal at first is to build a simple baseline model. The token pattern itself defaults to token_pattern=’(?u)\\b\\w\\w+\\b’, which is a regex pattern that says, “a word is 2 or more Unicode word characters surrounded by word boundaries.”.\n",
    "\n",
    "The classification model we are going to use is the logistic regression which is a simple yet powerful linear model that is mathematically speaking in fact a form of regression between 0 and 1 based on the input feature vector. By specifying a cutoff value (by default 0.5), the regression model is used for classification. You can use again scikit-learn library which provides the LogisticRegression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.796\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> classifier = LogisticRegression()\n",
    ">>> classifier.fit(X_train, y_train)\n",
    ">>> score = classifier.score(X_test, y_test)\n",
    ">>> print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the logistic regression reached an impressive 79.6%, but let’s have a look how this model performs on the other data sets that we have. In this script, we perform and evaluate the whole process for each data set that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for yelp data: 0.7960\n",
      "Accuracy for amazon data: 0.7960\n",
      "Accuracy for imdb data: 0.7487\n"
     ]
    }
   ],
   "source": [
    "for source in df['source'].unique():\n",
    "    df_source = df[df['source'] == source]\n",
    "    sentences = df_source['sentence'].values\n",
    "    y = df_source['label'].values\n",
    "\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_train)\n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You can see that this fairly simple model achieves a fairly good accuracy. It would be interesting to see whether we are able to outperform this model. In the next part, we will get familiar with (deep) neural networks and how to apply them to text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Keras Model\n",
    "\n",
    "Now you are finally ready to experiment with Keras. Keras supports two main types of models. You have the Sequential model API which you are going to see in use in this tutorial and the functional API which can do everything of the Sequential model but it can be also used for advanced models with complex network architectures.\n",
    "\n",
    "The Sequential model is a linear stack of layers, where you can use the large variety of available layers in Keras. The most common layer is the Dense layer which is your regular densely connected neural network layer with all the weights and biases that you are already familiar with.\n",
    "\n",
    "Let’s see if we can achieve some improvement to our previous logistic regression model. You can use the X_train and X_test arrays that you built in our earlier example.\n",
    "\n",
    "Before we build our model, we need to know the input dimension of our feature vectors. This happens only in the first layer since the following layers can do automatic shape inference. In order to build the Sequential model, you can add layers one by one in order as follows:\n",
    "Before you can start with the training of the model, you need to configure the learning process. This is done with the .compile() method. This method specifies the optimizer and the loss function.\n",
    "\n",
    "Additionally, you can add a list of metrics which can be later used for evaluation, but they do not influence the training. In this case, we want to use the binary cross entropy and the Adam optimizer you saw in the primer mentioned before. Keras also includes a handy .summary() function to give an overview of the model and the number of parameters available for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                25060     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 25,071\n",
      "Trainable params: 25,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "def build_model():\n",
    "    model1 = tf.keras.Sequential()\n",
    "    model1.add( layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model1.add(layers.Dense(1, activation='sigmoid'))  \n",
    "    #optimizer='adam'\n",
    "    optimizer =tf.keras.optimizers.Adam()\n",
    "    #optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "     ## Compile model\n",
    "    #epochs = 50\n",
    "    #learning_rate = 0.1\n",
    "    #decay_rate = learning_rate / epochs\n",
    "    #momentum = 0.8\n",
    "    #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "    #https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
    "    #optimizer=sgd\n",
    "    model1.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "              metrics=['acc'])\n",
    "    return model1\n",
    "\n",
    "model1 = build_model()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to start your training with the .fit() function.\n",
    "\n",
    "Since the training in neural networks is an iterative process, the training won’t just stop after it is done. You have to specify the number of iterations you want the model to be training. Those completed iterations are commonly called epochs. We want to run it for 100 epochs to be able to see how the training loss and accuracy are changing after each epoch.\n",
    "\n",
    "Another parameter you have to your selection is the batch size. The batch size is responsible for how many samples we want to use in one epoch, which means how many samples are used in one forward/backward pass. This increases the speed of the computation as it need fewer epochs to run, but it also needs more memory, and the model may degrade with larger batch sizes. Since we have a small training set, we can leave this to a low batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:From C:\\Users\\rzouga\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, y_train,\n",
    "                     epochs=100,\n",
    "                     verbose=False,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                        batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the .evaluate() method to measure the accuracy of the model. You can do this both for the training data and testing data. We expect that the training data has a higher accuracy then for the testing data. Tee longer you would train a neural network, the more likely it is that it starts overfitting.\n",
    "\n",
    "Note that if you rerun the .fit() method, you’ll start off with the computed weights from the previous training. Make sure to compile the model again before you start training the model again. Now let’s evaluate the accuracy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Training Accuracy: 1.0000\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Testing Accuracy:  0.7861\n"
     ]
    }
   ],
   "source": [
    ">>> loss, accuracy = model1.evaluate(X_train, y_train, verbose=False)\n",
    ">>> print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    ">>> loss, accuracy = model1.evaluate(X_test, y_test, verbose=False)\n",
    ">>> print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already see that the model was overfitting since it reached 100% accuracy for the training set. But this was expected since the number of epochs was fairly large for this model. However, the accuracy of the testing set has already surpassed our previous logistic Regression with BOW model, which is a great step further in terms of our progress.\n",
    "\n",
    "To make your life easier, you can use this little helper function to visualize the loss and accuracy for the training and testing data based on the History callback. This callback, which is automatically applied to each Keras model, records the loss and additional metrics that can be added in the .fit() method. In this case, we are only interested in the accuracy. This helper function employs the matplotlib plotting library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function, simply call plot_history() with the collected accuracy and loss inside the history dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ">>> plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have trained our model for too long since the training set reached 100% accuracy. A good way to see when the model starts overfitting is when the loss of the validation data starts rising again. This tends to be a good point to stop the model. You can see this around 20-40 epochs in this training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note: When training neural networks, you should use a separate testing and validation set. What you would usually do is take the model with the highest validation accuracy and then test the model with the testing set.\n",
    "\n",
    "This makes sure that you don’t overfit the model. Using the validation set to choose the best model is a form of data leakage (or “cheating”) to get to pick the result that produced the best test score out of hundreds of them. Data leakage happens when information outside the training data set is used in the model.\n",
    "\n",
    "In this case, our testing and validation set are the same, since we have a smaller sample size. As we have covered before, (deep) neural networks perform best when you have a very large number of samples. In the next part, you’ll see a different way to represent words as vectors. This is a very exciting and powerful way to work with words where you’ll see how to represent words as dense vectors.\n",
    "\n",
    "# What Is a Word Embedding?\n",
    "\n",
    "Text is considered a form of sequence data similar to time series data that you would have in weather data or financial data. In the previous BOW model, you have seen how to represent a whole sequence of words as a single feature vector. Now you will see how to represent each word as vectors. There are various ways to vectorize text, such as:\n",
    "\n",
    "    Words represented by each word as a vector\n",
    "    Characters represented by each character as a vector\n",
    "    N-grams of words/characters represented as a vector (N-grams are overlapping groups of multiple succeeding words/characters in the text)\n",
    "\n",
    "In this tutorial, you’ll see how to deal with representing words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are one-hot encoding and word embeddings.\n",
    "One-Hot Encoding\n",
    "\n",
    "The first way to represent a word as a vector is by creating a so-called one-hot encoding, which is simply done by taking a vector of the length of the vocabulary with an entry for each word in the corpus.\n",
    "\n",
    "In this way, you have for each word, given it has a spot in the vocabulary, a vector with zeros everywhere except for the corresponding spot for the word which is set to one. As you might imagine, this can become a fairly large vector for each word and it does not give any additional information like the relationship between words.\n",
    "\n",
    "Let’s say you have a list of cities as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London', 'Berlin', 'Berlin', 'New York', 'London']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\n",
    ">>> cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use scikit-learn and the LabelEncoder to encode the list of cities into categorical integer values like here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    ">>> encoder = LabelEncoder()\n",
    ">>> city_labels = encoder.fit_transform(cities)\n",
    ">>> city_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this representation, you can use the OneHotEncoder provided by scikit-learn to encode the categorical values we got before into a one-hot encoded numeric array. OneHotEncoder expects each categorical value to be in a separate row, so you’ll need to reshape the array, then you can apply the enco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    ">>> encoder = OneHotEncoder(sparse=False)\n",
    ">>> city_labels = city_labels.reshape((5, 1))\n",
    ">>> encoder.fit_transform(city_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that categorical integer value represents the position of the array which is 1 and the rest is 0. This is often used when you have a categorical feature which you cannot represent as a numeric value but you still want to be able to use it in machine learning. One use case for this encoding is of course words in a text but it is most prominently used for categories. Such categories can be for example city, department, or other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "This method represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions.\n",
    "\n",
    "Note that the word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus. Their aim is to map semantic meaning into a geometric space. This geometric space is then called the embedding space.\n",
    "\n",
    "This would map semantically similar words close on the embedding space like numbers or colors. If the embedding captures the relationship between words well, things like vector arithmetic should become possible. A famous example in this field of study is the ability to map King - Man + Woman = Queen.\n",
    "\n",
    "How can you get such a word embedding? You have two options for this. One way is to train your word embeddings during the training of your neural network. The other way is by using pretrained word embeddings which you can directly use in your model. There you have the option to either leave these word embeddings unchanged during training or you train them also. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you get such a word embedding? You have two options for this. One way is to train your word embeddings during the training of your neural network. The other way is by using pretrained word embeddings which you can directly use in your model. There you have the option to either leave these word embeddings unchanged during training or you train them also. \n",
    "Now you need to tokenize the data into a format that can be used by the word embeddings. Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text.\n",
    "\n",
    "You can start by using the Tokenizer utility class which can vectorize a text corpus into a list of integers. Each integer maps to a value in a dictionary that encodes the entire corpus, with the keys in the dictionary being the vocabulary terms themselves. You can add the parameter num_words, which is responsible for setting the size of the vocabulary. The most common num_words words will be then kept. I have the testing and training data prepared from the previous example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a fan of his ... This movie sucked really bad.  \n",
      "[7, 150, 2, 932, 4, 49, 6, 11, 563, 45, 30]\n"
     ]
    }
   ],
   "source": [
    ">>> from keras.preprocessing.text import Tokenizer\n",
    "\n",
    ">>> tokenizer = Tokenizer(num_words=5000)\n",
    ">>> tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    ">>> X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    ">>> X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    ">>> vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    ">>> print(sentences_train[2])\n",
    ">>> print(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing is ordered after the most common words in the text, which you can see by the word the having the index 1. It is important to note that the index 0 is reserved and is not assigned to any word. This zero index is used for padding, which I’ll introduce in a moment.\n",
    "\n",
    "Unknown words (words that are not in the vocabulary) are denoted in Keras with word_count + 1 since they can also hold some information. You can see the index of each word by taking a look at the word_index dictionary of the Tokenizer object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 1\n",
      "all: 27\n"
     ]
    }
   ],
   "source": [
    "for word in ['the', 'all']:\n",
    "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Note**: Pay close attention to the difference between this technique and the X_train that was produced by scikit-learn’s CountVectorizer.\n",
    "\n",
    "With CountVectorizer, we had stacked vectors of word counts, and each vector was the same length (the size of the total corpus vocabulary). With Tokenizer, the resulting vectors equal the length of each text, and the numbers don’t denote counts, but rather correspond to the word values from the dictionary tokenizer.word_index.\n",
    "\n",
    "One problem that we have is that each text sequence has in most cases different length of words. To counter this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros.\n",
    "\n",
    "Additionally you would want to add a maxlen parameter to specify how long the sequences should be. This cuts sequences that exceed that number. In the following code, you can see how to pad sequences with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170 116 390  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    ">>> from keras.preprocessing.sequence import pad_sequences\n",
    ">>> maxlen = 100\n",
    ">>> X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    ">>> X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    ">>> print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first values represent the index in the vocabulary as you have learned from the previous examples. You can also see that the resulting feature vector contains mostly zeros, since you have a fairly short sentence. In the next part you will see how to work with word embeddings in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import abc\n",
    "\n",
    "model= gensim.models.Word2Vec(abc.sents())\n",
    "X= list(model.wv.vocab)\n",
    "data=model.most_similar('science')\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
