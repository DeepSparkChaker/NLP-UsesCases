{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "In the video exercise, you were exposed to the various applications of sequence to sequence models. In this exercise you will see how to use a pre-trained model for sentiment analysis.\n",
    "\n",
    "The model is pre-loaded in the environment on variable model. Also, the tokenized test set variables X_test and y_test and the pre-processed original text data sentences from IMDb are also available.You will learn how to pre-process the text data and how to create and train the model using Keras later in the course.\n",
    "\n",
    "You will use the pre-trained model to obtain predictions of sentiment. The model returns a number between zero and one representing the probability of the sentence to have a positive sentiment. So, you will create a decision rule to set the prediction to positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the predicion for all the sentences\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x>0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame({'sentence': sentences, 'y_pred': pred_sentiment , 'y_true': y_test})\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Ladybugs', 1: 'Scissors', 2: 'Spock', 3: 'Spock,', 4: \"You're\", 5: 'afraid', 6: 'always', 7: 'and', 8: 'as', 9: 'catatonic.', 10: 'covers', 11: 'crushes', 12: 'cuts', 13: 'decapitates', 14: 'disproves', 15: 'eats', 16: 'has,', 17: 'insects', 18: 'it', 19: 'lizard', 20: 'lizard,', 21: 'must', 22: 'of', 23: 'paper', 24: 'paper,', 25: 'poisons', 26: 'render', 27: 'rock', 28: 'rock,', 29: 'scissors', 30: 'scissors,', 31: 'scissors.', 32: 'smashes', 33: 'vaporizes', 34: 'women,', 35: 'you'}\n",
      "{'Ladybugs': 0, 'Scissors': 1, 'Spock': 2, 'Spock,': 3, \"You're\": 4, 'afraid': 5, 'always': 6, 'and': 7, 'as': 8, 'catatonic.': 9, 'covers': 10, 'crushes': 11, 'cuts': 12, 'decapitates': 13, 'disproves': 14, 'eats': 15, 'has,': 16, 'insects': 17, 'it': 18, 'lizard': 19, 'lizard,': 20, 'must': 21, 'of': 22, 'paper': 23, 'paper,': 24, 'poisons': 25, 'render': 26, 'rock': 27, 'rock,': 28, 'scissors': 29, 'scissors,': 30, 'scissors.': 31, 'smashes': 32, 'vaporizes': 33, 'women,': 34, 'you': 35}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "text =\"You're afraid of insects and women, Ladybugs must render you catatonic.\", 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.'\n",
    "DOC=nlp(str(text))\n",
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(text).split(' ')\n",
    "#all_words=[w for w in DOC ]\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "print(index_to_word)\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i, wd in enumerate(sorted(unique_words))}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing text data for model input\n",
    "Previously, you learned how to create dictionaries of indexes to words and vice versa. In this exercise, you will split the text by characters and continue to prepare the data for supervised learning.\n",
    "\n",
    "Splitting the texts into characters may seem strange, but it is often done for text generation. Also, the process to prepare the data is the same, the only change is how to split the texts.\n",
    "\n",
    "You will create the training data containing a list of fixed-length texts and their labels, which are the corresponding next characters.\n",
    "\n",
    "You will continue to use the dataset containing quotes from Sheldon (The Big Bang Theory), available in the sheldon_quotes variable.\n",
    "\n",
    "The print_examples() function print the pairs so you can see how the data was transformed. Use help() for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2         # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n",
    "\n",
    "# Print 10 pairs\n",
    "print_examples(sentences, next_chars, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 8, 18, 0, 0, 0, 0, 18, 0, 7, 0, 18, 0, 0, 0, 0, 0, 0, 18, 7, 0, 0, 0, 0]\n",
      "Ladybugs Ladybugs Ladybugs Ladybugs Ladybugs as it Ladybugs Ladybugs Ladybugs Ladybugs it Ladybugs and Ladybugs it Ladybugs Ladybugs Ladybugs Ladybugs Ladybugs Ladybugs it and Ladybugs Ladybugs Ladybugs Ladybugs\n"
     ]
    }
   ],
   "source": [
    "new_text =['A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away', 'To the brave crew and passengers of the Kobayshi Maru sucks to be you', 'Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies', 'They are merely scars not mortal wounds and you must use them to propel you forward', 'You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose']\n",
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd,0 )\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models\n",
    "In this exercise you'll practice using two classes from the keras.models module. You will create one model using the two classes Sequential and Model.\n",
    "\n",
    "The Sequential class is easier since the layers are assumed to be in order, while the Model class is more flexible and allows multiple inputs, multiple outputs and shared layers (shared weights).\n",
    "\n",
    "The Model class needs to explicitly declare the input layer, while in the Sequential class, this is done with the input_shape parameter.\n",
    "\n",
    "The objects and modules Sequential, Model, Dense, Input, LSTM and np (numpy) are already loaded on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "LSTM (LSTM)                  (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from numpy import array\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "layers = tf.keras.layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# Instantiate the class\n",
    "model = tf.keras.Sequential(name=\"sequential_model\")\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(layers.LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionel Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"modelclass_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense,Input\n",
    "layers = tf.keras.layers\n",
    "import tensorflow as tf\n",
    "# Define the input layer\n",
    "main_input =Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = layers.LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = tf.keras.Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You can see that the keras.models.Sequential is very easy to use to add layers in sequence. On the other hand, the keras.models.Model class is very flexible and is usually the choice when scientists need deep customization in their solution. Also, you saw how one layer is connected to another layer in both cases, be by adding them in sequence using the method add, or by creating a layer and calling the desired (previous) layer like a function, in the Model class API, every layer is callable on a tensor and always return a tensor\n",
    "# Keras preprocessing\n",
    "\n",
    "The second most important module of Keras is keras.preprocessing. You will see how to use the most important modules and functions to prepare raw data to the correct input shape. Keras provides functionalities that substitute the dictionary approach you learned before.\n",
    "\n",
    "You will use the module keras.preprocessing.text.Tokenizer to create a dictionary of words using the method .fit_on_texts() and change the texts into numerical ids representing the index of each word on the dictionary using the method .texts_to_sequences().\n",
    "\n",
    "Then, use the function .pad_sequences() from keras.preprocessing.sequence to make all the sequences have the same size (necessary for the model) by adding zeros on the small texts and cutting the big ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sample texts: (29, 14)\n",
      "Now the texts have fixed length: 60. Let's see the first one: \n",
      "[ 7 12 13 14 15  5  3 16  1 17 18  3 19  8  4 20  3 21 22 23 24 25  8  3\n",
      "  4 26  1 27  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Import relevant classes/functions\n",
    "from tensorflow.keras.preprocessing.text import  Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "texts =['A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away', 'To the brave crew and passengers of the Kobayshi Maru sucks to be you', 'Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies', 'They are merely scars not mortal wounds and you must use them to propel you forward', 'You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose']\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60,padding='post')\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploding gradient problem\n",
    "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
    "\n",
    "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
    "\n",
    "The data is already loaded on the environment as X_train, X_test, y_train and y_test.\n",
    "\n",
    "You will use a Stochastic Gradient Descent (SGD) optimizer and Mean Squared Error (MSE) as the loss function.\n",
    "\n",
    "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the clipvalue parameter to solve the problem.\n",
    "\n",
    "The Stochastic Gradient Descent in Keras is loaded as SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoid Exploding \n",
    "### Exploding gradient problem\n",
    "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
    "\n",
    "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
    "\n",
    "The data is already loaded on the environment as X_train, X_test, y_train and y_test.\n",
    "\n",
    "You will use a Stochastic Gradient Descent (SGD) optimizer and Mean Squared Error (MSE) as the loss function.\n",
    "\n",
    "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the clipvalue parameter to solve the problem.\n",
    "\n",
    "The Stochastic Gradient Descent in Keras is loaded as SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse= model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Exploding gradient problem can happen when using RNN models. Luckily, this can be addressed with simple techniques such as gradient clipping. Notice how after applying this technique, the outputs are no longer NaN, meaning that the gradients didn't 'explode' during Step 2\n",
    "\n",
    "\n",
    "# Vanishing gradient problem\n",
    "The other possible gradient problem is when the gradients vanish, or go to zero. This is a much harder problem to solve because it is not as easy to detect. If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?\n",
    "\n",
    "This problem occurs more often in RNN models when long memory is required, meaning having long sentences.\n",
    "\n",
    "In this exercise you will observe the problem on the IMDB data, with longer sentences selected. The data is loaded in X and y variables, as well as classes Sequential, SimpleRNN, Dense and matplotlib.pyplot as plt. The model was pre-trained with 100 epochs and its weights are stored on the file model_weights.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that at some point the accuracy stopped to improve, which can happen because of the vanishing gradient problem. This kind of problem is harder to detect than the exploding gradient problem and will demand deeper analysis by the data scientist. Researchers found a model architecture way to solve this problem, which you will study later in this course. Instead of using SimpleRNN cells, you can use the more complex ones such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) cells.\n",
    "# GRU cells are better than simpleRNN\n",
    "In this exercise you will re-run the same model as the first chapter of the course to compare the accuracy of the model by simpling changing the SimpleRNN cell to a GRU cell.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with a SimpleRNN cell. In order to compare the models, a test set (x_test, y_test) is already loaded in the environment, as well as the old model SimpleRNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Print the old and new model summaries\n",
    "SimpleRNN_model.summary()\n",
    "gru_model.summary()\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n",
    "print(\"GRU model's accuracy:\\t{0}\".format( acc_GRU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking RNN layers\n",
    "Deep RNN models can have tens to hundreds of layers in order to achieve state-of-the-art results.\n",
    "\n",
    "In this exercise, you will get a glimpse of how to create deep RNN models by stacking layers of LSTM cells one after the other.\n",
    "\n",
    "To do this, you will set the return_sequences argument to True on the firsts two LSTM layers and to False on the last LSTM layer.\n",
    "\n",
    "To create models with even more layers, you can keep adding them one after the other or create a function that uses the .add() method inside a loop to add many layers with few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LSTM layer\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('lstm_stack_model_weights.h5')\n",
    "\n",
    "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Stacking more layers also improve the accuracy of the model when comparing to the baseline 'simple_RNN' model! In the next lesson you will learn what else you can do to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of parameters comparison\n",
    "You saw that the one-hot representation is not a good representation of words because it is very sparse. Using the Embedding layer creates a dense representation of the vectors, but also demands a lot of parameters to be learned.\n",
    "\n",
    "In this exercise you will compare the number of parameters of two models using embeddings and one-hot encoding to see the difference.\n",
    "\n",
    "The model model_onehot is already loaded in the environment, as well as the Sequential, Dense and GRU from keras. Finally, the parameters vocabulary_size=80000 and sentence_len=200 are also loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the embedding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size+1, output_dim=wordvec_dim, input_length=sentence_len, trainable=True))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the one-hot model\n",
    "model_onehot.summary()\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the immense difference in the number of parameters when using the embedding layer! Don't worry, in the next exercise you will learn how make transfer learning to avoid having to train this layer.\n",
    "# Transfer learning\n",
    "You saw that when training an embedding layer, you need to learn a lot of parameters.\n",
    "\n",
    "In this exercise, you will see that when using transfer learning it is possible to use the pre-trained weights and don't update them, meaning that all the parameters of the embedding layer will be fixed, and the model will only need to learn the parameters from the other layers.\n",
    "\n",
    "The function load_glove is already loaded on the environment and retrieves the glove matrix as a numpy.ndarray vector. It uses the function covered on the lesson's slides to retrieve the glove vectors with 200 embedding dimensions for the vocabulary present in this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove pre-trained vectors\n",
    "glove_matrix = load_glove('glove_200d.zip')\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
    "                    embeddings_initializer= Constant(glove_matrix), \n",
    "                    input_length=sentence_len, trainable=False))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the total parameters is very big, but the number of parameteres that will be trained is much smaller. The trained vectors already has values for the words, but is equal to a vector of zeros for new words not present in the pre-trained vectors. This can lead to problems if the task at hand is very specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with embedding\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=max_vocabulary, output_dim=wordvec_dim, input_length=max_len))\n",
    "model.add(SimpleRNN(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('embedding_model_weights.h5')\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_embeddings = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(acc_simpleRNN, acc_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings improves performance\n",
    "Does the embedding layer improves the accuracy of the model? Let's check it out in the same IMDB data.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with simpleRNN cell. In order to compare the models, a test set (X_test, y_test) is available in the environment, as well as the old model simpleRNN_model. The old model's accuracy is loaded in the variable acc_SimpleRNN.\n",
    "\n",
    "All required modules and functions as loaded in the environment: Sequential() from keras.models, Embedding and Dense from keras.layers and SimpleRNN from keras.layers.recurrent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification revisited\n",
    "\n",
    "## Better sentiment classification\n",
    "In this exercise, you go back to the sentiment classification problem seen in Chapter 1.\n",
    "\n",
    "You are going to add more complexity to the model and improve its accuracy. You will use an Embedding layer to train word vectors on the training set and two LSTM layers to keep track of longer texts. Also, you will add an extra Dense layer before the output.\n",
    "\n",
    "This is no longer a simple model, and the training can take some time. For this reason, a pre-trained model is available by loading its weights with the method .load_weights() from the keras.models.Sequential class. The model was trained with 10 epochs and its weights are available on the file model_weights.h5.\n",
    "\n",
    "The following modules are loaded on the environment: Sequential, Embedding, LSTM, Dropout, Dense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add( Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Print the obtained loss and accuracy\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superb! You just increased the accuracy of your sentiment classification task from poorly 50% to more than 80%, \n",
    "# Using the CNN layer\n",
    "In this exercise, you will use a pre-trained model that makes use of the Conv1D and MaxPooling1D layers from the keras.layers.convolutional module, and achieves even better accuracy on the classification task.\n",
    "\n",
    "This architecture achieved good results in language modeling tasks such as classification, and is added here as an extra exercise to see it in action and have some intuitions.\n",
    "\n",
    "Because this layer is not in the scope of the course, you will focus on how to use the layers together with the RNN layers you already learned.\n",
    "\n",
    "Please follow the instructions to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model_cnn.summary\n",
    "\n",
    "# Load pre-trained weights\n",
    "model_cnn.load_weights('model_weights.h5')\n",
    "\n",
    "# Evaluate the model to get the loss and accuracy values\n",
    "loss, acc = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing Multi Class Classification\n",
    "\n",
    "# Prepare label vectors\n",
    "In the video exercise, you learned the differences between binary classification and multi-class classification. You learned that there are some modifications to the data preparation process that need to be done before training the models.\n",
    "\n",
    "In this exercise, you will prepare a raw dataset with labels given as text. The data is given as a pandas.DataFrame called df, with two columns: text with the text data and label with the label names. Your task is to make all the necessary transformations to the labels: change string to number and one-hot encode.\n",
    "\n",
    "The module pandas as pd and the function to_categorical() from keras.utils.np_utils are already loaded in the environment and the first lines of the dataset is printed on the console for you to see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "\n",
    "# One-hot encode the indexes\n",
    "Y = to_categorical(numerical_ids)\n",
    "\n",
    "# Check the new shape of the variable\n",
    "print(Y.shape)\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(Y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, you are able to transform any dataset to the format needed by RNN models. You can try with a dataset of your liking. Also, you can see that each class is now equidistant and the loss function will treat every misclassification in the same way, allowing for a better learning phase. To train RNN models, it is necessary to transform the text representation of the classes to a numeric one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "\n",
    "# One-hot encode the indexes\n",
    "Y = to_categorical(numerical_ids)\n",
    "\n",
    "# Check the new shape of the variable\n",
    "print(Y.shape)\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(Y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data\n",
    "You learned the differences for pre-processing the data in the case of multi-class classification. Let's put that into practice by preprocessing the data in anticipation of creating a simple multi-class classification model.\n",
    "\n",
    "The dataset is loaded in the variable news_dataset, and has the following attributes:\n",
    "\n",
    "news_dataset.data: array with texts\n",
    "news_dataset.target: array with target categories as numerical indexes\n",
    "The sample data contains 5,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit tokenizer\n",
    "tokenizer =Tokenizer()\n",
    "tokenizer.fit_on_texts(news_dataset)\n",
    "# Prepare the data\n",
    "prep_data = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "prep_data = pad_sequences(prep_data, maxlen=200)\n",
    "# Prepare the labels\n",
    "prep_labels = to_categorical(news_dataset.target)\n",
    "# Print the shapes\n",
    "print(prep_data.shape)\n",
    "print(prep_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning for language models\n",
    "# Transfer learning starting point\n",
    "In this exercise you will see the benefit of using pre-trained vectors as a starting point for your model.\n",
    "\n",
    "You will compare the accuracy of two models trained with two epochs. The architecture of the models is the same: One embedding layer, one LSTM layer with 128 units and the output layer with 5 units which is the number of classes in the sample data. The difference is that one model uses pre-trained vectors on the embedding layer (transfer learning) and the other doesn't.\n",
    "\n",
    "The pre-trained vectors used were the GloVE with 200 dimension. The training accuracy history of the validation set of both models are available in the variables history_no_emb and history_emb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Insert lists of accuracy obtained on the validation set\n",
    "plt.plot(history_no_emb['acc'], marker='o')\n",
    "plt.plot(history_emb['acc'], marker='o')\n",
    "\n",
    "# Add extra descriptions to plot\n",
    "plt.title('Learning with and without pre-trained embedding vectors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['no_embeddings', 'with_embeddings'], loc='upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning provides a initial knowledge of the meaning of the words, and you can see that the model that used pre-trained embeddings started with higher accuracy. Of course, the model without transfer learning is learning directly from the corpus and is more specialized on the vocabulary present in the corpus, while the word embeddings used from transfer learning are more generic. By training the embeddings directly on the corpus, the model can be even better than the one initialized with the weights from transfer learning, but in many cases the computer power to train embeddings in a very big dataset is prohibitive.\n",
    "# Word2Vec\n",
    "In this exercise you will create a Word2Vec model using Keras.\n",
    "\n",
    "The corpus used to pre-train the model is the script of all episodes of the The Big Bang Theory TV show, divided sentence by sentence. It is available in the variable bigbang.\n",
    "\n",
    "The text on the corpus was transformed to lower case and all words were tokenized. The result is stored in the tokenized_corpus variable.\n",
    "\n",
    "A Word2Vec model was pre-trained using a window size of 10 words for context (5 before and 5 after the center word), words with less than 3 occurrences were removed and the skip gram model method was used with 50 dimension. The model is saved on the file bigbang_word2vec.model.\n",
    "\n",
    "The class Word2Vec is already loaded in the environment from gensim.models.word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2vec\n",
    "# Word2Vec model\n",
    "w2v_model = Word2Vec.load(\"bigbang_word2vec.model\")\n",
    "\n",
    "# Selected words to check similarities\n",
    "words_of_interest = ['bazinga', 'penny', 'universe', 'spock', 'brain']\n",
    "\n",
    "# Compute top 5 similar words for each of the words of interest\n",
    "top5_similar_words = []\n",
    "for word in words_of_interest:\n",
    "    top5_similar_words.append(\n",
    "      {word: [item[0] for item in w2v_model.wv.most_similar([word], topn=5)]}\n",
    "    )\n",
    "\n",
    "# Print the sisklearn.datasetsmilar words\n",
    "print(top5_similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rolfe@dsuvax.dsu.edu (Tim Rolfe)\n",
      "Subject: Re: What did Lazarus smell like?\n",
      "Lines: 15\n",
      "\n",
      "In the discussion as to why Jesus spoke aloud the \"Lazarus, come out\",\n",
      "I'm surprised that no one has noticed the verse immediately preceeding.\n",
      "\n",
      "Jn 12:41  \"Father, I thank you for listening to me, though I knew that\n",
      "you always listen to me.  But I have said this for the sake of the\n",
      "people that are standing around me that they may believe that you have\n",
      "made my your messenger.\"  (Goodspeed translation)\n",
      "\n",
      "My guess is that the \"Lazarus, come out!\" was also for the sake of the\n",
      "crowd.\n",
      "-- \n",
      "                                                    --- Tim Rolfe\n",
      "                                                 rolfe@dsuvax.dsu.edu\n",
      "                                                 rolfe@junior.dsu.edu\n",
      "                                                RolfeT@columbia.dsu.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "cats = ['alt.atheism', 'sci.space','soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "print(newsgroups_train.data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 5, 1, 10, 65, 10, 2, 1, 65, 2, 1, 10, 11, 10, 65, 47, 65, 6, 65, 2, 43, 66, 65, 10, 10, 11, 10]\n",
      "Before: 2\n",
      "After: [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Import relevant classes/functions\n",
    "from tensorflow.keras.preprocessing.text import  Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf \n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# Transform the text into numerical indexes\n",
    "news_num_indices = tokenizer.texts_to_sequences(newsgroups_train.data)\n",
    "\n",
    "# Print the transformed example article\n",
    "print(news_num_indices[2])\n",
    "\n",
    "# Transform the labels into one-hot encoded vectors\n",
    "labels_onehot = tf.keras.utils.to_categorical(newsgroups_train.target)\n",
    "\n",
    "# Check before and after for the sample article\n",
    "print(\"Before: {0}\\nAfter: {1}\".format(newsgroups_train.target[2] ,labels_onehot[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying news articles\n",
    "In this exercise you will create a multi-class classification model.\n",
    "\n",
    "The dataset is already loaded in the environment as news_novel. Also, all the pre-processing of the training data is already done and tokenizer is also available in the environment.\n",
    "\n",
    "A RNN model was pre-trained with the following architecture: use the Embedding layer, one LSTM layer and the output Dense layer expecting three classes: sci.space, alt.atheism, and soc.religion.christian. The weights of this trained model are available on the classify_news_weights.h5 file.\n",
    "\n",
    "You will pre-process the novel data and evaluate on a new dataset news_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change text for numerical ids and pad\n",
    "X_novel = tokenizer.texts_to_sequences(news_novel.data)\n",
    "X_novel = pad_sequences(X_novel, maxlen=400)\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y_novel = to_categorical(news_novel.target)\n",
    "\n",
    "# Load the model pre-trained weights\n",
    "model.load_weights('classify_news_weights.h5')\n",
    "\n",
    "# Evaluate the model on the new dataset\n",
    "loss, acc = model.evaluate(X_novel, Y_novel, batch_size=64)\n",
    "\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss:\\t{0}\\nAccuracy:\\t{1}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it can take some time to train one epoch of a RNN model. Also, you can modify the model architecture to add or change layers, the more layers the model has, the more time it need to train all the parameters.\n",
    "# Assessing the model's performance\n",
    "# Precision-Recall trade-off\n",
    "When working with classification tasks, the term Precision-Recall trade-off often appears. Where does it comes from?\n",
    "\n",
    "Usually, the class with higher probability (obtained by the .predict_proba() method) is chosen to assign the document to. But, what if the maximum probability is equal to 0.1? Should you consider that document to belong to this class with only 10% probability?\n",
    "\n",
    "The answer varies according to problem at hand. It is possible to add a minimum threshold to accept the classification, and by changing the threshold the values of precision and recall move in opposite directions.\n",
    "\n",
    "The variables y_true and the model model are loaded. Also, if the probability is lower than the threshold, the document will be assigned to DEFAULT_CLASS (chosen to be class 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for each class\n",
    "pred_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "# Thresholds at 0.5 and 0.8\n",
    "y_pred_50 = [np.argmax(x) if np.max(x) >= 0.5  else DEFAULT_CLASS for x in pred_probabilities]\n",
    "y_pred_80 = [np.argmax(x) if np.max(x) >= 0.8 else DEFAULT_CLASS for x in pred_probabilities]\n",
    "\n",
    "trade_off = pd.DataFrame({\n",
    "    'Precision_50': precision_score(y_true, y_pred_50, average=None), \n",
    "    'Precision_80': precision_score(y_true, y_pred_80, average=None), \n",
    "    'Recall_50': recall_score(y_true, y_pred_50, average=None), \n",
    "    'Recall_80': recall_score(y_true, y_pred_80, average=None)}, \n",
    "  index=['Class 1', 'Class 2', 'Class 3'])\n",
    "\n",
    "print(trade_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that for some classes precision increased and recall decresed, and the opposite also can happen. When one metric increases, the other has to decrease. The trade-off depends on the specific problem you are solving. If missclassification is not desirable for the class of interest, then you should change the threshold to increase the precision. Likewise, if misclassification is acceptable and you are interested in covering all the observations of a specific class, then you should tune the threshold for higher recall values.\n",
    "# Performance on multi-class classification\n",
    "In this exercise, you will compute the performance metrics for models using the module sklearn.metrics.\n",
    "\n",
    "The model is already trained and stored in the variable model. Also, the variables X_test and y_true are also loaded, together with the functions confusion_matrix() and classification_report() from sklearn.metrics package.\n",
    "\n",
    "You will first compute the confusion matrix of the model. Then, to summarize a model's performance, you will compute the precision, recall and F1-score using the classification_report() function. In this function, you can optionally pass a list containing the classes names (they are stored it in the news_cat variable) to the parameter target_names to make the report more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict on new data\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Choose the class with higher probability \n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Create the performance report\n",
    "print(classification_report(y_true, y_pred, target_names=news_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats great! You can have all the metrics in a single function call. It is an easy and fast way to evaluate the model performance in classification tasks. Also, remember that precision measures how good the predictions of the model are, meaning that high precision on one class makes the predictions the model make on that class to be reliable. In the other hand, recall measures how good the model is to predict each class, meaning that if you are interested in predicting a specific class and need high coverage on the number of true cases, you want high recall values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
