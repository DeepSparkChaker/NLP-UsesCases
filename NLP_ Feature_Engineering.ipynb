{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP feature engineering\n",
    "## 1-One-hot encoding\n",
    "In the previous exercise, we encountered a dataframe df1 which contained categorical features and therefore, was unsuitable for applying ML algorithms to.\n",
    "\n",
    "In this exercise, your task is to convert df1 into a format that is suitable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# import data \n",
    "df= pd.read_csv('C:/Users/rzouga/Downloads/Github/NLP/fake_or_real_news.csv')\n",
    "# Print the head of df\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the features of df1\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For machine learning, you almost definitely want to use sklearn.OneHotEncoder. For other tasks like simple analyses, you might be able to use pd.get_dummies, which is a bit more convenient.\n",
    "\n",
    "Note that sklearn.OneHotEncoder has been updated in the latest version so that it does accept strings for categorical variables, as well as integers.\n",
    "\n",
    "The crux of it is that the sklearn encoder creates a function which persists and can then be applied to new data sets which use the same categorical variables, with consistent results.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "##### Create the encoder.\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "encoder.fit(X_train)    # Assume for simplicity all features are categorical.\n",
    "\n",
    "##### Apply the encoder.\n",
    "X_train = encoder.transform(X_train)\n",
    "\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "Note how we apply the same encoder we created via X_train to the new data set X_test.\n",
    "\n",
    "Consider what happens if X_test contains different levels than X_train for one of its variables. For example, let's say X_train[\"color\"] contains only \"red\" and \"green\", but in addition to those, X_test[\"color\"] sometimes contains \"blue\".\n",
    "\n",
    "If we use pd.get_dummies, X_test will end up with an additional \"color_blue\" column which X_train doesn't have, and the inconsistency will probably break our code later on, especially if we are feeding X_test to an sklearn model which we trained on X_train.\n",
    "\n",
    "And if we want to process the data like this in production, where we're receiving a single example at a time, pd.get_dummies won't be of use.\n",
    "\n",
    "With sklearn.OneHotEncoder on the other hand, once we've created the encoder, we can reuse it to produce the same output every time, with columns only for \"red\" and \"green\". And we can explicitly control what happens when it encounters the new level \"blue\": if we think that's impossible, then we can tell it to throw an error with handle_unknown=\"error\"; otherwise we can tell it to continue and simply set the red and green columns to 0, with handle_unknown=\"ignore\".*\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/encoding-categorical-features-21a2651a065c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'text', 'label_FAKE', 'label_REAL'], dtype='object')\n",
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text  label_FAKE  label_REAL  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...           1           0  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...           1           0  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...           0           1  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...           1           0  \n",
      "4  It's primary day in New York and front-runners...           0           1  \n"
     ]
    }
   ],
   "source": [
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df, columns=['label'])\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Basic feature extraction\n",
    "###  2-1- Character count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4707.250355169692\n"
     ]
    }
   ],
   "source": [
    "# Create a feature char_count\n",
    "df1['char_count'] = df1['text'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(df1['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776.3007103393844\n"
     ]
    }
   ],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "df1['word_count'] = df1['text'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(df1['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "df1['hashtag_count'] = df1['text'].apply(count_hashtags)\n",
    "df1['hashtag_count'].hist(bins=5)\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'taxi', '-', 'hailing', 'company', 'Uber', 'brings', 'into', 'very', 'sharp', 'focus', 'the', 'question', 'of', 'whether', 'corporations', 'can']\n"
     ]
    }
   ],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "documents = 'The taxi-hailing company Uber brings into very sharp focus the question of whether corporations can'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(str(documents))\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the taxi - hail company Uber bring into very sharp focus the question of whether corporation can\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(documents)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc ]\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "\n",
    "Clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarm rise populism Europe warning sign come UK Brexit Referendum vote swinge way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "post='Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.'\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(post)\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords ]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning News in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Daniel Greenfield Shillman Journalism Fellow F...\n",
      "1    Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
      "2    Secretary State John Kerry Monday stop Paris l...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to df1['text']\n",
    "df1['text'] = df1['text'].apply(preprocess)\n",
    "print(df1['text'][0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging \n",
    "In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, Lord of the Flies, authored by William Golding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NOUN'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "novel='He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(novel)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting nouns in a piece of text\n",
    "In this exercise, we will write two functions, nouns() and proper_nouns() that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
    "\n",
    "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun usage in fake news\n",
    "In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines.\n",
    "\n",
    "Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['num_propn'] = df1['text'].apply(proper_nouns)\n",
    "# Compute mean of proper nouns\n",
    "real_propn = df1[df1['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = df1[df1['label'] == 'FAKE']['num_propn'].mean()\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    " we will identify and classify the labels of various named entities in a body of text using one of spaCy's statistical models. We will also verify the veracity of these labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai PERSON\n",
      "Google ORG\n",
      "Mountain View GPE\n"
     ]
    }
   ],
   "source": [
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "text='It’s been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.'\n",
    "def find_persons(text):\n",
    "    # Create Doc object\n",
    "    doc = nlp(text)\n",
    "  \n",
    "    # Identify the persons\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "    # Return persons\n",
    "    return persons\n",
    "\n",
    "print(find_persons(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a bag of words model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 52824)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer= CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(df1['text'])\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping feature indices with feature names\n",
    "In the lesson video, we had seen that CountVectorizer doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   an  decade  endangered  have  is  jungle  king  lifespans  lion  lions  of  \\\n",
      "0   0       0           0     0   1       1     1          0     1      0   1   \n",
      "1   0       1           0     1   0       0     0          1     0      1   1   \n",
      "2   1       0           1     0   1       0     0          0     1      0   0   \n",
      "\n",
      "   species  the  \n",
      "0        0    3  \n",
      "1        0    0  \n",
      "2        1    1  \n"
     ]
    }
   ],
   "source": [
    "text1=['The lion is the king of the jungle', 'Lions have lifespans of a decade', 'The lion is an endangered species']\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer ()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(text1)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a BoW Naive Bayes classifier\n",
    "\n",
    "\n",
    "\n",
    "In this exercise, you have been given two pandas Series, X_train and X_test, which consist of movie reviews\n",
    "\n",
    "They represent the training and the test review data respectively.\n",
    "\n",
    "Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer.\n",
    "# Normalization \n",
    "Normalization after splitting into train and test/validation. The reason is to avoid any data leakage.\n",
    "\n",
    "# Data Leakage:\n",
    "\n",
    "Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed.\n",
    "\n",
    "Always split before you do any data pre-processing. Performing pre-processing before splitting will mean that information from your test set will be present during training, causing a data leak.\n",
    "\n",
    "Think of it like this, the test set is supposed to be a way of estimating performance on totally unseen data. If it affects the training, then it will be partially seen data.\n",
    "\n",
    "I don't think the order of scaling/imputing is as strict. I would impute first if the method might throw of the scaling/centering.\n",
    "\n",
    "Your steps should be:\n",
    "\n",
    "    Splitting\n",
    "    Imputing\n",
    "    Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5068, 61360)\n",
      "(1267, 61360)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split \n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "y = df.iloc[:,-1] #target column i.e \n",
    "# Create training and test sets\n",
    "X_train,X_test, y_train, y_test= train_test_split(df[\"text\"],y, test_size=0.2, random_state=53) \n",
    "# Create a CountVectorizer object\n",
    "vectorizer =CountVectorizer(lowercase=True, stop_words='english')\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5068,)\n",
      "(1267,)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test=encoder.transform(y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.897\n",
      "The news predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow,y_train)\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow ,y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The news predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building n-gram models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 13, 27 and 39 features respectively\n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(text1)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2),lowercase=True, stop_words='english')\n",
    "ng2 = vectorizer_ng2.fit_transform(text1)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(text1)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher order n-grams for sentiment analysis\n",
    "Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task.\n",
    "\n",
    "The n-gram training reviews are available as X_train_ng. The corresponding test reviews are available as X_test_ng. Finally, use y_train and y_test to access the training and test sentiment classes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.904\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer object\n",
    "# Fit and transform X_train\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2),lowercase=True, stop_words='english')\n",
    "X_train_ng = vectorizer_ng2.fit_transform(X_train)\n",
    "# Transform X_test\n",
    "X_test_ng = vectorizer_ng2.transform(X_test)\n",
    "# Define an instance of MultinomialNB \n",
    "clf_ng =MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng ,y_train )\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng,y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(vectorizer_ng2.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 114.094 seconds to complete. The accuracy on the test set is 0.90. The ngram representation had 3779472 features.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X_train_ng3 = vectorizer.fit_transform(X_train)\n",
    "X_test_ng3 = vectorizer.transform(X_test)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_ng3, y_train)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(X_test_ng3, y_test), X_train_ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building tf-idf document vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13)\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer= TfidfVectorizer()\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(text1)\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "pairwise distance provide distance between two array.so more pairwise distance means less similarity.while cosine similarity is 1-pairwise_distance so more cosine similarity means more similarity between two arrays.\n",
    "\n",
    "# Best Document search :\n",
    "https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "https://towardsdatascience.com/the-best-document-similarity-algorithm-in-2020-a-beginners-guide-a01b9ef8cf05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.0260012149810791 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "corpus=['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']\n",
    "from sklearn.metrics.pairwise import cosine_similarity,linear_kernel\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim =cosine_similarity (tfidf_matrix,tfidf_matrix)\n",
    "print(cosine_sim)\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a plot line based recommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031    FBI REDUX: What’s Behind New Probe into Hillar...\n",
      "4016    Memo to Comey: Keep Your Damn Hands Off Our El...\n",
      "5514    FBI “Insurrection” to Scuttle Director, Rig El...\n",
      "3897    Clinton Camp Tries to Deflect Suspicion as FBI...\n",
      "1563    NBC's Baghdad Bob: There Is No FBI Investigati...\n",
      "750          Why Comey Reopened the Hillary Investigation\n",
      "4548    FBI Reopen Hillary Clinton Email Investigation...\n",
      "2462    Why Comey Reopened the Hillary Investigation (...\n",
      "5090    New emails under review in Clinton case emerge...\n",
      "6326    DOJ COMPLAINT: Comey Under Fire Over Partisan ...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['title'].iloc[movie_indices]\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(df[\"text\"])\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('You Can Smell Hillary’s Fear', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond n-grams: word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzouga\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like 0.060950655\n",
      "I apples -0.014093488\n",
      "I and -0.09230058\n",
      "I oranges -0.20608483\n",
      "like I 0.060950655\n",
      "like like 1.0\n",
      "like apples -0.018225208\n",
      "like and 0.09190109\n",
      "like oranges -0.12486788\n",
      "apples I -0.014093488\n",
      "apples like -0.018225208\n",
      "apples apples 1.0\n",
      "apples and 0.060377873\n",
      "apples oranges 0.6448554\n",
      "and I -0.09230058\n",
      "and like 0.09190109\n",
      "and apples 0.060377873\n",
      "and and 1.0\n",
      "and oranges 0.020107673\n",
      "oranges I -0.20608483\n",
      "oranges like -0.12486788\n",
      "oranges apples 0.6448554\n",
      "oranges and 0.020107673\n",
      "oranges oranges 1.0\n"
     ]
    }
   ],
   "source": [
    " sent = 'I like apples and oranges'\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(df1['text'][0]).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
    "\n",
    "\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "    readability_scores = Textatistic(excerpt).scores\n",
    "    gunning_fog = readability_scores['gunningfog_score']\n",
    "    gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
