{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization with NLTK\n",
    "\n",
    "word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wind] [clop clop clop]  KING ARTHUR: Whoa there!  [clop clop clop] SOLDIER #1: #Halt!  #Who goes there?ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!SOLDIER #1: Pull the other one!ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.SOLDIER #1: What?  @Ridden on a #horse?\n",
      "{'your', 'Pendragon', 'Britons', '...', 'What', 'sovereign', '#', 'my', 'goes', 'breadth', 'with', 'length', 'King', 'must', 'the', 'horse', 'SOLDIER', ',', 'of', 'Camelot', 'ridden', 'there', 'defeator', 'one', '?', 'this', 'all', 'Ridden', 'trusty', '.', 'It', 'knights', 'Arthur', 'Uther', 'Whoa', 'England', 'Pull', 'me', 'on', 'have', 'ARTHUR', 'and', 'KING', '[', 'wind', 'son', 'I', 'land', 'a', 'other', 'master.SOLDIER', '1', 'Saxons', 'Patsy', 'in', 'lord', 'who', 'court', 'We', 'clop', 'search', ':', 'at', 'castle', 'is', 'Halt', '!', ']', 'Who', 'servant', 'join', 'am', 'will', 'from', 'speak', '@'}\n"
     ]
    }
   ],
   "source": [
    "scene_one =\"[wind] [clop clop clop]  KING ARTHUR: Whoa there!  [clop clop clop] SOLDIER #1: #Halt!  #Who goes there?ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!SOLDIER #1: Pull the other one!ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.SOLDIER #1: What?  @Ridden on a #horse?\"\n",
    "# Import necessary modules\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent =word_tokenize (sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "print(scene_one)\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#1']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(sentences[3], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@Ridden', '#horse']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(sentences[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'wind', ']', '[', 'clop', 'clop', 'clop', ']', 'KING', 'ARTHUR', ':', 'Whoa', 'there', '!'], ['[', 'clop', 'clop', 'clop', ']', 'SOLDIER', '#', '1', ':', '#Halt', '!'], ['#Who', 'goes', 'there', '?', 'ARTHUR', ':', 'It', 'is', 'I', ',', 'Arthur', ',', 'son', 'of', 'Uther', 'Pendragon', ',', 'from', 'the', 'castle', 'of', 'Camelot', '.'], ['King', 'of', 'the', 'Britons', ',', 'defeator', 'of', 'the', 'Saxons', ',', 'sovereign', 'of', 'all', 'England', '!', 'SOLDIER', '#', '1', ':', 'Pull', 'the', 'other', 'one', '!', 'ARTHUR', ':', 'I', 'am', ',', '...', 'and', 'this', 'is', 'my', 'trusty', 'servant', 'Patsy', '.'], ['We', 'have', 'ridden', 'the', 'length', 'and', 'breadth', 'of', 'the', 'land', 'in', 'search', 'of', 'knights', 'who', 'will', 'join', 'me', 'in', 'my', 'court', 'at', 'Camelot', '.'], ['I', 'must', 'speak', 'with', 'your', 'lord', 'and', 'master.SOLDIER', '#', '1', ':', 'What', '?'], ['@Ridden', 'on', 'a', '#horse', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in sentences]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'üçï', 'Und', 'f√§hrst', 'du', 'mit', '√úber', '?', 'üöï']\n",
      "['Wann', 'Pizza', 'Und', '√úber']\n",
      "['üçï', 'üöï']\n"
     ]
    }
   ],
   "source": [
    "german_text='Wann gehen wir Pizza essen? üçï Und f√§hrst du mit √úber? üöï'\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-Z√ú]\\w+\"\n",
    "print(regexp_tokenize(german_text,capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "# Split the script into lines: lines\n",
    "lines = scene_one.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s,'\\w+') for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 7), ('clop', 6), (':', 6), ('#', 6), (',', 6), ('the', 6), ('arthur', 4), ('!', 4), ('[', 3), (']', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(scene_one)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing practice\n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rzouga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('clop', 6), ('arthur', 4), ('king', 2), ('soldier', 2), ('camelot', 2), ('ridden', 2), ('wind', 1), ('whoa', 1), ('halt', 1), ('go', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "english_stops= stopwords.words('english')\n",
    "# Import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(scene_one)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [w.lower() for w in tokens]\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops ]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
